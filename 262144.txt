O vídeo é uma forma poderosa de provar o seu ponto de vista. Quando clica em Vídeo Online, pode colar o código incorporado para o vídeo que quer adicionar. Pode também escrever uma palavra-chave para procurar online o vídeo que melhor se adapta ao seu documento. Para dar um ar de produção profissional ao seu documento, o Word disponibiliza desenhos de cabeçalho, rodapé, folha de rosto e caixas de texto que se complementam entre si.
Por exemplo, pode adicionar uma folha de rosto, um cabeçalho e uma barra lateral que combinam entre si. Clique em Inserir e escolha os elementos que quiser das diferentes galerias. Os temas e estilos também contribuem para manter o seu documento coordenado. Quando clica em Estrutura e escolhe um novo Tema, as imagens, gráficos e gráficos SmartArt são alterados para que combinem com o seu novo tema.
Quando aplica estilos, os seus títulos alteram-se para combinar com o novo tema. Poupe tempo no Word com novos botões que aparecem onde precisa deles. Para alterar a forma como uma imagem se adequa ao seu documento, clique nela e aparecerá um botão para opções de esquema junto à mesma. Quando trabalha numa tabela, clique onde quiser para adicionar uma linha ou uma coluna e  clique no sinal de adição.
O vídeo é uma forma poderosa de provar o seu ponto de vista. Quando clica em Vídeo Online, pode colar o código incorporado para o vídeo que quer adicionar. Pode também escrever uma palavra-chave para procurar online o vídeo que melhor se adapta ao seu documento. Para dar um ar de produção profissional ao seu documento, o Word disponibiliza desenhos de cabeçalho, rodapé, folha de rosto e caixas de texto que se complementam entre si. Por exemplo, pode adicionar uma folha de rosto, um cabeçalho e uma barra lateral que combinam entre si. Clique em Inserir e escolha os elementos que quiser das diferentes galerias.
Os temas e estilos também contribuem para manter o seu documento coordenado. Quando clica em Estrutura e escolhe um novo Tema, as imagens, gráficos e gráficos SmartArt são alterados para que combinem com o seu novo tema. Quando aplica estilos, os seus títulos alteram-se para combinar com o novo tema. Poupe tempo no Word com novos botões que aparecem onde precisa deles. Para alterar a forma como uma imagem se adequa ao seu documento, clique nela e aparecerá um botão para opções de esquema junto à mesma. Quando trabalha numa tabela, clique onde quiser para adicionar uma linha ou uma coluna e  clique no sinal de adiçao.
Ler é também mais fácil com a nova vista de Leitura. Pode fechar partes do documento e concentrar-se no texto que quiser. Se precisar de parar de ler antes de chegar ao final, o Word lembra-se do local onde ficou - mesmo noutro dispositivo. O vídeo é uma forma poderosa de provar o seu ponto de vista. Quando clica em Vídeo Online, pode colar o código incorporado para o vídeo que quer adicionar. Pode também escrever uma palavra-chave para procurar online o vídeo que melhor se adapta ao seu documento.
Para dar um ar de produção profissional ao seu documento, o Word disponibiliza desenhos de cabeçalho, rodapé, folha de rosto e caixas de texto que se complementam entre si. Por exemplo, pode adicionar uma folha de rosto, um cabeçalho e uma barra lateral que combinam entre si. Clique em Inserir e escolha os elementos que quiser das diferentes galerias. Os temas e estilos também contribuem para manter o seu documento coordenado. Quando clica em Estrutura e escolhe um novo Tema, as imagens, gráficos e gráficos SmartArt são alterados para que combinem com o seu novo tema. Quando aplica estilos, os seus títulos alteram-se para combinar com o novo tema.
Poupe tempo no Word com novos botões que aparecem onde precisa deles. Para alterar a forma como uma imagem se adequa ao seu documento, clique nela e aparecerá um botão para opções de esquema junto à mesma. Quando trabalha numa tabela, clique onde quiser ser o que eu sou pq sou feliz neste lugar natal .hhhhhhhhhwdwqbdjuvdkhq hsqvsvgwdvwxdjbhqkabqsbnnksqbbqhksbqka 222
ASSIGNMENT #1: Performance Benchmarking of Cryptographic Mechanisms
Due date: March 22, 23:59
Grading: Assignment #1 is worth 2 points
TO BE DONE IN GROUPS OF TWO (MANDATORY)
[ Cryptography hazmat python manual: https://cryptography.io/en/latest/hazmat/primitives/ ]
In this exercise you should measure the time AES, RSA and SHA take to process files of different sizes, using a
python implementation of the encryption/description and hash mechanisms.
Some notes:
• You should measure the time of cryptographic operations/algorithms only, not including the time for generation
of files and others side aspects.
• If you use padding, this may affect the results specially for small file sizes
A. Generate random text files with the following sizes:
• For AES (in bytes): 8, 64, 512, 4096, 32768, 262144, 2097152
• For SHA (in bytes): 8, 64, 512, 4096, 32768, 262144, 2097152
• For RSA (in bytes): 2, 4, 8, 16, 32, 64, 128
B. Encrypt and decrypt all these files using AES. Employ a key of 256 bits. Measure the time it takes to encrypt
and decrypt each of the files. To do this, you might want to use the python module timeit.
Make sure to produce statistically significant results. Do results change if you run a fixed algorithm over
the same file multiple times? And what if you run an algorithm over multiple randomly generated files
of fixed size?
C. Using the python module for RSA encryption and decryption, measure the time of RSA encryption and decryption
for the file sizes listed in part A, with a key of size 2048 bits (minimum recommended for RSA).
D. Measure the time for SHA-256 hash generation for the file sizes listed in part a.
E. Prepare a report of your observations, including the following information:
• Code implemented for points b., c., and d. above
• Brief explanation of the main components of the code (the rest should be submitted in a separate compressed
file)
• Explain how you generated/obtained the results – must be statistically significant. This must include a
description of the experimental setup (e.g. computer characteristics, OS, software versions).
• Plots showing: (i) AES encryption/decryption times; (ii) RSA encryption times; (iii) RSA decryption times;
and (iv) SHA digests generation times (plots can be combined for easier comparison). In these graphs, the
X axis should plot the file sizes in units of bytes, and the Y axis should plot time measurements in units of
microseconds (us).
1
• The report should also analyze and explain the performance results of:
– Comparison between AES encryption and RSA encryption.
– Comparison between AES encryption and SHA digest generation.
– Comparison between RSA encryption and decryption times.
Submit your report in moodle.
A inteligência artificial (de sigla: IA; do inglês: artificial intelligence, de sigla: AI) é um campo de estudo multidisciplinar que abrange varias áreas do conhecimento.[1][2] É também um conjunto de novas tecnologias que permitem aos aparelhos smart executarem várias funções avançadas de modo quase autônomo,[2][3] representanto um marco histórico na computação moderna.[3] Embora seu desenvolvimento tenha avançado mais na ciência da computação, sua abordagem interdisciplinar envolve contribuições de diversas disciplina.

Algumas das principais áreas relacionadas à IA incluem:

Ciência da Computação: A ciência da computação desempenha um papel central na IA, fornecendo as bases teóricas e práticas para o desenvolvimento de algoritmos, modelos e técnicas computacionais para simular a inteligência humana.
Matemática e Estatística: A matemática e a estatística fornecem os fundamentos teóricos para a modelagem e análise de algoritmos de IA, incluindo aprendizagem de máquina, redes neurais e processamento de dados.
Aprendizagem de Máquina (Machine Learning): A aprendizagem de máquina é uma subárea da IA que se concentra no desenvolvimento de algoritmos que permitem aos computadores aprender e melhorar com base em dados. Isso envolve a aplicação de técnicas estatísticas e algoritmos de otimização.
Ciência Cognitiva: A ciência cognitiva estuda os processos mentais e a inteligência humana, e suas contribuições para a IA estão relacionadas à compreensão e modelagem dos processos cognitivos para o desenvolvimento de sistemas inteligentes.
Neurociência Computacional: A neurociência computacional busca entender o funcionamento do cérebro humano e aplicar esses insights no desenvolvimento de modelos e algoritmos de IA inspirados no cérebro.
Filosofia da Mente: A filosofia da mente explora questões relacionadas à natureza da mente, da consciência e da inteligência, oferecendo perspectivas teóricas importantes para o campo da IA.
Linguística Computacional: A linguística computacional envolve o processamento de linguagem natural (PLN), que se concentra no desenvolvimento de algoritmos e técnicas para que os computadores compreendam e processem a linguagem humana.
É difícil definir especificamente a Inteligência Artificial, mas ao longo do tempo seguiu essas quatro linhas de pensamento:[1]

Sistemas computacionais que pesam semelhante aos humanos... “O novo e interessante esforço para fazer os computadores pensarem... máquinas com mentes, no sentido total e literal”. (HAUGELAND, 1985).
Sistemas computacionais que trabalham semelhante aos humanos... “A arte de criar máquinas que executam funções que exigem inteligência quando executadas por pessoas.” (KURZWEIL, 1990).
Sistemas computacionais que pensam racionalmente... “O estudo das faculdades mentais pelo seu uso de modelos computacionais.” (CHARNIAK; MCDERMOTT, 1985).
Sistemas computacionais que trabalham racionalmente... “A Inteligência Computacional é o estudo do projeto de agentes inteligentes.” (POOLE et al., 1998).
É um conceito amplo e que recebe tantas definições quanto significados diferentes à palavra inteligência.[4] É possível considerar algumas características básicas desses sistemas, como a capacidade de raciocínio (aplicar regras lógicas a um conjunto de dados disponíveis para chegar a uma conclusão), aprendizagem (aprender com os erros e acertos para que no futuro possa agir de maneira mais eficaz), reconhecer padrões (tanto padrões visuais e sensoriais, como também padrões de comportamento) e inferência (capacidade de conseguir aplicar o raciocínio nas situações do nosso cotidiano).[4]

O desenvolvimento da área de estudo começou logo após a Segunda Guerra Mundial, com o artigo "Computing Machinery and Intelligence" do matemático inglês Alan Turing,[5] e o próprio nome foi cunhado em 1956.[6][7] Seus principais idealizadores foram os cientistas Herbert Simon, Allen Newell, John McCarthy, Warren McCulloch, Walter Pitts e Marvin Minsky. A construção de máquinas inteligentes interessa à humanidade há muito tempo, havendo na história tanto um registro significante de autômatos mecânicos (reais) quanto de personagens fictícios construídos pelo homem com inteligência própria, tais como o Golem e o Frankenstein. Tais relatos, lendas e ficções demonstram expectativas contrastantes do homem, de fascínio e de medo, em relação à Inteligência Artificial.[8][9]

Apenas recentemente, com o surgimento do computador moderno, é que a inteligência artificial ganhou meios e massa crítica para se estabelecer como ciência integral, com problemáticas e metodologias próprias. Desde então, seu desenvolvimento tem extrapolado os clássicos programas de xadrez ou de conversão e envolvido áreas como visão computacional, análise e síntese da voz, lógica difusa, redes neurais artificiais e muitas outras. Inicialmente, os modelos de IA visavam reproduzir o pensamento humano. Posteriormente, no entanto, tais modelos abraçaram a ideia de reproduzir capacidades humanas como criatividade, auto aperfeiçoamento e uso da linguagem. Porém, o conceito de inteligência artificial ainda é bastante difícil de se definir. Por essa razão, Inteligência Artificial foi (e continua sendo) uma noção que dispõe de múltiplas interpretações, não raro conflitantes ou circulares.[10]

História
Ver artigo principal: História da inteligência artificial
O interesse no desenvolvimento de máquinas autônomas capazes de simular o pensamento humano e de realizar varias tarefas cresceu vertiginosamente nas últimas décadas, da segunda metade do século XX, realizando assim os primeiros estudos sobre inteligência artificial (IA) a um propósito comum, a partir de iniciativas de cientistas de diversas áreas, como: psicologia, ciência cognitiva, ciência da computação e, robótica.[2] Ferramentas eficientes em analisar problemas e oferecer soluções e planejamentos (tomada de decisão), automatização de tarefas no cotidiano das pessoas.[2]

Mas apesar dos estudos serem modernos, o conceito de inteligência artificial não é contemporâneo; Aristóteles (professor de Alexandre, o Grande) sonhava em substituir a mão-de-obra escrava por ferramentas autônomas, sendo esta possivelmente a primeira idéia de Inteligência Artificial relatada, que a ciência da computação exploraria muito tempo depois.[11] O desenvolvimento dessa ideia ocorreu plenamente no Século XX, principalmente na década de 1950, com pensadores como Herbert Simon e John McCarthy. Inicialmente os teste em IA foram repletos de sucessos – porém limitados devido o desempenho reduzido dos primeiros computadores - oque causava surpresa, foi o fato de um computador realizar atividade remotamente inteligente.[12]

O sucesso inicial prosseguiu com o General Problem Solver (Solucionador de problemas gerais) ou GPS, desenvolvido por Newell e Simon.[13] Esse programa foi projetado para imitar protocolos humanos de resolução de problemas. Dentro da classe limitada de quebra-cabeças com a qual podia lidar, verificou-se que a ordem em que os seres humanos abordavam os mesmos problemas. Desse modo, o GPS talvez tenha sido o primeiro programa a incorporar a abordagem de “pensar de forma humana”.

Desde o início os fundamentos da inteligência artificial tiveram o suporte de várias disciplinas que contribuíram com ideias, pontos de vista e técnicas para a IA. Os filósofos (desde 400 a.C.) tornaram a IA concebível, considerando as ideias de que a mente é, em alguns aspectos, semelhante a uma máquina, de que ela opera sobre o conhecimento codificado em alguma linguagem interna e que o pensamento pode ser usado para escolher as ações que deverão ser executadas. Por sua vez, os matemáticos forneceram as ferramentas para manipular declarações de certeza lógica, bem como declarações incertas e probabilísticas. Eles também definiram a base para a compreensão da computação e do raciocínio sobre algoritmos.

Os economistas formalizaram o problema de tomar decisões que maximizam o resultado esperado para o tomador de decisões. Os psicólogos adotaram a ideia de que os seres humanos e os animais podem ser considerados máquinas de processamento de informações. Os linguistas mostraram que o uso da linguagem se ajusta a esse modelo. Os engenheiros de computação fornecem os artefatos que tornam possíveis as aplicações de IA. Os programas de IA tendem a ser extensos e não poderiam funcionar sem os grandes avanços em velocidade e memória que a indústria de informática tem proporcionado.

Atualmente, a IA abrange uma enorme variedade de subcampos. Dentre esses subcampos está o estudo de modelos conexionistas ou redes neurais. Uma rede neural pode ser vista como um modelo matemático simplificado do funcionamento do cérebro humano.[14] Este consiste de um número muito grande de unidades elementares de processamento, ou neurônios, que recebem e enviam estímulos elétricos uns aos outros, formando uma rede altamente interconectada.

No processamento, são compostos os estímulos recebidos conforme a intensidade de cada ligação, produzindo um único estímulo de saída. É o arranjo das interconexões entre os neurônios e as respectivas intensidades que define as principais propriedades e o funcionamento de uma RN. O estudo das redes neurais ou o conexionismo se relaciona com a capacidade dos computadores aprenderem e reconhecerem padrões. Podemos destacar também o estudo da biologia molecular na tentativa de construir vida artificial e a área da robótica, ligada à biologia e procurando construir máquinas que alojem vida artificial. Outro subcampo de estudo é a ligação da IA com a Psicologia, na tentativa de representar na máquina os mecanismos de raciocínio e de procura.

Nos últimos anos, houve uma revolução no trabalho em inteligência artificial, tanto no conteúdo quanto na metodologia. Agora, é mais comum usar as teorias existentes como bases, em vez de propor teorias inteiramente novas, fundamentar as informações em teoremas rigorosos ou na evidência experimental rígida, em vez de utilizar como base a intuição e destacar a relevância de aplicações reais em vez de exemplos hipotéticos.

A utilização da IA permite obter não somente ganhos significativos de performance, mas também possibilita o desenvolvimento de aplicações inovadoras, capazes de expandir de forma extraordinária nossos sentidos e habilidades intelectuais. Cada vez mais presente, a inteligência artificial simula o pensamento humano e se alastra por nosso cotidiano. Em maio de 2017 no Brasil, foi criada a ABRIA (Associação Brasileira de Inteligência Artificial) com o objetivo de mapear iniciativas brasileiras no setor de  inteligência artificial, englobando os esforços entre as empresas nacionais e formação de mão de obra especializada. Esse passo reforça que, atualmente, a inteligência artificial é impactante no setor econômico.
Investigação na IA experimental
A inteligência artificial começou como um campo experimental nos anos 50 com pioneiros como Allen Newell e Herbert Simon, que fundaram o primeiro laboratório de inteligência artificial na Universidade Carnegie Mellon, e McCarty que juntamente com Marvin Minsky, que fundaram o MIT AI Lab em 1959. Foram eles alguns dos participantes na famosa conferência de verão de 1956 em Darthmouth College.[15]

Historicamente, existem dois grandes estilos de investigação em IA: IA "neats" e IA "scruffies". A IA "neats", limpa, clássica ou simbólica. Envolve a manipulação de símbolos e de conceitos abstractos, e é a metodologia utilizada na maior parte dos sistemas periciais.

Paralelamente a esta abordagem existe a abordagem IA "scruffies", ou "coneccionista", da qual as redes neuronais são o melhor exemplo. Esta abordagem cria sistemas que tentam gerar inteligência pela aprendizagem e adaptação em vez da criação de sistemas desenhados com o objectivo especifico de resolver um problema. Ambas as abordagems apareceram num estágio inicial da história de IA. Nos anos 60s e 70s os coneccionistas foram retirados do primeiro plano da investigação em IA, mas o interesse por esta vertente da IA foi retomada nos anos 80s, quando as limitações da IA "limpa" começaram a ser percebidas.

Pesquisas sobre inteligência artificial foram intensamente custeadas na década de 1980 pela Agência de Projetos de Pesquisas Avançadas sobre Defesa (“Defense Advanced Research Projects Agency”), nos Estados Unidos, e pelo Projeto da Quinta Geração (“Fifth Generation Project”), no Japão. O trabalho subsidiado fracassou no sentido de produzir resultados imediatos, a despeito das promessas grandiosas de alguns praticantes de IA, o que levou proporcionalmente a grandes cortes de verbas de agências governamentais no final dos anos 80, e em consequência a um arrefecimento da atividade no setor, fase conhecida como O inverno da IA. No decorrer da década seguinte, muitos pesquisadores de IA mudaram para áreas relacionadas com metas mais modestas, tais como aprendizado de máquinas, robótica e visão computacional, muito embora pesquisas sobre IA pura continuaram em níveis reduzidos.

Abordagens principais
Existem duas abordagens principais para a criação de Sistemas de Inteligência Artificial: O Simbolismo e o Conexionismo.[16]

A primeira, chamada de IA Simbólica,[17] propõe a representação de conhecimento por meio da manipulação de símbolos, isto é, na forma de estruturas construídas por seres humanos, normalmente baseadas em noçoes de Lógica. Ela teve grande impulso durante uma fase onde foram criados muitos Sistemas Especialistas, muitos deles basados em Lógica de Primeira Ordem, implementados em Prolog, ou em linguagens de programação derivadas desta ou especializadas, como CLIPS. Normalmente programas desse tipo têm o conhecimento programado diretamente por seres humanos, o que levou a trabalhos de elicitação de conhecimento. Apesar do sucesso inicial dos Sistemas Especialistas, a grande dificuldade de levantar e registrar conhecimento a partir de humanos e o sucesso dos processos de aprendizado de máquina a partir de dados levou a dimimuição da importância dessa vertente.[17]

A segunda, chamada de IA Conexionista,[18] se baseia em um modelo matemático inspirado no funcionamento dos neurônios,[19] e depende do aprendizado de máquina baseado em grandes massas de dados para calibrar esse modelo, que normalmente começa com parâmetros aleatórios.[20] Essa abordagem, apesar de proposta muito cedo, não encontrou computadores capazes de modelar problemas complexos, apesar de ter sucesso com problemas restritos de reconhecimento de padrão, o que só acontece a partir da década de 2010, com resultados extramemente fortes no final dessa década e no início da década de 2020, a partir de modelos contendo bilhões de parametros, como o GPT-3[21] e conceitos como Redes Neurais Profundas,[22] Transformers,[23] e Atenção.[24]

Em torno de 2022, a maior parte da pesquisa em IA gira em torno dos conceitos de Aprendizado de Máquina e Conexionismo, havendo também propostas para sistemas híbridos.

Definição do termo
A questão sobre o que é "inteligência artificial", mesmo como definida anteriormente, pode ser separada em duas partes: "qual a natureza do artificial" e "o que é inteligência". A primeira questão é de resolução relativamente fácil, apontando no entanto para a questão de o que poderá o homem construir.

A segunda questão seria consideravelmente mais difícil, levantando a questão da consciência, identidade e mente (incluindo a mente inconsciente) juntamente com a questão de que componentes estão envolvidos no único tipo de inteligência que universalmente se aceita como estando ao alcance do nosso estudo: a inteligência do ser humano. O estudo de animais e de sistemas artificiais que não são modelos triviais começa a ser considerado como pauta de estudo na área da inteligência.

Ao conceituar inteligência artificial, presume-se a interação com o ambiente, diante de necessidades reais como relações entre indivíduos semelhantes, a disputa entre indivíduos diferentes, perseguição e fuga; além da comunicação simbólica específica de causa e efeito em diversos níveis de compreensão intuitiva, consciente ou não.

Suponhamos uma competição de cara ou coroa, cujos resultados sejam observados ou não. Se na segunda tentativa der o mesmo resultado que a primeira, então não existiam as mesmas chances para ambas opções iniciais. Claro que a coleta de informação em apenas duas amostragens é confiável apenas porque a quantidade de tentativas é divisível pelo número de opções de resultados prováveis.

A verdade é que o conceito de cara ou coroa está associado a artigos de valor, como moedas e medalhas que podem evitar que as pessoas abandonem o jogo e induza os participantes a acompanhar os resultados até o final. Para manter a disposição do adversário em desafiar a máquina seria necessário aparentar fragilidade e garantir a continuidade da partida. Isso é muito utilizado em máquinas de cassino, sendo que vários apostadores podem ser induzidos a dispensar consideráveis quantias em apostas.

A utilização de uma máquina de resultados pode compensar a ausência de um adversário, mas numa partida de xadrez, por exemplo, para que a máquina não precise armazenar todas as informações que excedem a capacidade de próprio universo imaginável são necessárias fórmulas que possam ser armazenadas para que então sejam calculadas por princípios físicos, lógicos, geométricos, e estatísticos para refletir o sistema completo em cada uma das suas partes; como a integração do Google com Wikipédia, por exemplo.

Uma popular e inicial definição de inteligência artificial, introduzida por John McCarthy na famosa conferência de Dartmouth em 1956 é "fazer a máquina comportar-se de tal forma que seja chamada inteligente caso fosse este o comportamento de um ser humano." No entanto, esta definição parece ignorar a possibilidade de existir a IA forte (ver abaixo).

Outra definição de Inteligência Artificial é a inteligência que surge de um "dispositivo artificial". A maior parte das definições podem ser categorizadas em sistemas que: "pensam como um humano; agem como um humano; pensam racionalmente ou agem racionalmente".[18][25]

Campo de estudo
Os principais pesquisadores e livros didáticos definem o campo como "o estudo e projeto de agentes inteligentes", onde um agente inteligente é um sistema que percebe seu ambiente e toma atitudes que maximizam suas chances de sucesso. Andreas Kaplan e Michael Haenlein definem a inteligência artificial como “uma capacidade do sistema para interpretar corretamente dados externos, aprender a partir desses dados e utilizar essas aprendizagens para atingir objetivos e tarefas específicas através de adaptação flexível”.[26] John McCarthy, quem cunhou o termo em 1956 ("numa conferência de especialistas celebrada em Darmouth Colege" Gubern, Román: O Eros Eletrónico), a define como "a ciência e engenharia de produzir sistemas inteligentes". É uma área de pesquisa da computação dedicada a buscar métodos ou dispositivos computacionais que possuam ou multipliquem a capacidade racional do ser humano de resolver problemas, pensar ou, de forma ampla, ser inteligente. Também pode ser definida como o ramo da ciência da computação que se ocupa do comportamento inteligente[27] ou ainda, o estudo de como fazer os computadores realizarem coisas que, atualmente, os humanos fazem melhor.[10]

Abordagens filosóficas
Não existe uma teoria ou paradigma unificador que orienta a pesquisa de IA. Pesquisadores discordam sobre várias questões.[28] Algumas das perguntas constantes mais longas que ficaram sem resposta são as seguintes: a inteligência artificial deve simular inteligência natural, estudando psicologia ou neurociência? Ou será que a biologia humana é tão irrelevante para a pesquisa de IA como a biologia das aves é para a engenharia aeronáutica? O comportamento inteligente pode ser descrito usando princípios simples e elegantes (como lógica ou otimização)? Ou ela necessariamente requer que se resolva um grande número de problemas completamente não relacionados? A inteligência pode ser reproduzida usando símbolos de alto nível, similares às palavras e ideias? Ou ela requer processamento "sub-simbólico"?[29] John Haugeland, que cunhou o termo GOFAI (Good Old-Fashioned Artificial Intelligence - Boa Inteligência Artificial à Moda Antiga), também propôs que a IA deve ser mais apropriadamente chamada de inteligência sintética, um termo que já foi adotado por alguns pesquisadores não-GOFAI.[30]

Cibernética e simulação cerebral
Nos anos de 1940 e 1950, um número de pesquisadores exploraram a conexão entre neurologia, teoria da informação e cibernética. Alguns deles construíram máquinas que usaram redes eletrônicas para exibir inteligência rudimentar, como as tartarugas de W. Grey Walter e a Besta de Johns Hopkins. Muitos desses pesquisadores se reuniram para encontros da Sociedade teleológica da Universidade de Princeton e o Ratio Club na Inglaterra. Em 1960, esta abordagem foi abandonada, apesar de seus elementos serem revividos na década de 1980.

Sub-simbólica
Inteligência computacional
Interesse em redes neurais e "conexionismo" foi revivida por David Rumelhart e outros em meados de 1980. Estas e outras abordagens sub-simbólicas, como sistemas de fuzzy e computação evolucionária, são agora estudados coletivamente pela disciplina emergente inteligência computacional.

IA forte e IA fraca
Entre os teóricos que estudam o que é possível fazer com a IA existe uma discussão onde se consideram duas propostas básicas: uma conhecida como "forte" e outra conhecida como "fraca". Basicamente, a hipótese da IA forte considera ser possível criar uma máquina consciente, ou seja, afirma que os sistemas artificiais devem replicar a mentalidade humana.[31]

Inteligência artificial forte
A investigação em Inteligência Artificial Forte aborda a criação da forma de inteligência baseada em computador que consiga raciocinar e resolver problemas; uma forma de IA forte é classificada como auto-consciente.

A IA forte é tema bastante controverso, pois envolve temas como consciência e fortes problemas éticos ligados ao que fazer com uma entidade que seja cognitivamente indistinguível de seres humanos.

A ficção científica tratou de muitos problemas desse tipo. Isaac Asimov, por exemplo, escreveu O Homem Bicentenário, onde um robô consciente e inteligente luta para possuir um status semelhante ao de um humano na sociedade.[32] E Steven Spielberg dirigiu "A.I. Inteligência Artificial" onde um garoto-robô procura conquistar o amor de sua "mãe", procurando uma maneira de se tornar real. Por outro lado, o mesmo Asimov reduz os robôs a servos dos seres humanos ao propor as três leis da robótica.[8]

Stephen Hawking alertou sobre os perigos da inteligência artificial e considerou uma ameaça à sobrevivência da humanidade[33] (ver: Rebelião das máquinas).

Inteligência artificial fraca
Trata-se da noção de como lidar com problemas não determinísticos. Uma contribuição prática de Alan Turing foi o que se chamou depois de Teste de Turing (TT),[34] de 1950: em lugar de responder à pergunta "podem-se ter computadores inteligentes?" ele formulou seu teste, que se tornou praticamente o ponto de partida da pesquisa em "Inteligência Artificial".[5]

O teste consiste em se fazer perguntas a uma pessoa e um computador escondidos. Um computador e seus programas passam no TT se, pelas respostas, for impossível a alguém distinguir qual interlocutor é a máquina e qual é a pessoa. No seu artigo original ele fez a previsão de que até 2000 os computadores passariam seu teste.[34] Pois bem, há um concurso anual de programas para o TT, e o resultado dos sistemas ganhadores é tão fraco (o último tem o nome "Ella") que com poucas perguntas logo percebe-se as limitações das respostas da máquina. É interessante notar que tanto a Máquina de Turing quanto o Teste de Turing talvez derivem da visão que Turing tinha de que o ser humano é uma máquina.

Há quem diga que essa visão está absolutamente errada, do ponto de vista linguístico, já que associamos à "máquina" um artefato inventado e eventualmente construído. Dizem eles: "Nenhum ser humano foi inventado ou construído". Afirma-se ainda que a comparação, feita por Turing, entre o homem e a máquina é sinônimo de sua "ingenuidade social", pois as máquinas são infinitamente mais simples do que o homem, apesar de, paradoxalmente, se afirmar que a vida é complexa. No entanto, esta linha de raciocínio é questionável, afinal de contas, os computadores modernos podem ser considerados "complexos" quando comparados ao COLOSSUS (computador cujo desenvolvimento foi liderado por Tommy Flowers, em 1943), ou a qualquer máquina do início do século XX.

A inteligência artificial fraca centra a sua investigação na criação de inteligência artificial que não é capaz de verdadeiramente raciocinar e resolver problemas. Uma tal máquina com esta característica de inteligência agiria como se fosse inteligente, mas não tem autoconsciência ou noção de si. O teste clássico para aferição da inteligência em máquinas é o Teste de Turing.[34]

Em computação, muitos problemas são resolvidos por meio da escrita de um algoritmo que
especifica, passo a passo, como resolver um problema. No entanto, não é fácil escrever um
programa de computador que realize com eficiência algumas tarefas que realizamos com
facilidade no nosso dia a dia. Por exemplo, como reconhecer pessoas pelo rosto ou pela
fala? Que caraterísticas dos rostos ou da fala devem ser consideradas? Como podemos
codificar aspetos como diferentes expressões faciais de uma mesma pessoa, alterações na
face (e.g. óculos, bigode, cortes de cabelo), mudanças na voz (e.g. devido a uma gripe) ou
estados de espírito? No entanto, os seres humanos conseguem realizar estas tarefas com
relativa facilidade. Fazem isso por meio de reconhecimento de padrões , quando aprendem
o que deve ser observado num rosto ou na fala para conseguir identificar pessoas, após
terem tido vários exemplos de rostos ou falas com identificação clara.
Um bom médico consegue diagnosticar se um paciente está com problemas cardíacos,
tendo por base um conjunto de sintomas e de resultados de exames clínicos. Para esse
efeito, o médico utiliza o conhecimento adquirido durante a sua formação e a experiência
proveniente do exercício da profissão. Como escrever um programa de computador que,
dados os sintomas e os resultados de exames clínicos, apresente um diagnóstico que seja
tão bom quanto o de um médico experiente?
Também pode ser difícil escrever um programa que efetue a análise de dados das vendas de uma loja. Para descobrir quantas pessoas fizeram mais de uma compra numa loja
no ano anterior, podem ser facilmente utilizados os Sistemas de Gestão de Bases de Dados
(SGBD). No entanto, como podemos escrever um programa que responda a questões mais
complicadas, como por exemplo:
• Identificar conjuntos de produtos que são frequentemente vendidos em conjunto.
• Recomendar novos produtos a clientes que costumam comprar produtos semelhantes.
• Agrupar os consumidores da loja em grupos de forma a melhorar as operações de
marketing.
Não obstante a dificuldade em escrever um programa de computador que possa lidar
10 EXTRAÇÃO DE CONHECIMENTO DE DADOS
de forma eficiente com estas tarefas, o número de vezes em que tarefas tão complexas
como estas necessitam ser realizadas diariamente é frequente. Além disso, o volume de
informação que precisa ser considerado na sua implementação torna difícil, ou mesmo
impossível, a sua realização.
As técnicas de Inteligência Artificial (IA), em particular de Extração de Conhecimento
de Dados (ECD) ou Aprendizagem Automática, têm sido utilizadas com sucesso num
grande número de problemas reais, incluindo os problemas citados anteriormente.
Este capítulo está organizado da seguinte forma. A Seção 1.1 apresenta a relação entre
ECD e IA, mostrando alguns exemplos da utilização de ECD em problemas reais. Na
Secção 1.2 é introduzida a relação entre um conjunto de dados e a qualidade da hipótese
induzida por um algoritmo de ECD. O conceito de viés indutivo, essencial para que a
aprendizagem ocorra, é discutido na Secção 1.3. A Secção 1.4 descreve as diferentes
tarefas de aprendizagem, que são agrupadas em aprendizagem preditiva e aprendizagem
descritiva. Por fim, a Secção 1.5 apresenta a estrutura dos capítulos do livro.
1.1 Inteligência Artificial e Aprendizagem Automática
Até há alguns anos atrás, a área de IA era vista como uma área teórica, com aplicações
apenas em pequenos problemas curiosos, desafiantes, mas de pouco valor prático. Grande
parte dos problemas práticos que necessitavam de computação eram resolvidos pela codificação, nalguma linguagem de programação, dos passos necessários à respetiva resolução.
A partir da década de 1970, verificou-se uma maior disseminação do uso de técnicas de
computação baseadas em IA para a resolução de problemas reais. Muitas vezes, estes
problemas eram computacionalmente tratados por meio da aquisição de conhecimento de
especialistas do domínio (e.g. especialistas da área da medicina), que era então codificado por regras lógicas, num programa de computador. Estes programas eram conhecidos
como Sistemas Especialistas ou Sistemas Baseados em Conhecimento. O processo de
aquisição do conhecimento envolvia entrevistas com os especialistas para descobrir as regras utilizadas na tomada de decisão. Esse processo possuía várias limitações, tais como:
subjetividade, decorrente do fato dos especialistas sustentarem, com frequência, a tomada
de decisão na sua intuição; e dificuldade em verbalizar e exteriorizar esse conhecimento.
Nas últimas décadas, a crescente complexidade dos problemas a serem tratados computacionalmenteeovolume de dados gerados em diferentes setores, reforçou a necessidade de desenvolvimento de ferramentas computacionais mais sofisticadas e autónomas,
que reduzissem a necessidade de intervenção humana e a dependência de especialistas.
Para alcançar estes objetivos, as técnicas desenvolvidas devem ser capazes de criar, de
forma autónoma e a partir da experiência passada, uma hipótese, ou função, capaz de
resolver o problema que se deseja tratar. Um exemplo simples é a descoberta de uma hipótese, na forma de uma regra ou conjunto de regras, para definir quais os clientes de um
supermercado que devem receber publicidade de um novo produto, utilizando os dados
de compras anteriores dos clientes registados na base de dados do supermercado. A este
processo de indução de uma hipótese (ou aproximação de função) a partir da experiên-
INTRODUÇÃO 11
cia passada, dá-se o nome de Aprendizagem Automática ou Extração de Conhecimento de
Dados (ECD).
A capacidade de aprendizagem é considerada essencial para um comportamento inteligente. Atividades como memorizar, observar e explorar situações para aprender fatos,
melhorar habilidades motoras/cognitivas através da prática, organizar conhecimento novo
e utilizar representações apropriadas, podem ser consideradas atividades relacionadas com
aprendizagem. Existem várias definições de ECD na literatura. Uma delas, apresentada
em Mitchell (1997), define ECD como:
’A capacidade de melhorar o desempenho na realização de alguma tarefa por
meio da experiência.’
Em ECD, os computadores são programados para aprender com a experiência passada.
Para tal, empregam um princípio de inferência denominado indução, no qual se obtêm
conclusões genéricas a partir de um conjunto particular de exemplos. Assim, os algoritmos
de ECD aprendem a induzir uma função, ou hipótese, capaz de resolver um problema,
a partir de dados que representam instâncias do problema a ser resolvido. Estes dados
formam um conjunto, simplesmente denominado conjunto de dados (Seção 1.2). Embora
ECD esteja naturalmente associada à IA, outras áreas de investigação são importantes e
têm contribuições diretas e significativas no avanço da ECD, tais como, Probabilidade
e Estatística, Teoria da Computação, Neurociência, Teoria da Informação, entre outras.
ECD é uma das áreas de investigação da computação que mais tem crescido nos últimos
anos. Diferentes algoritmos de ECD, diferentes formas de utilizar os algoritmos existentes
e adaptações de algoritmos são continuamente propostos. Além disso, em cada instante
surgem novas variações nas caraterísticas dos problemas reais a serem tratados.
Existem várias aplicações bem-sucedidas de técnicas de ECD na resolução de problemas reais, entre as quais podem ser citadas:
• Reconhecimento de voz - palavras faladas;
• Predição de taxas de cura de pacientes com diferentes doenças;
• Deteção do uso fraudulento de cartões de crédito;
• Condução autónoma de automóveis;
• Ferramentas que jogam gamão e xadrez ao nível de campeões;
• Diagnóstico de cancro por meio da análise de dados de expressão genética.
Além do enorme volume de aplicações que beneficiam das caraterísticas da área de
ECD, outros fatores têm favorecido a expansão desta área, nomeadamente, o desenvolvimento de algoritmos cada vez mais eficazes e eficientes, e a elevada capacidade dos
recursos computacionais atualmente disponíveis. Outras motivações para a investigação
em ECD incluem a possibilidade de aumentar a compreensão de como se processa a aprendizagem nos seres vivos. Além disso, algumas tarefas são naturalmente melhor definidas
por meio de exemplos. Os modelos gerados são, ainda, capazes de lidar com situações não
apresentadas durante o seu desenvolvimento, sem necessariamente necessitar de uma nova
fase de projeto.
12 EXTRAÇÃO DE CONHECIMENTO DE DADOS
1.2 Indução de Hipóteses
Para caracterizar um conjunto de dados, vamos considerar a informação sobre os doentes
de um hospital. Neste conjunto, cada observação corresponde a um doente. Cada observação, também denominada objeto, exemplo ou registo, é uma tupla formada pelos valores
das caraterísticas que descrevem os principais aspetos desse doente. Essas características
são designadas por atributos ou variáveis independentes). A título de exemplo, os atributos
de um doente podem ser: a sua identificação, o nome, a idade, o género, o estado civil,
os sintomas e resultados de exames clínicos. Exemplos de sintomas podem ser presença e
distribuição de manchas na pele, o peso e a temperatura do corpo.
Conforme será visto mais adiante, em algumas tarefas de extração de conhecimento,
um dos atributos é considerado o atributo de saída (também denominado atributo alvo ou
variável dependente), cujos valores podem ser estimados utilizando os valores dos demais
atributos, denominados atributos de entrada, ou atributos previsores. O objetivo de um
algoritmo de ECD utilizado nestas tarefas é aprender, a partir de um subconjunto dos
dados, denominado conjunto de treino, um modelo ou hipótese capaz de relacionar os
valores dos atributos de entrada de um objeto com o valor do seu atributo de saída.
Um requisito importante para os algoritmos de ECD é a capacidade de lidar com dados
imperfeitos. Muitos conjuntos de dados apresentam algum tipo de problema, como presença de ruído, dados inconsistentes, dados em falta e dados redundantes. Idealmente, os
algoritmos de ECD devem ser robustos a estes problemas, minimizando a sua influência no
processo de indução de hipóteses. Porém, dependendo da sua extensão, estes problemas
podem prejudicar o processo indutivo. Por esse motivo, técnicas de pré-processamento são
frequentemente utilizadas na identificação e minimização da ocorrência desses problemas.
Retomando o exemplo dos pacientes, considere a situação em que um algoritmo de
ECD é utilizado para aprender uma hipótese (por exemplo, uma regra) capaz de diagnosticar pacientes de acordo com os valores associados aos seus atributos de entrada. Os
atributos referentes à identificação e nome do paciente não são considerados entradas relevantes, uma vez que não possuem qualquer tipo de relação com o diagnóstico de uma
doença. Na verdade, o que se pretende, é induzir uma hipótese capaz de realizar diagnósticos corretos para novos pacientes, i.e. pacientes diferentes daqueles que foram utilizados
na aprendizagem da regra de decisão. Assim, uma vez induzida uma hipótese, é desejável
que esta também seja válida para outros objetos do mesmo domínio, ou problema, que não
fazem parte do conjunto de treino. A esta propriedade de uma hipótese continuar a ser
válida para novos objetos dá-se o nome de capacidade de generalização da hipótese. Para
que uma hipótese se revista de utilidade quando aplicada a novos dados, é fundamental
que apresente uma boa capacidade de generalização.
Quando uma hipótese apresenta uma capacidade de generalização reduzida, pode ser
porque esta se encontra superajustada aos dados (overfitting). Neste caso, diz-se que a
hipótese memorizou, ou especializou-se nos dados de treino. No caso inverso, o algoritmo de ECD pode induzir hipóteses que apresentem uma baixa taxa de acerto mesmo no
conjunto de treino, gerando uma condição de subajustamento (underfitting). Esta situação
INTRODUÇÃO 13
pode ocorrer, por exemplo, quando os exemplos de treino disponíveis são pouco representativos, ou o modelo usado é muito simples e, por conseguinte, incapaz de capturar os
padrões existentes nos dados (Monard e Baranauskas, 2003). Estes conceitos são ilustrados e novamente discutidos na Secção 7.2.1. São feitas então considerações e motivações
sobre a escolha de modelos com boa capacidade de generalização.
1.3 Viés Indutivo
Quando um algoritmo de ECD aprende a partir de um conjunto de dados de treino, procura
uma hipótese, no espaço de hipóteses possíveis, que seja capaz de descrever as relações
entre os objetos, e que melhor se ajuste aos dados de treino.
Cada algoritmo utiliza uma forma, ou representação, para descrever a hipótese induzida. Por exemplo, as redes neuronais artificiais representam uma hipótese por um conjunto de valores reais, associados aos pesos das conexões da rede. As árvores de decisão
utilizam uma estrutura de árvore em que cada nó interno é representado por uma pergunta
referente ao valor de um atributo e cada nó externo está associado a uma classe. A representação utilizada define a preferência ou viés (bias) de representação do algoritmo e
pode restringir o conjunto de hipóteses que podem ser induzidas pelo algoritmo. A Figura
1.1 ilustra o viés de representação utilizado por técnicas de indução de árvores de decisão,
redes neuronais artificiais e regras de decisão.
<
M F
Doente Saudável Doente
>–
Sexo
Peso
50 50
(a) Árvore de decisão
Se Peso 50 então Doente
Se Peso < 50 e Sexo = M então Doente
Se Peso < 50 e Sexo = F então Saudável
–
>
(b) Conjunto de regras
0,45 –0,40 0,54 0,12 0,98 0,37
–0,45 0,11 0,91 0,34 –0,20 0,83
–0,29 0,32 –0,25 –0,51 0,41 0,70
(c) Redes neuronais
Figura 1.1 Diferentes vieses de representação.
Além do viés de representação, os algoritmos de ECD possuem também um viés de
procura. O viés de procura de um algoritmo é a forma como o algoritmo procura a hipótese
que melhor se ajusta aos dados de treino. Este viés define como as hipóteses são procuradas no espaço de hipóteses. Por exemplo, o algoritmo ID3, que é utilizado na indução
de árvores de decisão, tem como viés de procura a preferência por árvores de decisão com
poucos nós, conforme será apresentado no Capítulo 6.
Assim, cada algoritmo de ECD possui dois vieses: um viés de representação e um
viés de procura. O viés é necessário para restringir as hipóteses a serem visitadas no
14 EXTRAÇÃO DE CONHECIMENTO DE DADOS
espaço de procura. Sem viés não haveria aprendizagem/generalização. Os modelos seriam
especializados para os exemplos individuais. Embora, à primeira vista, o viés pareça ser
uma limitação dos algoritmos de ECD, segundo Mitchell (1997), sem viés um algoritmo de
ECD não consegue generalizar o conhecimento adquirido durante o treino para aplicá-lo
com sucesso a novos dados.
1.4 Tarefas de aprendizagem
Os algoritmos de ECD têm sido amplamente utilizados em diversas tarefas, que podem
ser organizadas de acordo com diferentes critérios. Um deles diz respeito ao paradigma
de aprendizagem a ser adotado para lidar com a tarefa. De acordo com esse critério, as
tarefas de aprendizagem podem ser divididas em: Preditivas e Descritivas.
Em tarefas de previsão, o objetivo consiste em encontrar uma função (também denominada de modelo, ou hipótese), a partir dos dados de treino, que possa ser utilizada para
prever um rótulo, ou valor, que caraterize um novo exemplo, com base nos valores dos
seus atributos de entrada. Para esse efeito, é necessário que cada objeto do conjunto de
treino possua atributos de entrada e de saída.
Os algoritmos, ou métodos, de ECD utilizados nesta tarefa induzem modelos preditivos. Estes algoritmos seguem o paradigma da aprendizagem supervisionada. O termo
supervisionada vem da simulação da presença de um supervisor externo, que conhece a
saída (rótulo) associada a cada exemplo (conjunto de valores para os atributos de entrada).
Com base neste conhecimento, o supervisor externo pode avaliar a capacidade da hipótese
induzida em predizer o valor de saída para novos exemplos.
Em tarefas de descrição, o objetivo consiste em explorar, ou descrever, um conjunto
de dados. Os algoritmos de ECD utilizados nestas tarefas ignoram o atributo de saída.
Por esse motivo, diz-se que estes algoritmos seguem o paradigma de aprendizagem não
supervisionada. Por exemplo, uma tarefa descritiva de agrupamento de dados tem por meta
encontrar grupos de objetos semelhantes no conjunto de dados. Outra tarefa descritiva
consiste em encontrar regras de associação que relacionam um grupo de atributos com
outro grupo de atributos.
A Figura 1.2 apresenta uma hierarquia de aprendizagem, de acordo com os tipos de
tarefas de aprendizagem. No topo aparece a aprendizagem indutiva, processo pelo qual
são realizadas as generalizações a partir dos dados. Em seguida, surgem os tipos de aprendizagem supervisionada (preditivo) e não supervisionada (descritivo). As tarefas supervisionadas distinguem-se pelo tipo dos rótulos dos dados: discreto, no caso de classificação;
e contínuo, no caso de regressão. As tarefas descritivas são genericamente divididas em:
agrupamento, em que os dados são agrupados de acordo com sua semelhança; sumarização, cujo objetivo é encontrar uma descrição simples e compacta de um conjunto de
dados; e associação, que consiste em encontrar padrões frequentes de associações entre os
atributos de um conjunto de dados. Com exceção da sumarização, as demais tarefas serão
descritas neste livro.
Note-se que, apesar desta divisão básica de modelos em preditivos e descritivos, um
INTRODUÇÃO 15
Figura 1.2 Hierarquia de aprendizagem.
modelo preditivo também providencia uma descrição compacta de um conjunto de dados,
e um modelo descritivo pode efetuar previsões após ser validado.
Uma tarefa de aprendizagem que não se enquadra nas tarefas anteriores é a de aprendizagem por reforço. Nesta tarefa, o objetivo é reforçar, ou recompensar, uma ação considerada positiva, e punir uma ação considerada negativa. Um exemplo de tarefa de reforço
é ensinar um robô a encontrar a melhor trajetória entre dois pontos. Os algoritmos de
aprendizagem utilizados nesta tarefa, em geral, punem a passagem por caminhos pouco
promissores, e recompensam a passagem por caminhos promissores. Devido ao foco adotado para este livro, esta tarefa não será abordada.
1.5 Estrutura do Livro
Este livro tem por objetivo apresentar os principais conceitos e algoritmos de ECD e mostrar como ECD pode ser utilizada para na resolução de problemas reais. Para esse efeito,
serão cobertos tanto temas tradicionais como resultados de pesquisas recentes na área.
De forma a agrupar os temas cobertos de uma maneira mais uniforme, os capítulos do
livro foram organizados em três grandes temas ou módulos:
• Preparação de dados: engloba tópicos de descrição dos dados, análise estatística
de dados e pré-processamento de dados.
• Métodos preditivos: este módulo está relacionado com o paradigma da aprendizagem supervisionada e, após definir os conceitos gerais referentes a este tema,
descreve os principais algoritmos de aprendizagem preditiva, explica como as hipóteses podem ser combinadas formando comités, introduz possíveis estratégias para
planear experiências com esses métodos, e descreve as principais métricas utilizadas
na avaliação do seu desempenho.
16 EXTRAÇÃO DE CONHECIMENTO DE DADOS
• Modelos descritivos: este módulo foca a aprendizagem não supervisionada. São
abordados os temas de padrões frequentes e análise de agrupamentos. Descrevemos
os conceitos básicos, os algoritmos principais, e as formas de combinação. É também discutido como as experiências utilizando estes métodos podem ser planeados
e avaliados.
• Tópicos avançados: inclui temas de investigação recente na área de ECD. Os temas considerados são: fluxos de dados, meta-aprendizagem, estratégias para classificação multiclasse, classificação hierárquica, classificação multirótulo e análise de
redes sociais.
Estes tópicos foram cuidadosamente escolhidos, de modo a que os leitores tenham
acesso a uma dose equilibrada entre abrangência e profundidade dos temas básicos e avançados nas áreas de Inteligência Artificial, que utilizam aprendizagem automática na indução de modelos de decisão. Esperamos que este livro, ao mesmo tempo que introduz o
leitor aos principais aspetos de ECD e a temas de investigação recentes, sirva de alicerce
à realização de investigação que promova o crescimento e o fortalecimento da área. Esperamos ainda que o livro estimule o leitor a utilizar as várias técnicas aqui abordadas na
resolução de problemas reais.
Parte I
Preparação de Dados

Introdução
Todos os dias é gerada uma enorme quantidade de dados. Estima-se que, a cada 20 meses,
a quantidade de dados armazenada em todas as bases de dados do mundo duplica (Witten
et al., 2011). Estes dados são gerados por atividades como transações financeiras, monitorização ambiental, obtenção de dados clínicos e genéticos, captura de imagens, tráfego na
internet, entre outras. Os dados podem, ainda, assumir vários formatos diferentes, como
séries temporais, conjuntos de produtos em transações, grafos ou redes sociais, textos, páginas web, imagens (vídeos) e áudio (músicas). Com o aumento crescente da quantidade
de dados gerada, o fosso entre a quantidade de dados existente e a porção de dados que é
analisada e compreendida tem-se acentuado de forma significativa ao longo do tempo.
Conjuntos de dados são formados por objetos que podem representar um objeto físico,
como uma cadeira, ou uma noção abstrata, como os sintomas apresentados por um paciente
que se dirige a um hospital. Em geral, cada objeto é descrito por um conjunto de atributos
de entrada, ou vetor de caraterísticas. Cada objeto corresponde a uma ocorrência dos
dados. Cada atributo está associado a uma propriedade do objeto.
Formalmente, um conjunto de dados pode ser representado por uma matriz de objetos
Xn×d, em que n é o número de objetos e d é o número de atributos de entrada de cada
objeto. O valor de d define a dimensionalidade dos objetos, ou do espaço de objetos (também denominado espaço de entradas, ou espaço de atributos). Cada elemento dessa matriz,
x j
i , contém o valor da j-ésima caraterística para o i-ésimo objeto. Os d atributos também
podem ser vistos como um conjunto de eixos ortogonais e os objetos, como pontos no
espaço de dimensão d, também designado por espaço de objetos. A Figura 1.3 ilustra um
exemplo de um espaço de objetos. Nesse espaço, a posição de cada objeto é definida pelos
valores de dois atributos de entrada (d = 2) que, neste caso, representam os resultados de
dois exames clínicos. O atributo de saída é representado pelo formato do objeto na figura:
círculo para pacientes doentes e triângulo para pacientes saudáveis.
Apesar do crescente número de bases de dados disponíveis, na maioria das vezes não
é possível utilizar diretamente algoritmos de ECD sobre esses dados. Técnicas de préprocessamento são frequentemente utilizadas para tornar os conjuntos de dados mais adequados para o uso de algoritmos de ECD. Essas técnicas podem ser agrupadas nas seguintes tarefas: Integração de dados, Amostragem de dados, Balanceamento de dados,
Limpeza de dados, Redução de dimensionalidade, e Transformação de dados.
20 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Exame 1
Classe
saudável
Classe
doente
Exame 2
Figura 1.3 Objetos no espaço definido pelos atributos.
Grande parte das empresas, órgãos do governo e outras organizações possuem os seus
dados armazenados em bases de dados. Assim, os dados podem ser oriundos de mais do
que uma fonte, ou tabela atributo-valor. Quando os dados presentes em diferentes conjuntos precisam ser utilizados por um algoritmo de ECD, esses conjuntos devem ser integrados de forma a constituir uma única tabela. Essa integração pode levar a inconsistências
e redundâncias. Os algoritmos de ECD também podem apresentar dificuldades quando
precisam de lidar com um grande volume de dados. Essa grande quantidade pode estar
relacionada com o número de objetos, com o número de atributos, ou ambos. Problemas
como a redundância e a inconsistência, estão muitas vezes relacionados com a quantidade
de dados. Técnicas de amostragem e de seleção de atributos têm sido empregues para amenizar estes problemas. Em dados reais, a distribuição dos objetos entre as classes pode não
ser uniforme. Por conseguinte, algumas classes podem ter um número de objetos muito
superior a outras, formando um conjunto de dados desbalanceado. Alguns algoritmos de
ECD têm dificuldade em induzir um bom modelo a partir de conjuntos desbalanceados.
Muitos dos conjuntos de dados reais apresentam problemas, tais como, a presença de ruído
e dados incompletos e/ou inconsistentes. Os dados podem estar incompletos devido à ausência de valores. Os dados podem ser inconsistentes por causa de erros na sua geração,
captação ou entrada. O desempenho da maioria dos algoritmos de ECD é afetado pela presença destes problemas. Para lidar com eles, diversas técnicas para limpeza de dados têm
sido propostas e investigadas na literatura de ECD. Mesmo após a eliminação de atributos
por especialistas no domínio, os atributos que restam podem dificultar a tarefa de algoritmos de ECD, devido a diversos motivos, como sejam a presença de um número muito
grande de atributos, de atributos redundantes, irrelevantes e/ou inconsistentes.
Vários algoritmos de ECD têm dificuldade em utilizar os dados no seu formato original. Para tratar este problema, são efetuadas transformações aos dados originais antes
destes serem utilizados pelo algoritmo. Um exemplo de transformação é a conversão de
valores simbólicos em valores numéricos.
Capítulo 2
Análise Exploratória de Dados
A análise das caraterísticas presentes num conjunto de dados permite a descoberta de
padrões e tendências que podem fornecer informações valiosas que ajudem a compreender
o processo que gera os dados. Muitas dessas caraterísticas podem ser obtidas por meio da
aplicação de fórmulas estatísticas simples. Outras podem ser observadas por meio do uso
de técnicas de visualização. Neste capítulo são descritas as principais caraterísticas para
a análise e compreensão de um conjunto de dados utilizados em experiências de ECD.
Analisa-se a forma como os dados podem estar organizados e os tipos de valores que estes
podem assumir. Serão apresentados ainda vários tipos de gráficos que facilitam a análise
visual da distribuição dos valores em conjuntos de dados com uma ou mais variáveis. Este
capítulo está organizado da seguinte maneira. A Seção 2.1 descreve como os atributos
de um conjunto de dados podem ser caraterizados pelo seu tipo e escala. Por fim, na
Seção 2.2, são apresentadas várias medidas, assim como gráficos, que permitem descrever
conjuntos de dados, tanto univariados como multivariados.
2.1 Caraterização de Dados
Os conjuntos de dados são formados por objetos que podem representar um objeto físico,
como uma cadeira, ou uma noção abstrata, como os sintomas apresentados por um paciente
que se dirige a um hospital. Estes objetos são também designados por instâncias, objetos,
registos ou exemplos. Em geral, são representados por um vetor de caraterísticas que os
descreve, também denominadas atributos, campos ou variáveis. Cada objeto corresponde
a uma linha na tabela de dados. Cada atributo está associado a uma propriedade do objeto.
Formalmente, os dados podem ser representados por uma matriz de objetos Xn×d, em
que n é o número de objetos e d é o número de atributos de entrada de cada objeto. O valor
de d define a dimensionalidade dos objetos ou do espaço de objetos. Cada elemento dessa
matriz, x j
i , contém o valor da j-ésima caraterística para o i-ésimo objeto. Os d atributos
também podem ser interpretados como um conjunto de eixos ortogonais, e os objetos como
pontos no espaço de dimensão d, denominado espaço de objetos.
Para ilustrar os conceitos abordados neste capítulo com um exemplo prático, considere
22 EXTRAÇÃO DE CONHECIMENTO DE DADOS
novamente o conjunto de dados provenientes de pacientes de um hospital, denominado
hospital, ilustrado pela Tabela 2.1. Este conjunto foi inicialmente apresentado no Capítulo 1. No conjunto hospital, cada objeto corresponde a um paciente, sendo por isso
formado pelos valores de atributos de entrada (também denominados atributos preditivos)
referentes ao paciente. Esses atributos são: identificação, nome, idade, sexo, sintomas
e resultados de exames clínicos. Exemplos de sintomas são presença e distribuição de
manchas na pele, peso do paciente e temperatura do seu corpo. Além destes atributos,
a tabela apresenta um atributo alvo, também denominado atributo meta ou de saída, que
representa o fenómeno de interesse sobre o qual se pretende fazer previsões. Em tarefas
descritivas, os dados não apresentam este atributo e, muitas vezes, a definição deste tipo de
atributo pode ser obtida como um dos seus resultados. Por outro lado, as tarefas preditivas
baseiam-se na presença deste atributo, como mencionado no Capítulo 1. Na maioria dos
casos, os dados apresentam apenas um atributo alvo 1.
Quando um atributo alvo contém rótulos que identificam categorias ou classes às quais
os objetos pertencem, é denominado classe e assume valores discretos 1,...,k. Nestes casos, estamos perante um problema de classificação. Se o número de objetos por classe for
diferente, a classe mais frequente é denominada classe maioritária, e a menos frequente,
minoritária. Por outro lado, se o atributo alvo é descrito por valores numéricos contínuos, estamos perante um problema de regressão (Mitchell, 1997). Um caso especial de
regressão é a previsão de valores em séries temporais, que se carateriza pelo fato de os
seus valores apresentarem uma relação de periodicidade. Quer em problemas de classificação quer em problemas de regressão, os restantes atributos são denominados atributos
preditivos, por poderem ser utilizados na previsão do valor do atributo alvo.
Na Tabela 2.1, para cada paciente são apresentados os valores dos atributos Id. (identificação do paciente), Nome, Idade, Sexo, Peso, Manchas (presença e distribuição de manchas no corpo), Temp. (temperatura do corpo), #Int. (número de internamentos), Est.
(estado de origem) e Diagnóstico, que indica o diagnóstico do paciente e corresponde ao
atributo alvo. As tabelas com este formato também são denominadas por tabelas atributovalor.
O domínio de um atributo, ou seja os possíveis valores que um atributo pode assumir,
determina o tipo de análise que podemos efetuar. Neste livro, consideramos dois aspetos: tipo e escala. O tipo de um atributo diz respeito ao grau de quantificação nos dados,
e a escala indica a significância relativa dos seus valores. Conhecer o tipo e a escala dos
atributos permite identificar a forma adequada para a preparação dos dados e posterior modelação. As definições que apresentamos são utilizadas para classificar os valores que os
atributos podem assumir no que diz respeito aos dois aspetos mencionados (Jain e Dubes,
1988; Barbara, 2000; Yang et al., 2005).
1Resultados mais recentes consideram dados com mais de um atributo alvo. Este é o foco, por exemplo, da
classificação multirótulo.
ANÁLISE EXPLORATÓRIA DE DADOS 23
Tabela 2.1 Conjunto de dados hospital com seus atributos
Id. Nome Idade Sexo Peso Manchas Temp. # Int. Est. Diagnóstico
4201 João 28 M 79 Concentradas 38,0 2 SP Doente
3217 Maria 18 F 67 Inexistentes 39,5 4 MG Doente
4039 Luiz 49 M 92 Espalhadas 38,0 2 RS Saudável
1920 José 18 M 43 Inexistentes 38,5 8 MG Doente
4340 Cláudia 21 F 52 Uniformes 37,6 1 PE Saudável
2301 Ana 22 F 72 Inexistentes 38,0 3 RJ Doente
1322 Marta 19 F 87 Espalhadas 39,0 6 ECD Doente
3027 Paulo 34 M 67 Uniformes 38,4 2 GO Saudável
2.1.1 Tipo
O tipo define se o atributo representa quantidades, sendo então denominado quantitativo ou
numérico, ou qualidades, sendo então designado de qualitativo, simbólico ou categórico,
pois os seus valores podem ser associados a categorias. Exemplos de conjuntos de valores qualitativos são {pequeno, médio, grande}e{matemática, física, química}. Apesar
de alguns atributos qualitativos poderem ter os respetivos valores ordenados, não podem
ser aplicadas operações aritméticas. Os atributos quantitativos são numéricos, como no
conjunto de valores {23,45,12}. Os valores de um atributo quantitativo são ordenados e
podem ser utilizados em operações aritméticas. Os valores quantitativos podem ser ainda
contínuos ou discretos.
Os atributos contínuos podem assumir um número infinito de valores. Geralmente
esses atributos são resultados de medidas, sendo os seus valores representados por números
reais. No entanto, deve-se timar em consideração que em computadores digitais a precisão
para valores reais é, geralmente, limitada. Exemplos de atributos contínuos são atributos
que representam pesos, tamanhos ou distâncias.
Os atributos discretos contêm um número finito ou infinito contável de valores. Um
caso especial de atributos discretos são os atributos binários (ou booleanos), que apresentam apenas dois valores, como 0/1, sim/não, ausência/presença e verdadeiro/falso. Para
efeitos ilustrativos, na Tabela 2.2 é apresentada a classificação por tipo dos atributos presentes no conjunto de dados da Tabela 2.1.
É importante observar que uma medida quantitativa possui, além do valor numérico,
uma unidade, por exemplo, metro. No processo de extração de conhecimento, se o atributo
altura assume o valor 100, o valor em si não indica se a altura é medida em centímetros,
metros ou jardas, e essa informação pode ser importante na avaliação do conhecimento
adquirido.
Os atributos quantitativos ou numéricos podem assumir valores binários, inteiros ou
reais. Por outro lado, atributos qualitativos são, geralmente, representados por um número
finito de símbolos ou nomes. Em alguns casos, os atributos categóricos são representados
por números. No entanto, estes números não representam quantidades pelo que não são
passíveis de serem submetidos a operações aritméticas. Por exemplo, qual seria o sentido
24 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Tabela 2.2 Tipo dos atributos do conjunto hospital
Atributo Classificação
Id. Qualitativo
Nome Qualitativo
Idade Quantitativo discreto
Sexo Qualitativo
Peso Quantitativo contínuo
Manchas Qualitativo
Temp. Quantitativo contínuo
#Int. Quantitativo discreto
Est. Qualitativo
Diagnóstico Qualitativo
de calcular a média dos valores de um atributo categórico representando o número de
identificação de um paciente?
2.1.2 Escala
A escala define as operações que podem ser realizadas sobre os valores do atributo. Em
relação à escala, os atributos podem ser classificados como nominais, ordinais, intervalares e racionais. Os dois primeiros são do tipo qualitativo e os dois últimos são do tipo
quantitativo. Estas quatro escalas são seguidamente definidas em detalhe.
Na escala nominal, os valores consistem apenas nomes diferentes, carregando a menor
quantidade de informação possível, não existindo uma relação de ordem entre os seus
valores. Consequentemente, as operações mais utilizadas na manipulação dos seus valores
são as de igualdade e desigualdade entre valores. Por exemplo, se o atributo representa
continentes do planeta, apenas é possível verificar se dois valores são iguais ou diferentes
(a não ser que se pretenda ordenar os continentes por ordem alfabética, mas nesse caso
o atributo seria do tipo ordinal). São exemplos de atributos com escala nominal: nome
do paciente, RG, CPF, número da conta no banco, CEP, cores (com as categorias verde,
amarelo, branco etc.) e sexo (com as categorias feminino e masculino).
Os valores numa escala ordinal refletem também uma ordem das categorias representadas. Dessa forma, além dos operadores anteriores, podem também ser utilizados operadores como <, >, ≤, ≥. Por exemplo, quando um atributo categórico possui como domínio
o conjunto pequeno, médio e grande, é possível definir uma relação de ordem, ou seja
indicar se um valor é igual, maior ou menor que outro. Exemplos de atributos com escala
ordinal incluem a hierarquia militar e avaliações qualitativas de temperatura, como frio,
morno e quente.
Na escala intervalar, os atributos são representados por números que variam dentro de
um intervalo. Assim, é possível definir tanto a ordem como a diferença em magnitude
entre dois valores. A diferença em magnitude indica a distância que separa dois valores no
ANÁLISE EXPLORATÓRIA DE DADOS 25
Tabela 2.3 Escala dos atributos do conjunto hospital
Atributo Classificação
Id. Nominal
Nome Nominal
Idade Racional
Sexo Nominal
Peso Racional
Manchas Nominal
Temp. Intervalar
#Int. Racional
Est. Nominal
Diagnóstico Nominal
intervalo de possíveis valores. O valor zero não tem o mesmo significado que o zero utilizado em operações aritméticas. Por exemplo, considere a escala de temperatura medida
em graus Celsius. Se o instituto de previsão do tempo em dias de verão, para uma dada cidade, informa que a temperatura vai variar entre 26 e 34 graus e em dias de inverno, para a
mesma cidade, informa que vai variar entre 13 e 21 graus, temos que a temperatura dessa
cidade apresenta uma variação de 8 graus entre a mínima e a máxima, independente da
estação do ano. por outro lado, 90 graus Celsius é diferente de 90 graus Fahrenheit, apesar
de ambos os valores se referirem ao atributo temperatura, do mesmo modo que não é possível afirmar que a cidade é duas vezes mais quente no verão do que no inverno. Não faz
sentido, para este tipo de atributo, utilizar como informação o quociente entre dois valores.
Isto ocorre porque o ponto em que o atributo assume o valor 0, denominado de ponto zero
ou origem da escala, é definido de forma arbitrária. Esta é uma caraterística dos atributos
intervalares. Este problema seria eliminado caso fosse utilizada a escala de temperatura
Kelvin, cujo valor do ponto zero é o ponto zero verdadeiro (Pyle, 1999). Outros exemplos
são a duração de um evento em minutos e datas num calendário.
Os atributos com escala racional são os que contêm mais informação. Os números têm
um significado absoluto, ou seja, existe um zero absoluto juntamente com uma unidade
de medida que confere significado ao quociente. Por exemplo, considerando o número de
vezes que uma pessoa foi ao hospital, o ponto zero está associado à ausência de visitas. Se
um paciente foi internado duas vezes e outro foi internado oito vezes, é correto afirmar que
o segundo paciente esteve internado quatro vezes mais que o primeiro. Ou seja, faz sentido,
para estes atributos, utilizar a razão entre dois valores. Outros exemplos de atributos com
escala de razão incluem o tamanho, a distância e os valores financeiros, tais como salário
ou saldo em conta-corrente.
Tendo por base a descrição anterior, os atributos do conjunto de dados da Tabela 2.1
podem ser classificados como indicado na Tabela 2.3.
Recentemente, têm sido adotados tipos mais complexos de atributos, como os atributos
26 EXTRAÇÃO DE CONHECIMENTO DE DADOS
hierárquicos, cujos valores representam uma hierarquia de valores, ou atributos simbólicos, cujos valores podem assumir intervalos ou histogramas.
2.2 Exploração de Dados
A informação obtida na análise exploratória de dados pode ajudar, por exemplo, na seleção
das técnicas mais apropriadas para o pré-processamento dos dados assim como para a
construção de modelos preditivos. Uma das formas mais simples de explorar um conjunto
de dados consiste em extrair estatísticas descritivas. A estatística descritiva resume de
forma quantitativa as principais caraterísticas de um conjunto de dados. Muitas dessas
medidas são calculadas rapidamente. Por exemplo, no conjunto de dados de pacientes,
duas medidas estatísticas podem ser facilmente calculadas: a idade média dos pacientes e
a percentagem de pacientes do sexo masculino.
A estatística descritiva assume que os dados são gerados por um processo estatístico.
Como o processo pode ser caraterizado por vários parâmetros, as medidas podem ser interpretadas como estimativas dos parâmetros estatísticos da distribuição que gerou os dados.
Por exemplo, os dados podem ter sido gerados por uma distribuição normal com média 0
e variância 1. Estas medidas permitem capturar informações como:
• Frequência;
• Localização ou tendência central (por exemplo, a média);
• Dispersão (por exemplo, o desvio padrão);
• Distribuição ou formato.
A medida mais simples é a frequência, que mede a proporção de vezes que um atributo
assume um dado valor num determinado conjunto de dados. Pode ser aplicada quer a
valores quer numéricos quer a valores simbólicos. Um exemplo da sua utilização seria:
Num conjunto de dados médicos, 40% dos pacientes têm febre.
As outras medidas diferem nos casos em que os dados apresentam apenas um atributo
(dados univariados) ou mais do que um atributo (dados multivariados), sendo geralmente
aplicadas a dados numéricos. Apesar da maioria dos conjuntos de dados utilizados em
ECD apresentar mais de um atributo, as análises realizadas para cada atributo podem oferecer informações valiosas sobre os dados.
Seguidamente serão apresentadas as principais medidas de centralidade, dispersão e
distribuição para dados uni e multivariados.
2.2.1 Dados Univariados
Para os dados univariados, um objeto xi é descrito por apenas um atributo. Um conjunto
com n objetos pode ser então representado por xj = {x1,x2,...,xn}, em que cada xi é representado por um único valor. É importante observar que o termo conjunto não tem o
ANÁLISE EXPLORATÓRIA DE DADOS 27
mesmo significado do termo utilizado em teoria dos conjuntos. Num conjunto de dados, o
mesmo valor pode surgir mais do que uma vez num mesmo atributo.
Medidas de Centralidade
As medidas de centralidade definem pontos de referência nos dados e variam para dados
numéricos e simbólicos. Para dados simbólicos, geralmente utiliza-se a moda, que corresponde ao valor encontrado com maior frequência para um dado atributo. Por exemplo, a
moda do atributo manchas na Tabela 2.1éovalor Inexistentes, que aparece em três dos
oito pacientes.
Para os atributos numéricos, as medidas mais utilizadas são a média, a mediana e o
percentil. Suponha um conjunto de n valores numéricos: xj = {x1, x2,...,xn}. O valor
médio desse conjunto pode ser facilmente calculado pela Equação 2.1.
x¯j = 1
n
n
∑
i=1
xi (2.1)
Um dos problemas associados à média é a sua sensibilidade à presença de outliers, que
são valores muito diferentes dos demais valores observados para o mesmo atributo (ver
detalhes no Capítulo 3). A média é um bom indicador do valor central de um conjunto de
valores apenas se os valores estão distribuídos de forma simétrica. Um valor muito mais
alto ou muito mais baixo que os demais valores do conjunto pode gerar um valor da média
distorcido, o que poderia mudar radicalmente se o outlier for retirado. Este problema
é minimizado com o uso da mediana, que é menos sensível a outliers. Para utilizar a
mediana, o primeiro passo consiste em ordenar de forma crescente o conjunto de valores.
Ordenados os valores, a Equação 2.2 pode ser utilizada para o cálculo da mediana.
mediana(xj
) = ! 1
2 (xr +xr+1) se n for par (n = 2r)
xr+1 se n for ímpar (n = 2r +1) (2.2)
Assim, se o número de valores, n, é ímpar, a mediana é igual ao valor do meio do
conjunto ordenado. Caso contrário, se for par, é dada pela média dos dois valores do
meio. Por exemplo, seja o conjunto de valores {17,4,8,21,4}. A ordenação desse conjunto gera a sequência de valores (4,4,8,17,21). Observe que valores repetidos são mantidos na sequência. Como o número de valores é ímpar, 5, a mediana é dada pelo terceiro valor, isto é, mediana = 8. Se o conjunto de valores fosse formado pelos elementos,
{17,4,8,21,4,15,13,9}, como o número de elementos, 8, é par, a mediana é dada pela média entre o quarto e o quinto valor da sequência ordenada (4,4,8,9,13,15,17,21). Nesse
caso, mediana = (9+13)/2 = 11. O recurso à mediana facilita a observação da assimetria
da distribuição e da existência de outliers. Existem ainda variações da média e da mediana, como, por exemplo, a média truncada, que minimiza os problemas da média por meio
do descarte dos exemplos nos extremos da sequência ordenada dos valores. Para isso, é
necessário definir a percentagem de exemplos a serem eliminados em cada extremidade.
28 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Algoritmo 2.1 Algoritmo para cálculo do percentil
Entrada: Um conjunto de n valores e o percentil p (valor real entre 0,0 (equivale a
0%) e 1,0 (equivale a 100%)) a ser retornado
Saída: Valor do percentil
1 Ordenar os n valores em ordem crescente
2 Calcular o produto np
3 se np não for um número inteiro então
4 Arredondar para o próximo inteiro
5 Retornar o valor dessa posição na sequência
6 fim
7 senão
8 Considerar np = k
9 Retornar a média entre os valores nas posições k e k +1
10 fim
Outras medidas muito utilizadas são os quartis e os percentis. Assim como a mediana, estas medidas são utilizadas após prévia ordenação dos valores. Enquanto a mediana
divide os dados ao meio, estas outras medidas utilizam pontos de divisão diferentes. Os
quartis dividem os valores ordenados em quartos. Assim, o 1º quartil de uma sequência,
Q1, é o valor para o qual 25% dos outros valores são inferiores a ele. Este também é o
valor do 25º percentil, P25. O 2º quartil é a mediana, que é equivalente ao 50º percentil,
P50.
Seja p um valor entre 0 e 100. O pº percentil, Pp, é um valor xi do conjunto de valores
tal que p% dos valores observados são menores que xi. Assim, o 40º percentil, P40, de um
conjunto de valores é o valor para o qual 40% dos demais valores são menores ou iguais a
ele. Para calcular o pº percentil, basta seguir os passos do Algoritmo 2.1.
Uma forma simples de visualizar a distribuição dos dados consiste em utilizar técnicas de visualização como, por exemplo, boxplots, histogramas e scatter plots, que serão
introduzidos posteriormente neste capítulo. Para efeitos de ilustrar algumas das medidas
e gráficos, será utilizado o conjunto de dados iris (Fisher, 1936). O conjunto de dados
iris é um dos mais populares em ECD, contendo 150 objetos, cada um deles caraterizado
por quatro atributos de entrada (tamanho da sépala, largura da sépala, tamanho da pétala
e largura da pétala), que assumem valores contínuos, e um atributo alvo com três valores
nominais ou classes, que representam as espécies íris setosa, íris versicolor e íris virgínica.
A Figura 2.1 mostra duas variações do gráfico boxplot para o atributo largura da sépala,
do conjunto de dados iris. O boxplot, também designado por diagrama de Box e Whisker
ou caixa de bigodes, apresenta um resumo dos valores para o 1º , 2º (mediana) e 3º quartis,
além dos limites inferior e superior.
Do lado esquerdo da figura é apresentado o gráfico boxplot original, onde a linha ho-
ANÁLISE EXPLORATÓRIA DE DADOS 29
Boxplot Boxplot modificado
Largura da sépala
4,0
3,5
3,0
2,5
Largura da sépala
2,0
4,0
3,5
3,0
2,5
Figura 2.1 Boxplots para o atributo largura da sépala do conjunto de dados iris.
rizontal mais baixa e a linha horizontal mais alta indicam, respetivamente, os valores mínimo e máximo presentes nos dados. Os lados inferior e superior do retângulo representam
o 1º quartil e o 3º quartil, respetivamente. A linha no interior do retângulo é o 2º quartil,
ou mediana. O limite superior (inferior) da linha tracejada vai até o maior (menor) valor
do conjunto de dados.
O gráfico da direita ilustra uma variação do gráfico boxplot, conhecido como boxplot
modificado. Nesse gráfico, o limite superior (inferior) da linha tracejada vai até o maior
(menor) valor apenas se esse valor não for muito distante do 3º (1º) quartil (no máximo
1,5 × intervalo entre quartis). Os valores acima do limite superior e abaixo do limite inferior são considerados outliers. Neste gráfico, 4 valores outliers são representados por
círculos, 3 maiores que 3º quartil + 1,5 × (3º quartil − 1º quartil) e 1 menor que 1º quartil
− 1,5 × (3º quartil − 1º quartil).
Medidas de Dispersão
As medidas de dispersão medem a variabilidade de um conjunto de valores. Permitem
averiguar se os valores se encontram amplamente dispersos ou relativamente concentrados
em torno de um valor, por exemplo, a média. As medidas de dispersão mais comuns são:
Intervalo, Variância, Desvio padrão.
O intervalo é a medida mais simples indicando a dispersão máxima entre os valores de
um conjunto. Assim, sejam xj = {x1, x2,...,xn} os valores do atributo para n objetos. O
intervalo desse conjunto é medido pela Equação 2.3.
intervalo(xj
) = max
i=1,...,n
(xi)− min
i=1,...,n
(xi) (2.3)
30 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Se a maioria dos valores for próxima de um ponto, com um pequeno número de valores
extremos, o intervalo não será uma boa medida da dispersão dos valores. A medida mais
utilizada para avaliar a dispersão de valoreséavariância, que é dada pela Equação 2.4.
variância(xj
) = 1
n−1
n
∑
i=1
(xi −x¯j
)
2 (2.4)
Nesta equação, ¯x j representa a média dos valores de x. O uso do denominador n − 1,
conhecido por correção de Bessel, oferece uma melhor estimativa da variância verdadeira
do que o uso de n. Outra medida de dispersão, o desvio padrão, é dada pela raiz quadrada
da variância.
Assim como a média, o valor da variância pode ser distorcido pela presença de outliers,
uma vez que a variância calcula a diferença entre cada valor e a média. Outras estimativas
mais robustas da dispersão frequentemente utilizadas são:
• Desvio médio absoluto (AAD, do inglês absolute average deviation), ilustrado pela
Equação 2.5.
• Desvio mediano absoluto (MAD, do inglês median absolute deviation), ilustrado
pela Equação 2.6.
• Intervalo interquartil (IQR, do inglês interquartil range), ilustrado pela Equação 2.7.
AAD(xj
) = 1
n
n
∑
i=1
"
"xi −x¯j
"
" (2.5)
MAD(xj
) = mediana#$"
"x1 −x¯j
"
",...,"
"xn −x¯j
"
"
%& (2.6)
IQR(xj
) = P75% −P25% (2.7)
Medidas de Distribuição
As medidas que são definidas em torno da média de um conjunto de valores, como as medidas média e desvio padrão são, na sua maioria, instanciações de uma medida denominada
momento, que é definida pela Equação 2.8.
momentok(xj
) =
n
∑
i=1
(xi −x¯j
)k
(n−1) (2.8)
Para cada valor do parâmetro k, uma medida diferente de momento é definida. Assim:
• quando k = 1, tem-se o valor 0, que é o primeiro momento em torno da origem ou
primeiro momento central;
ANÁLISE EXPLORATÓRIA DE DADOS 31
20
15
10
Quantidade de objetos
5
0
25
30
4 5 6 7 8
Intervalos de valores
Tamanho da sépala
20
10
Quantidade de objetos
5
0
30
2,0 2,5 3,0 3,5 4,0
Intervalos de valores
Largura da sépala
20
10
Quantidade de objetos
0
30
1 4 7
Intervalos de valores
Tamanho da sépala
20
15
10
Quantidade de objetos
5
0
25
30
0,0 0,5 1,0 1,5 2,0
Intervalos de valores
Largura da sépala
2 3 5 6 2,5
35
Figura 2.2 Histograma para a distribuição de valores dos atributos de entrada do conjunto iris.
• quando k = 2, tem-se a variância, que é o segundo momento central;
• quando k = 3, tem-se a obliquidade, que é o terceiro momento central;
• quando k = 4, tem-se a curtose, que é o quarto momento central.
Conforme mencionado, os dois primeiros momentos, o valor 0 e o desvio padrão,
são medidas de localização e dispersão, respetivamente. O terceiro e quarto momentos,
obliquidade e curtose, são medidas de distribuição, uma vez que mostram como os valores
estão distribuídos.
Outra forma simples de visualizar a distribuição dos dados é por via da sua representação num histograma. Um histograma divide os valores de um conjunto de dados em
intervalos. Para cada intervalo, é desenhada uma barra cuja altura é proporcional ao número de elementos nesse intervalo. Para valores numéricos, os valores são divididos em
intervalos contíguos, geralmente, do mesmo tamanho. O formato do histograma depende
do número de intervalos utilizados. Na Figura 2.2, é apresentado um histograma com a
distribuição de valores para cada atributo do conjunto de dados iris.
O terceiro momento, obliquidade (em inglês skewness), mede a simetria da distribuição
dos dados em torno da média. Numa distribuição simétrica, se os valores forem distribuídos em intervalos do mesmo tamanho, um histograma com a quantidade de valores em
cada intervalo tem a mesma aparência à direita e à esquerda do ponto central. A Equação 2.9 ilustra o cálculo da obliquidade, que utiliza k = 3 e divide o valor calculado pelo
desvio padrão elevado ao cubo, s3, para tornar a medida independente da escala.
32 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Positiva
10
Normal
4
2
0
6
8
10
12
Negativa
4
2
0
6
8
10
12
4
2
0
6
8
12
Figura 2.3 Distribuição dos valores de obliquidade.
obliquidade(xj
) = momento3(xj
)
s3 =
n
∑
i=1
(xi −x¯j
)3
(n−1)s3 (2.9)
A distribuição dos valores num conjunto de dados está associada ao valor da obliquidade da seguinte forma:
• obliquidade = 0 (simétrica): a distribuição é aproximadamente simétrica (ocorre
para uma distribuição normal);
• obliquidade > 0 (positiva): a distribuição concentra-se mais no lado esquerdo;
• obliquidade < 0 (negativa): a distribuição concentra-se mais no lado direito.
A Figura 2.3 ilustra estes três tipos de distribuição medidos pela obliquidade.
O quarto momento calcula a curtose (em inglês kurtosis), que é uma medida de dispersão que captura o achatamento da função de distribuição. A medida de curtose, calculada
pela Equação 2.10, verifica se os dados apresentam um pico ou se são achatados em relação a uma distribuição normal. À semelhança da Equação 2.9, para tornar a medida
independente de escala, o quarto momento é dividido pelo desvio padrão elevado à quarta
potência, s4.
curtose(xj
) = momento4(xj
)
s4 =
n
∑
i=1
(xi −x¯j
)4
(n−1)s4 (2.10)
Visto que, para uma distribuição normal com média 0 e variância 1, o valor da curtose
é igual a 3, é feita uma correção na sua fórmula para que a distribuição normal padrão
tenha curtose igual a 0. Para isso, geralmente é utilizada a Equação 2.11.
ANÁLISE EXPLORATÓRIA DE DADOS 33
10
2
0
Positiva
4
6
8
12
10
2
0
4
6
8
12
10
2
0
4
6
8
12
Normal Negativa
Figura 2.4 Distribuição dos valores de curtose.
Ausentes 43%
Concentradas 14%
Espalhadas 14% Uniformes 29%
Figura 2.5 Gráfico circular para a distribuição de valores do atributo Manchas.
curtose(xj
) = momento4(xj
)
σ4 −3 =
n
∑
i=1
(xi −x¯j
)4
(n−1)σ4 −3 (2.11)
Assim como na medida de obliquidade, a seguinte relação, ilustrada pela Figura 2.4, é
observada entre o valor da curtose e a distribuição dos valores num conjunto de dados:
• curtose = 0 (normal): o histograma de distribuição dos dados apresenta o mesmo
achatamento que uma distribuição normal;
• curtose > 0 (positiva): o histograma de distribuição dos dados apresenta uma distribuição mais alta e concentrada que a distribuição normal;
• curtose < 0 (negativa): o histograma de distribuição dos dados apresenta uma distribuição mais achatada que a distribuição normal.
Outro gráfico muito utilizado para ilustrar a distribuição de um conjunto de valores é o
gráfico circular. Neste gráfico, cada valor ocupa uma fatia cuja área é proporcional ao número de vezes que o valor surge no conjunto de dados. Este tipo de gráfico é indicado para
valores qualitativos. A Figura 2.5 mostra a distribuição dos valores do atributo Manchas
do conjunto de dados hospital utilizando um gráfico circular. Para atributos quantitativos, os valores podem, à semelhança do que é feito em histogramas, ser agrupados em
intervalos.
34 EXTRAÇÃO DE CONHECIMENTO DE DADOS
2.2.2 Dados Multivariados
Dados multivariados são aqueles que possuem mais do que um atributo de entrada. O conjunto de dados iris é um conjunto multivariado. Nesses casos, as medidas de centralidade
podem ser obtidas calculando a medida de centralidade de cada atributo separadamente.
Por exemplo, a média para um conjunto de objetos com d atributos pode ser calculada pela
Equação 2.12.
x¯ = (x¯
1,...,x¯
d) (2.12)
As medidas de dispersão podem ser calculadas para cada atributo independentemente
dos demais utilizando qualquer medida de dispersão. Conforme visto na seção anterior
para o conjunto de dados iris, cada atributo pode ser explorado visualmente de forma
independente.
Os dados multivariados permitem ainda análises sobre a relação entre dois ou mais
atributos. Por exemplo, para atributos quantitativos, a dispersão de um conjunto de dados
é capturada melhor através de uma matriz de covariância, em que cada elemento da matriz
corresponde à covariância entre dois atributos. Cada elemento covi j de uma matriz de covariância Cov mede a covariância entre os atributos xi e xj
, que é dada pela Equação 2.13.
covariância(xi
,xj
) = 1
n−1
n
∑
k=1
(xi
k −x¯
i
)(x j
k −x¯j
) (2.13)
Nesta equação, ¯xi representa o valor médio do i-ésimo atributo e xi
k, o valor
do i-ésimo atributo para o k-ésimo objeto. É importante observar que covariância(xi
,xi
)
= variância(xi
). Deste modo, a diagonal da matriz de covariância contém as variâncias
dos atributos.
A covariância entre dois atributos mede o grau com que os atributos variam juntos. Seu
valor depende da magnitude dos atributos. Um valor próximo de 0 indica que os atributos
não têm um relacionamento linear. Um valor positivo indica que os atributos são diretamente relacionados. Quando o valor de um dos atributos aumenta, o do outro também
aumenta. O contrário ocorre se a covariância for negativa. A medida de covariância é afetada pela dimensão dos atributos avaliados. Assim, dois atributos com dimensão elevada
podem apresentar um valor de covariância maior do que dois atributos mais semelhantes
entre si, mas de menor dimensão. Por isso, não é possível avaliar o relacionamento entre
dois atributos observando apenas a covariância entre eles. A medida de correlação elimina
esse problema removendo a influência da escala. Como resultado, a correlação apresenta
uma indicação mais clara da força da relação linear entre dois atributos, sendo, por esse
motivo, mais utilizada para explorar dados multivariados do que a covariância. A matriz
de correlação apresenta a correlação entre cada possível par de atributos de um conjunto de
dados. Cada elemento da matriz de correlação tem o seu valor definido pela Equação 2.14.
correlação(xi
,xj
) = covariância(xi
,xj
)
sisj
(2.14)
ANÁLISE EXPLORATÓRIA DE DADOS 35
2,0
Tamanho da sépala
Largura da sépala
2,5 3,0 3,5 4,0
4,5
5,0
5,5
6,0
6,5
7,0
7,5
8,0
2,0
2,5
3,0
3,5
4,0
4,5 5,0 5,5 7,06,56,0 7,5 8,0
Figura 2.6 Matriz de scatter plot para dois atributos do conjunto de dados iris.
Nesta equação, xi é o i-ésimo atributo e si é o desvio padrão dos valores desse atributo.
De realçar que correlação(xi
,xi
) = 1. Desta forma, os elementos da diagonal têm valor
1. Os outros elementos assumem um valor entre −1 (correlação negativa máxima) e +1
(correlação positiva máxima).
Assim como na medida de covariância, quando dois atributos apresentam uma correlação positiva, o aumento do valor de um deles é geralmente acompanhado por um aumento
no valor do outro. Da mesma forma, quando dois atributos estão negativamente correlacionados, a redução no valor de um deles é geralmente acompanhada da redução do valor
do outro.
A análise de dados multivariados pode ser facilitada pelo uso de recursos de visualização. Assim como histogramas, gráficos de pizza e boxplots são utilizados para visualizar
dados univariados, outros diagramas têm sido adotados para visualizar dados multivariados, particularmente a relação entre os diferentes atributos. Entre esses diagramas, um dos
mais utilizados é o scatter plot, que ilustra a correlação linear entre dois atributos.
Num scatter plot, a cada objeto, considerando apenas dois de seus atributos, é associada uma posição ou ponto num plano bidimensional. Os valores dos atributos, que podem
ser números inteiros ou reais, definem as coordenadas desse ponto. Na Figura 2.6, um
scatter plot ilustra a correlação entre dois atributos do conjunto de dados iris: tamanho
da sépala e largura da sépala.
No scatter plot do canto superior direito desta figura, cada ponto representa a Largura
da sépala (eixo horizontal) e o Tamanho da sépala (eixo vertical) de um dos 150 objetos
36 EXTRAÇÃO DE CONHECIMENTO DE DADOS
4,0
3,5
3,0
2,5
2,0
Largura da sépala
4,5 5,0 5,5 6,0 6,5 7,0 7,5 8,0
Tamanho da sépala
+ +
+ +++
+ +
+
+ +
+
+
++++
+ +
++
+
+ ++
+ +
+ +
+++
+ +
+
+ +
+ +
+
+
+
+
+
+
+
+
+
Figura 2.7 Diagramas de bagplot para dois atributos do conjunto de dados iris.
do conjunto de dados iris. No scatter plot do canto inferior esquerdo ocorre o contrário,
isto é, cada ponto representa o Tamanho da sépala (eixo horizontal) e a Largura da sépala
(eixo vertical) de um objeto. Gráficos de scatter plot para diferentes combinações de
atributos podem ser exibidos em matrizes de scatter plot. Embora os scatter plots com
duas dimensões sejam os mais comuns, também podem ser utilizados para visualizar a
relação entre três atributos, definindo scatter plots tridimensionais. Os atributos adicionais
podem ser exibidos utilizando tamanho, formato e cor nos marcadores que representam os
objetos. Quando as classes dos objetos são disponibilizadas, o scatter plot pode ser usado
para investigar o grau com que dois atributos separam as classes, se for possível separar a
maioria dos objetos de uma das classes com uma reta.
Outras técnicas de visualização para dados multivariados bastante utilizadas são os
bagplots, as faces de Chernoff, os star plots e os heatmaps (Wu et al., 1998).
O bagplot é uma generalização bivariada do boxplot que permite apresentar, numa
mesma figura, o boxplot de dois atributos ou variáveis (Rousseeuw et al., 1999). Cada
eixo do gráfico pode ser considerado um boxplot associado a um dos dois atributos. A
Figura 2.7 ilustra um gráfico de bagplot para dois atributos do conjunto de dados iris:
tamanho da sépala e largura da sépala. O gráfico apresenta três regiões convexas. A
menor delas tem apenas um objeto, representado por um asterisco, cujas coordenadas são
definidas pela mediana de seus dois atributos. A segunda região, denominada bag, possui
50% dos objetos, cujas coordenadas são definidas pelos valores entre o primeiro e terceiro
quartis para cada um dos atributos. A maior região, chamada loop,éa bag expandida
três vezes o intervalo entre quartis nas duas dimensões, 1,5 vez em cada sentido. Para
cada eixo, o valor máximoeovalor mínimo são definidos, respetivamente, pelos limites
superior e inferior do atributo correspondente. Objetos que se encontrem fora dos limites
da maior região são considerados outliers. Cada dimensão vista isoladamente representa
o boxplot para o atributo associado.
No diagrama de Chernoff, cada atributo é representado por uma ou mais caraterísticas
de uma face, como altura e largura da cabeça, da boca, do cabelo, do nariz, das orelhas,
ANÁLISE EXPLORATÓRIA DE DADOS 37
136 137 138 139 140
141 142 143 144
146 147 148 149
145
150
Figura 2.8 Diagramas de Chernoff para 15 objetos do conjunto de dados iris.
se a face apresenta um sorriso e o estilo do cabelo. O número de atributos representados
é limitado pelo número de caraterísticas presentes na implementação do algoritmo que
desenha as faces. Se o conjunto de dados tem menos atributos que o número de possíveis
caraterísticas, um mesmo atributo pode ser representado por mais de uma caraterística da
face. O diagrama de Chernoff para 15 exemplos do conjunto de dados iris pode ser
visto na Figura 2.8. Nesta figura, o tamanho da sépala, por exemplo, é representado pelas
caraterísticas altura da face, largura da boca, altura do cabelo e largura do nariz. Os outros
atributos do conjunto de dados iris são representados nas figuras por outras caraterísticas
da face.
Assim como o diagrama de Chernoff, o star plot desenha uma figura geométrica para
cada objeto, normalmente um polígono. Cada linha do polígono corresponde a um dos
atributos, e o tamanho da linha é proporcional ao valor do atributo. Um segmento de
reta liga o centro do polígono a cada um de seus vértices. Um conjunto de star plots
para os objetos do conjunto iris é ilustrado pela Figura 2.9. Quanto mais atributos são
considerados, mais o polígono se assemelha a uma estrela. Quando dois ou mais atributos
38 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Figura 2.9 Star plots para os 150 objetos do conjunto de dados iris.
de um objeto têm valores semelhantes, apresentam coordenadas similares no diagrama,
deformando o formato de estrela.
Um heatmap representa a relação entre os exemplos e as classes, associando um eixo
para cada um. Para cada eixo é construída uma imagem gráfica referente ao agrupamento
hierárquico (dendrograma) dos elementos do eixo, utilizando como entrada os elementos
do outro eixo, como pode ser visto na Figura 2.10. Para facilitar a visualização, em vez
do nome completo de cada atributo, foram utilizadas apenas as suas iniciais. Por exemplo,
TS significa Tamanho da Sépala.
Nesta imagem, cada linha da imagem representa um objeto do conjunto de dados iris
e cada coluna, um dos atributos desse conjunto. Elementos semelhantes são agrupados
juntos, apresentando cores ou tonalidades semelhantes. No topo da imagem é apresentado
um dendrograma das três classes do conjunto iris e no lado esquerdo um dendrograma
para os 150 objetos. Como os elementos em cada eixo são agrupados na imagem, de
acordo com a sua semelhança, os heatmaps permitem ver tendências nos seus valores.
Em bioinformática, os heatmaps são muito utilizados para análise de dados de expressão
genética.
2.3 Considerações Finais
Antes de aplicar os algoritmos de ECD a um conjunto de dados, é importante que os dados
sejam analisados. Essa análise, que pode ser realizada por meio de técnicas estatísticas e de
visualização, permite uma melhor compreensão da distribuição dos dados e pode suportar
a escolha de formas para modelar o problema.
Neste capítulo, foram apresentados conceitos considerados importantes para analisar
ANÁLISE EXPLORATÓRIA DE DADOS 39
TS LP LS TP
Figura 2.10 Heatmap para atributos de entrada do conjunto de dados iris.
os principais aspetos de um conjunto de dados. Após uma caraterização dos tipos de dados
encontrados na maioria das aplicações de ECD, foram apresentadas medidas estatísticas
frequentemente utilizadas para exploração de dados univariados e multivariados. Por fim,
diversas técnicas de visualização foram descritas para dados com uma ou mais varáveis:
boxplots, histogramas, gráficos de pizza, scatter plots, bagplots, faces de Chernoff, star
plots e heatmaps. Note-se que existem outras técnicas de visualização, sendo que neste
capítulo se introduziram as mais conhecidas e utilizadas.

Capítulo 3
Pré-processamento de Dados
Apesar de os algoritmos de ECD serem frequentemente adotados para extrair conhecimento de conjuntos de dados, o seu desempenho é geralmente afetado pelo estado dos dados. Diferentes conjuntos de dados podem apresentar diferentes caraterísticas, dimensões
ou formatos. Por exemplo, conforme visto no capítulo anterior, os valores dos atributos
de um conjunto de dados podem ser numéricos ou simbólicos. Podem ainda estar limpos
ou conter ruído e imperfeições, com valores incorretos, inconsistentes, duplicados ou ausentes. Por outro lado, os atributos podem ser independentes ou estar relacionados. Além
disso, os conjuntos de dados podem apresentar poucos ou muitos objetos que, por sua vez,
podem ser caraterizados por um pequeno ou elevado número de atributos.
As técnicas de pré-processamento de dados são frequentemente utilizadas para melhorar a qualidade dos dados por via da eliminação ou minimização dos problemas citados.
Esta melhoria pode facilitar a utilização de técnicas de ECD, permitir a construção de
modelos mais fiéis à distribuição real dos dados reduzindo a sua complexidade computacional, facilitar e acelerar o ajuste dos parâmetros do modelo e a sua posterior utilização.
Adicionalmente pode facilitar a interpretação dos padrões extraídos pelo modelo.
As técnicas de pré-processamento de dados são úteis não apenas devido à sua capacidade de minimizar ou eliminar problemas existentes num conjunto de dados, mas também
porque podem melhorar a adequabilidade dos dados a um determinado algoritmo de ECD.
Por exemplo, alguns algoritmos de ECD trabalham apenas com valores numéricos.
Neste capítulo serão apresentadas algumas operações de pré-processamento que podem ser realizadas nos conjuntos de dados antes da aplicação de algoritmos de ECD. Estas
operações englobam o uso de técnicas de amostragem, tratamentos para dados desbalanceados, modificações para adequação dos tipos de atributo, limpeza dos dados, integração de
dados, transformações dos dados e redução de dimensionalidade. Note-se que não existe
uma ordem fixa para a aplicação das diferentes técnicas de pré-processamento.
Este capítulo está organizado de acordo com as diferentes tarefas de pré-processamento.
A Secçao 3.1 apresenta os conceitos básicos da teoria da informação. Na Seção 3.2 é
mostrado como atributos claramente irrelevantes podem ser identificados e extraídos do
conjunto de dados. Na Seção 3.3 é discutida a agregação de dados de diferentes fontes. A
seleção de um subconjunto dos dados originais através de amostragem de dados é abor-
42 EXTRAÇÃO DE CONHECIMENTO DE DADOS
dada na Seção 3.4. A Seção 3.5 trata do tema de conjuntos de dados desbalanceados.
Técnicas para a limpeza de conjuntos de dados são introduzidas na Seção 3.6. Na Seção
3.7 são descritas operações para a transformação do tipo dos valores de um atributo. Os
problemas causados pelo elevado número de atributos preditivos, bem como formas de
reduzir a quantidade desses atributos são analisados na Seção 3.8. A Seção 3.9 remata o
Capítulo com alguns comentários finais.
3.1 Teoria da Informação
A teoria da informação é baseada na estatística e na teoria da probabilidade. As medidas
de informação mais importantes são a entropia, que mede o grau de aleatoriedade de uma
variável aleatória, e a informação mútua, que mede a quantidade de informação partilhada
por duas variáveis aleatórias. A escolha da base logarítmica nas seguintes fórmulas determina a unidade de entropia da informação que é usada. A unidade de informação mais
comum é o bit, para a função do logaritmo base 2. Por convenção, uma expressão da forma
0×log2(0) é considerada como sendo igual a zero.
A entropia H da variável aleatória discreta X mede a incerteza associada ao valor de
X.
H(X) = − ∑
x∈X
p(x)×log(p(x)) (3.1)
Considere uma variável aleatória discreta X cujo domínio é {x1,x2,...,xv}. Suponha que a
probabilidade de observar cada valor é p1, p2,..., pv. A entropia de X é dada por: H(X) =
−∑i pi ×log2 pi. A função entropia de uma variável aleatória que pode assumir v valores
distintos, apresenta as seguintes propriedades:
1. H(X) ∈ [0,log2(v)];
2. H(X) tem um máximo igual a log2(v) se pi = pj,∀i '= j;
3. H(X) tem um mínimo igual a 0 se ∃i: pi = 1.
A entropia condicional de X, dada a variável aleatória Y, é a média da entropia condicional sobre Y:
H(X|Y) = − ∑
y∈Y
p(y) ∑
x∈X
p(x|y)log p(x|y) = −∑x,y
p(x, y)log p(x, y)
p(y) (3.2)
A entropia condicional tem as seguintes propriedades básicas:
1. H(X|X) = 0
2. H(X|Y) = H(X,Y)−H(Y)
3. H(X|Y) = H(X) se X e Y forem variáveis independentes
PRÉ-PROCESSAMENTO DE DADOS 43
A informação mútua entre duas variáveis aleatórias é dada pela expressão:
I(X,Y) = ∑x,y
p(x,y)log p(x,y)
p(x) p(y) (3.3)
Pode ser compreendida como a diminuição na incerteza de uma das variáveis, proporcionada pelo conhecimento da outra. A informação mútua é simétrica: I(X,Y) = I(Y,X) e
pode ser considerada uma estatística para avaliar a independência entre um par de variáveis.
3.2 Eliminação Manual de Atributos
Observando novamente a Tabela 2.1 no capítulo anterior, onde é apresentado o conjunto de
dados hospital, pode ser facilmente percebido que nem todos os atributos apresentados
são necessários para o diagnóstico clínico de um paciente. Não faz sentido, por exemplo,
usar os valores dos atributos Nome e Id. (identificação do paciente) para o diagnóstico.
Quando um atributo claramente não contribui para a estimativa do valor do atributo alvo,
ele é considerado irrelevante.
O conjunto de atributos que formarão o conjunto de dados a ser analisado é geralmente
definido de acordo com a experiência de especialistas no domínio do problema de decisão.
Os especialistas podem decidir, por exemplo, que atributos associados à identificação do
paciente, ao nome do paciente e ao estado de origem do paciente não são relevantes para
seu diagnóstico clínico. A Tabela 3.1 mostra o conjunto de dados hospital após remover
os atributos considerados irrelevantes.
Tabela 3.1 Conjunto de dados sem atributos considerados irrelevantes
Idade Sexo Peso Manchas Temp. # Int. Diagnóstico
28 M 79 Concentradas 38,0 2 Doente
18 F 67 Inexistentes 39,5 4 Doente
49 M 92 Espalhadas 38,0 2 Saudável
18 M 43 Inexistentes 38,5 8 Doente
21 F 52 Uniformes 37,6 1 Saudável
22 F 72 Inexistentes 38,0 3 Doente
19 F 87 Espalhadas 39,0 6 Doente
34 M 67 Uniformes 38,4 2 Saudável
Existem outras situações em que um atributo irrelevante pode ser facilmente detetado.
Por exemplo, quando um atributo possui o mesmo valor para todos os objetos. Tal atributo
não contém informação que ajude a distinguir os objetos. Por esse motivo, pode ser considerado irrelevante. Um atributo não precisa ter exatamente o mesmo valor para todos os
objetos para ser irrelevante. Neste capítulo iremos ver algumas das técnicas de seleção de
atributos, utilizadas para eliminar atributos irrelevantes.
44 EXTRAÇÃO DE CONHECIMENTO DE DADOS
3.3 Integração de Dados
Conforme mencionado anteriormente, quando os dados a serem utilizados numa aplicação
de ECD estão distribuídos em diferentes bases de dados, os dados têm de ser integrados
antes do início do uso da técnica de ECD. Na integração, é necessário identificar quais
são os objetos que estão presentes nos diferentes conjuntos a serem combinados. Este
problema é conhecido como o problema de identificação de entidade. Esta identificação
é realizada por meio da procura por atributos comuns nos conjuntos a serem combinados.
Por exemplo, os conjuntos de dados médicos podem ter um atributo que identifica o paciente. Desta forma, os objetos dos diferentes conjuntos que possuem o mesmo valor para o
atributo que identifica o paciente são combinados num único objeto do conjunto integrado.
É fácil ver que o(s) atributo(s) utilizado(s) para combinação deve(m) ter um valor único
para cada objeto.
Alguns aspetos podem dificultar a integração. Por exemplo, atributos correspondentes
podem ter nomes diferentes em diferentes bases de dados. Além disso, os dados a serem
integrados podem ter sido atualizados em momentos diferentes. Para minimizar esses problemas, é comum o uso de metadados em bases de dados. Os metadados são dados sobre
dados que, ao descrever as principais caraterísticas dos dados, podem ser utilizados para
evitar erros no processo de integração. O processo de integração origina um depósito ou
repositório de dados (data warehouse), que funciona como uma base de dados centralizada.
Com ou sem integração, é cada vez mais frequente o uso de técnicas de ECD em grandes conjuntos de dados, que muitas vezes crescem com o tempo. Um conjunto de dados
é considerado ’grande’ quer porque contem um número elevado de objetos, quer porque
cada objeto é descrito por um número elevado de atributos. Em geral, o desempenho de
um algoritmo de aprendizagem melhora com o aumento do número de objetos, e diminui
com o crescimento do número de atributos.
A existência de um conjunto de dados grande, tanto em termos de número de objetos
como de atributos, não implica que um algoritmo de ECD o deva utilizar todo. Muitas
vezes é mais eficiente usar apenas parte do conjunto original. É sabido que um número
elevado de atributos pode comprometer o desempenho do algoritmo de aprendizagem.
Este é um problema conhecido como a maldição da dimensionalidade, que será discutido
no Capítulo 4. As técnicas para lidar com um grande número de atributos serão abordadas
na Seção 3.8. Com relação à quantidade de objetos, problemas podem ocorrer por causa
de saturação de memória e aumento do tempo computacional necessário para ajustar os
parâmetros do modelo. Para minimizar esses problemas, podem ser utilizadas técnicas de
amostragem, que serão apresentadas a seguir.
3.4 Amostragem de Dados
Os algoritmos de ECD podem ter dificuldades em lidar com um número elevado de objetos. Este problema é observado em algoritmos de ECD baseados em instâncias, como
PRÉ-PROCESSAMENTO DE DADOS 45
k-vizinhos mais próximos (k-NN, do inglês k-nearest neighbours (Fix, 1951)) (descritos
no Capítulo 4), que podem apresentar problemas de saturação de memória quando um
conjunto de dados contém um elevado número de exemplos.
Associado ao número de objetos num conjunto de dados, existe um equilíbrio entre
eficiência computacional e taxa de acerto (taxa de predições corretas). Quanto maior a
quantidade de dados utilizada, maior tende a ser a taxa de acerto do modelo e menor a
eficiência computacional do processo indutivo, uma vez que um número muito elevado
de objetos pode aumentar significativamente o tempo de processamento. Para se obter
um bom compromisso entre eficiência e taxa de acerto, geralmente, trabalha-se com uma
amostra ou um subconjunto dos dados. Muitas vezes, o recurso a uma amostra permite
alcançar o mesmo desempenho que se obteria utilizando o conjunto de dados completo,
porém com um custo computacional muito menor.
Note-se, porém, que uma amostra pequena pode não representar bem o problema que
se pretende modelar. A amostra deve ser representativa do conjunto de dados original.
Se as amostras não forem representativas, diferentes amostras de uma mesma população
podem gerar modelos diferentes, uma vez que caraterísticas importantes do problema ou
da distribuição que gerou os dados poderem não estar presentes. Por outro lado, se a
amostra for muito grande, são reduzidas as vantagens de utilizar amostragem. Assim,
deve-se novamente procurar um compromisso entre a eficiência e o desempenho.
O ideal é que a amostra não seja grande, mas que os dados que a constituem obedeçam
à mesma distribuição estatística que gerou o conjunto de dados original. Desta forma, seria
capaz de fornecer uma estimativa da informação contida na população original, permitindo
tirar conclusões de um todo a partir de uma parte. Por exemplo, a média dos valores
para cada atributo dos dados originais deve ser semelhante à média observada nos valores
desses atributos na amostra gerada. Embora não seja possível garantir que isso aconteça,
existem técnicas de amostragem estatística que aumentam a probabilidade de isso ocorrer.
Existem basicamente três abordagens para amostragem:
• Amostragem aleatória simples;
• Amostragem estratificada;
• Amostragem progressiva.
A amostragem aleatória simples possui duas variações: amostragem simples sem reposição de exemplos, em que os exemplos são extraídos do conjunto original para a amostra
a ser utilizada, com a condição de que cada exemplo apenas pode ser selecionado uma
vez; e amostragem simples com reposição, quando uma cópia dos exemplos selecionados
é mantida no conjunto de dados original. A amostragem com reposição é mais fácil de
analisar, pois a probabilidade de escolher qualquer objeto se mantém constante. No entanto, as duas formas são semelhantes quando o tamanho da amostra é muito menor que o
tamanho do conjunto original.
A amostragem estratificada é usada quando as classes apresentam propriedades diferentes, por exemplo, números de objetos bastante diferentes. Em problemas de classificação, um aspeto que deve tomar em consideração na amostragem diz respeito à distribuição
46 EXTRAÇÃO DE CONHECIMENTO DE DADOS
dos dados para as diferentes classes. A existência de classes com um número de exemplos
significativamente maior que as demais pode levar à indução de classificadores tendenciosos para as classes maioritárias. Nesse caso, o conjunto de dados é dito desbalanceado.
Este problema será retomado na Seção 3.5. Por outro lado, pode ocorrer que algumas classes sejam mais difíceis de classificar do que outras e isso possa ser minimizado no processo
de amostragem. Esta abordagem também possui variações. A mais simples consiste em
manter o mesmo número de objetos para cada classe. Outra opção é manter o número de
objetos em cada classe proporcional ao número de objetos da classe no conjunto original.
A terceira alternativa, a amostragem progressiva, começa com uma amostra pequena
e aumenta progressivamente o tamanho da amostra extraída, enquanto a taxa de acerto
preditiva continuar a melhorar. Como resultado, é possível definir a menor quantidade de
dados necessária, reduzindo ou eliminando a perda de taxa de acerto. O tamanho pode
ser confirmado com outras amostras de tamanho semelhante. Geralmente, esta abordagem
fornece uma boa estimativa para o tamanho da amostra.
O especialista do domínio pode também decidir que um subconjunto dos objetos deve
ser utilizado nas suas análises. Por exemplo, numa análise de pacientes de um hospital,
podem ser utilizados apenas os objetos correspondentes a pacientes do sexo feminino.
3.5 Dados Desbalanceados
O problema dos dados desbalanceados é um tópico da área de classificação de dados. Em
vários conjuntos de dados reais, o número de objetos varia para as diferentes classes. Este
problema é comum em aplicações em que dados de um subconjunto das classes aparecem
com uma frequência maior que os dados das restantes classes. Recorrendo ao exemplo
do conjunto de dados de pacientes de um hospital, suponha que 80% dos pacientes que
se deslocam a esse hospital apresentam sintomas de uma dada doença. O conjunto de
dados apresentará então 20% dos seus objetos relacionados com pacientes saudáveis e
80% associados a pacientes com a doença. A classe com pacientes doentes seria então a
classe maioritária, enquanto que a classe de pacientes saudáveis, seria a classe minoritária.
Outro exemplo seria um conjunto de dados de clientes de um banco, em que cada cliente
é rotulado como tendo ou não ficado com o saldo da sua conta negativo nos últimos 90
dias. Se a percentagem de clientes que ficou com saldo negativo nesse período for de 5%,
a classe maioritária terá 95% dos dados.
Para ser aceitável, a taxa de acerto preditiva de um classificador para um conjunto de
dados desbalanceados deve ser maior que a taxa de acerto obtida quando se atribui a classe
maioritária a todo e qualquer objeto. Vários algoritmos de ECD pioram o seu desempenho
na presença de dados desbalanceados. Quando treinados com dados desbalanceados, estes
algoritmos tendem a favorecer a classificação de novos dados na classe maioritária. Se for
possível gerar novos dados através do mesmo processo que gerou o conjunto atual, o conjunto de dados pode ser naturalmente balanceado. No entanto, na maioria das aplicações
práticas isso não é possível. Nestes casos, podem ser utilizadas técnicas que procuram ba-
PRÉ-PROCESSAMENTO DE DADOS 47
lancear artificialmente o conjunto de dados. As principais técnicas propostas na literatura
seguem uma destas alternativas:
• Redefinir o tamanho do conjunto de dados;
• Utilizar diferentes custos de classificação para as diferentes classes;
• Induzir um modelo para uma classe.
No primeiro caso, tanto pode ocorrer o acréscimo de objetos à classe minoritária como
a eliminação de objetos da classe maioritária. No caso de se acrescentarem novos objetos, existe o risco dos objetos acrescentados representarem situações que nunca ocorrerão,
induzindo um modelo inadequado para os dados. Além disso, pode ocorrer um problema
conhecido como overfitting, em que o modelo é superajustado aos dados de treino. Por
outro lado, quando se eliminam objetos da classe maioritária, é possível que dados de
grande importância para a indução do modelo correto sejam perdidos. Isto pode levar ao
problema de underfitting, em que o modelo induzido não se ajusta aos dados de treino. Os
conceitos de overfitting e underfitting serão explicados mais adiante no livro.
A utilização de custos de classificação diferentes para as classes maioritária e minoritária tem como dificuldade a definição desses custos. Por exemplo, se o número de exemplos
da classe maioritária for o dobro do número de exemplos da classe minoritária, um erro de
classificação para um exemplo da classe minoritária pode equivaler à ocorrência de dois
erros de classificação para um exemplo da classe maioritária. Entretanto, a definição dos
diferentes custos geralmente não é tão direta. Outro problema desta abordagem é a dificuldade de incorporar a consideração de diferentes custos em alguns algoritmos de ECD.
Além disso, esta abordagem pode apresentar um desempenho baixo quando uma boa parte
dos objetos da classe maioritária apresenta um elevado grau de semelhança. A existência
de um grande número de objetos semelhantes na classe maioritária pode aproximar a distribuição de exemplos relevantes para o treino, à distribuição presente na classe minoritária,
fazendo com que os custos diferentes privilegiem a classificação na classe minoritária.
A última alternativa inclui as técnicas de classificação com apenas uma classe, em que
a classe minoritária ou a classe maioritária (ou ambas as classes) são aprendidas separadamente. Neste caso, podem ser utilizados algoritmos de classificação para uma classe
apenas (Manevitz et al., 2001). Estes algoritmos são treinados utilizando apenas exemplos
da classe positiva. A classe positiva pode ser, por exemplo, a classe minoritária.
3.6 Limpeza de Dados
Os conjuntos de dados também podem apresentar dificuldades relacionadas com a qualidade dos dados. Exemplos frequentes dessas dificuldades incluem dados ruidosos (que
possuem erros ou valores que são diferentes do esperado), inconsistentes (que não combinam ou contradizem valores de outros atributos do mesmo objeto), redundantes (quando
dois ou mais objetos têm os mesmos valores para todos os atributos ou dois ou mais atributos têm os mesmos valores para dois ou mais objetos) ou incompletos (ausência de valores,
48 EXTRAÇÃO DE CONHECIMENTO DE DADOS
para alguns dos atributos, em parte dos dados). Dados inconsistentes, redundantes ou com
valores ausentes são de fáceis de detetar. A principal dificuldade consiste na deteção de
dados ruidosos.
Essas deficiências nos dados podem ser causadas por problemas nos equipamentos que
realizam a recolha, a transmissão e o armazenamento dos dados, ou ainda problemas no
preenchimento dos dados por seres humanos.
Algumas técnicas de ECD conseguem lidar bem com algumas dessas imperfeições
nos dados. Por exemplo, as máquinas de vetores de suporte lidam com dados que apresentam ruído e um número elevado de atributos preditivos. Outras técnicas apresentam
dificuldades ou não conseguem lidar com dados que apresentem algumas das deficiências
mencionadas anteriormente. Por exemplo, o algoritmo de agrupamento de dados k-médias,
não é robusto em dados com ruido.
Mesmo que a técnica seja robusta o suficiente para lidar com tais imperfeições, podem
reduzir a qualidade da análise. A presença destas deficiências num conjunto de dados
pode resultar em estatísticas e análises incorretas. Portanto, todas as técnicas beneficiam
da melhoria na qualidade dos dados. Cada um destes problemas será detalhado a seguir.
3.6.1 Dados Incompletos
Como já foi previamente mencionado, um dos problemas que pode ser encontrado em
conjuntos de dados é a ausência de valores para alguns atributos de alguns objetos. Na
Tabela 3.2 é ilustrado um exemplo de um conjunto de dados em que três dos objetos
possuem três, dois e um atributos com valores ausentes, respetivamente. Na tabela, os
atributos com valores ausentes são assinalados pelo valor “—”.
Tabela 3.2 Conjunto de dados com atributos com valores ausentes
Idade Sexo Peso Manchas Temp. # Int. Diagnóstico
— M 79 — 38,0 — Doente
18 F 67 Inexistentes 39,5 4 Doente
49 M 92 Espalhadas 38,0 2 Saudável
18 — 43 Inexistentes 38,5 8 Doente
21 F 52 Uniformes 37,6 1 Saudável
22 F 72 Inexistentes 38,0 3 Doente
— F 87 Espalhadas 39,0 6 Doente
34 M 67 Uniformes 38,4 2 Saudável
A ausência de valores em alguns dos atributos da Tabela 3.2 pode ter diferentes causas,
nomeadamente:
• O atributo não foi considerado importante quando os primeiros dados foram recolhidos. Considere, por exemplo, que um dos atributos é o email do paciente, que
não era comum na década de 1990.
PRÉ-PROCESSAMENTO DE DADOS 49
• Desconhecimento do valor do atributo no momento do preenchimento dos valores
do objeto. Uma situação possível seria não saber o tipo sanguíneo de um paciente
quando foi efetuado o seu cadastro.
• Distração na hora do preenchimento.
• Falta de necessidade ou obrigação de apresentar um valor para o(s) atributo(s), para
alguns objetos. Por exemplo, se houver um atributo renda, alguns pacientes podem
não querer preenchê-lo.
• Inexistência de um valor para o atributo em alguns objetos. Esta situação pode
ocorrer se um dos atributos especificar o número de partos e o paciente em questão
for do sexo masculino.
• Problema com o equipamento ou com o processo utilizado para recolha, transmissão
e armazenamento de dados.
Algumas técnicas de ECD podem gerar erro de execução quando um ou mais atributos
do conjunto de treino não apresentam valor. Várias alternativas têm sido propostas para
lidar com esses atributos. As alternativas mais utilizadas são:
• Eliminar os objetos com valores ausentes. Esta alternativa é geralmente empregue
quando um dos atributos com valores ausentes num objeto é o que indica a sua
classe. Esta alternativa não é indicada quando poucos atributos do objeto possuem
valores ausentes, quando o número de atributos com valores ausentes varia muito
entre os objetos com esse problema ou quando o número de objetos que restarem
for reduzido.
• Definir e preencher manualmente valores para os atributos com valores ausentes.
Esta alternativa não é factível quando o número de objetos ou atributos com valores
ausentes for muito grande.
• Utilizar algum método ou heurística para definir automaticamente valores para atributos com valores ausentes. Esta é a alternativa mais utilizada. Diferentes abordagens podem ser utilizadas, como será discutido em seguida.
• Aplicar algoritmos de ECD que lidam internamente com valores ausentes. Este é
o caso, por exemplo, de alguns algoritmos indutores de árvores de decisão (Seção
6.1).
A definição automática de valores para completar os valores ausentes tem seguido três
abordagens diferentes:
• Criar um novo valor para o atributo que indique que o atributo possuía um valor
desconhecido. Esse valor pode ser comum a todos os atributos ou um valor diferente
para cada atributo. O problema desta alternativa é que o algoritmo indutor pode
assumir que o valor desconhecido representa um conceito importante.
50 EXTRAÇÃO DE CONHECIMENTO DE DADOS
• Utilizar a média, ou a moda (no caso de valores simbólicos) ou a mediana dos valores conhecidos para esse atributo. Esta medida pode ser calculada utilizando todos
os objetos ou apenas os objetos da mesma classe do objeto com o atributo a ser
preenchido. Outra variação desta abordagem utiliza o valor mais frequente nos k
objetos mais semelhantes àquele cujo valor ausente necessita de ser estimado. Se
os objetos tiverem uma relação temporal, a medida pode ser calculada utilizando os
objetos associados ao instante imediatamente anterior e posterior ao objeto modificado.
• Utilizar um indutor para estimar o valor do atributo. Neste contexto, o atributo alvo
é o atributo com valores desconhecidos e os demais atributos seriam os atributos de
entrada. A vantagem deste método é justamente a utilização de informação presente
nos restantes atributos para inferir o valor do atributo ausente. Esta abordagem é a
mais popular.
Se os atributos com valores ausentes forem substituídos pelo valor da média (valores
numéricos) ou da moda (valores simbólicos) dos valores de cada um desses atributos, a
Tabela 3.2 seria substituída pela Tabela 3.3. A imputação de valores ausentes pela média
pode gerar inconsistências como, por exemplo, um paciente de 2 anos de idade com peso
igual a 60 quilos.
Tabela 3.3 Conjunto de dados com substituição dos valores ausentes
Idade Sexo Peso Manchas Temp. # Int. Diagnóstico
27 M 79 Inexistentes 38,0 4 Doente
18 F 67 Inexistentes 39,5 4 Doente
49 M 92 Espalhadas 38,0 2 Saudável
18 F 43 Inexistentes 38,5 8 Doente
21 F 52 Uniformes 37,6 1 Saudável
22 F 72 Inexistentes 38,0 3 Doente
27 F 87 Espalhadas 39,0 6 Doente
34 M 67 Uniformes 38,4 2 Saudável
3.6.2 Dados Inconsistentes
Dados inconsistentes são aqueles que possuem valores contraditórios nos seus atributos.
Este tipo de inconsistência pode verificar-se entre valores de atributos de entrada (por
exemplo, valor 120 para o atributo Peso e o valor 3 para o atributo Idade) ou entre todos os
valores dos atributos de entrada e o valor do atributo de saída (por exemplo, dois pacientes
com os mesmos valores para os atributos de entrada e diagnósticos diferentes, um saudável
e o outro doente). Inconsistências podem ser identificadas quando relações conhecidas
entre os atributos são violadas. Por exemplo, quando se sabe que os valores de um atributo
variam de forma inversamente proporcional em relação aos valores de um outro atributo.
PRÉ-PROCESSAMENTO DE DADOS 51
Dados inconsistentes podem resultar do processo de integração de dados de fontes ou
tabelas diferentes, ou da presença de ruído nos dados.
Dados inconsistentes são muitas vezes produzidos no processo de integração de dados.
Por exemplo, diferentes conjuntos de dados podem usar escalas diferentes para a mesma
medida (metros e centímetros) ou codificar de forma diferente um atributo associado ao
tamanho.
Na Tabela 3.4 é ilustrado um exemplo de conjunto de dados em que se verificam inconsistências. Essa inconsistência pode ser observada nos dois objetos destacados, que
apresentam valores de entrada iguais para valores de saída diferentes.
Tabela 3.4 Conjunto de dados com objetos inconsistentes
Idade Sexo Peso Manchas Temp. # Int. Diagnóstico
28 M 79 Concentradas 38,0 2 Doente
18 F 67 Inexistentes 39,5 4 Doente
49 M 92 Espalhadas 38,0 2 Saudável
18 M 43 Inexistentes 38,5 8 Doente
21 F 52 Uniformes 37,6 1 Saudável
22 F 72 Inexistentes 38,0 3 Doente
19 F 87 Espalhadas 39,0 6 Doente
22 F 72 Inexistentes 38,0 3 Saudável
Existem formas de prevenir a ocorrência de inconsistências. Algoritmos simples podem verificar automaticamente se relacionamentos existentes entre atributos são violados.
Quando o conjunto de dados não é muito grande, dados inconsistentes podem ser removidos manualmente.
3.6.3 Dados Redundantes
Um conjunto de dados pode conter quer objetos, quer atributos redundantes. Um objeto é
redundante quando é muito semelhante a um outro objeto do mesmo conjunto de dados,
ou seja, os seus atributos possuem valores muito semelhantes aos atributos de, pelo menos,
outro objeto. No caso extremo, apresentam os mesmos valores para cada um dos atributos.
Por outro lado, um atributo é redundante quando o seu valor pode ser deduzido a partir
do valor de um ou mais atributos. No caso extremo, possui o mesmo valor que um outro
atributo para cada um dos objetos do conjunto de dados.
Para ilustrar a presença de objetos redundantes, observe-se a Tabela 3.5, que apresenta redundância entre três objetos. Nesta tabela, o segundo e o quarto pacientes têm os
mesmos valores para todos os atributos, e por isso são considerados objetos redundantes,
conforme destacado.
Objetos redundantes num conjunto de dados participam mais de uma vez no processo
de ajuste de parâmetros de um modelo contribuindo, assim, mais do que os outros objetos
na definição do modelo final. Isto pode dar ao modelo a falsa impressão de que esse
52 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Tabela 3.5 Conjunto de dados com objetos redundantes
Idade Sexo Peso Manchas Temp. # Int. Diagnóstico
28 M 79 Concentradas 38,0 2 Doente
18 F 67 Inexistentes 39,5 4 Doente
49 M 92 Espalhadas 38,0 2 Saudável
18 F 67 Inexistentes 39,5 4 Doente
18 M 43 Inexistentes 38,5 8 Doente
21 F 52 Uniformes 37,6 1 Saudável
22 F 72 Inexistentes 38,0 3 Doente
19 F 87 Espalhadas 39,0 6 Doente
34 M 67 Uniformes 38,4 2 Saudável
perfil de objeto é mais importante que os demais. Geralmente, é desejável a eliminação de
redundâncias, o que pode ser feita em dois passos:
• Identificação de objetos redundantes;
• Eliminação das redundâncias encontradas.
A eliminação da redundância pode ocorrer pela eliminação dos objetos semelhantes a
um dado objeto ou pela combinação dos valores dos atributos dos objetos semelhantes. A
eliminação de redundância é, geralmente, efetuada no final do processo de limpeza.
Se a redundância não for eliminada, o algoritmo de ECD utilizado pode atribuir ao
objeto repetido uma importância maior que aos demais objetos. Por exemplo, um objeto
A com duas cópias adicionais num conjunto de dados é considerado pelo algoritmo de
indução três vezes mais importante que um objeto B que não está replicado. Se o objeto
A “puxar” a fronteira de decisão para o lado esquerdo e o objeto B para o lado direito,
o movimento da fronteira para o lado esquerdo será três vezes maior do que para o lado
direito. Note-se que algumas técnicas de ECD, como o boosting, utilizam esse artifício
para duplicar a quantidade de exemplos difíceis de ser classificados.
Conforme mencionado no início da seção, além dos objetos, também os atributos podem apresentar redundância. Um atributo é considerado redundante se o seu valor puder
ser estimado a partir de, pelo menos, um dos demais atributos. Isto ocorre quando dois ou
mais atributos têm a mesma informação preditiva. No caso extremo, dois atributos podem
compartilhar o mesmo valor entre si para cada um dos objetos de um conjunto de dados.
Um exemplo simples de redundância de atributos é a presença de um atributo Idade e de
um atributo Data de nascimento num conjunto de dados, pois é fácil definir o valor do
atributo Idade usando o valor do atributo Data de nascimento. Outro exemplo com mais
de dois atributos seria ter um atributo Quantidade de vendas, um atributo Valor por venda
e um atributo Venda total. Neste caso, o valor do atributo Venda total pode ser facilmente
definido a partir do valor dos dois outros atributos. Um atributo redundante pode supervalorizar um dado aspeto dos dados, por estar presente mais de uma vez, ou tornar mais lento
o processo de indução, devido ao maior número de atributos que têm de ser analisados pelo
PRÉ-PROCESSAMENTO DE DADOS 53
algoritmo de ECD. Assim, o desempenho de um algoritmo de ECD geralmente melhora
com a eliminação de atributos redundantes. Muitas vezes a redundância entre atributos
não é tão clara. Atributos redundantes são geralmente eliminados por técnicas de seleção
de atributos, como será visto mais adiante.
A redundância de um atributo está relacionada com a correlação com um ou mais
dos atributos do conjunto de dados. Dois ou mais atributos estão correlacionados quando
apresentam um perfil de variação semelhante para os diferentes objetos. Quanto mais
correlacionados os atributos, maior o grau de redundância. Se a correlação ocorrer entre
um atributo de entrada e o atributo rótulo, então, esse atributo de entrada terá uma grande
influência na predição do valor do atributo rótulo.
Na Tabela 3.6 é ilustrado um conjunto de dados em que um dos atributos é claramente
redundante. Esta tabela adiciona à Tabela 2.1 o atributo número de visitas (#Vis.), que indica quantas vezes um dado paciente esteve no hospital. O atributo redundante, #Vis., está
destacado a negrito. Nestes casos, a redundância é removida mantendo um dos atributos e
eliminando o outro.
Tabela 3.6 Conjunto de dados com atributos redundantes
Idade Sexo Peso Manchas Temp. # Int. # Vis. Diagnóstico
28 M 79 Concentradas 38,0 2 2 Doente
18 F 67 Inexistentes 39,5 4 4 Doente
49 M 92 Espalhadas 38,0 2 2 Saudável
18 M 43 Inexistentes 38,5 8 8 Doente
21 F 52 Uniformes 37,6 1 1 Saudável
22 F 72 Inexistentes 38,0 3 3 Doente
19 F 87 Espalhadas 39,0 6 6 Doente
34 M 67 Uniformes 38,4 2 2 Saudável
3.6.4 Dados com Ruído
Dados com ruído são dados que contêm objetos que, aparentemente, não pertencem à
distribuição que gerou os dados analisados. Ruído pode ser definido como uma variância
ou erro aleatório no valor gerado ou medido para um atributo (Han e Kamber, 2000).
Dados inconsistentes podem resultar da presença de ruído.
Podem ser várias as causas da presença de ruído, as quais foram discutidas anteriormente na Secção 3.6. Dados com ruído podem levar a um superajuste do modelo utilizado,
pois o algoritmo que induz o modelo, pode se ajustar às especificidades relacionadas com
o ruído, em vez da distribuição que gerou os dados. Por outro lado, a eliminação de dados
ruidosos pode conduzir à perda de informação importante. A eliminação desses dados
pode fazer com que algumas regiões do espaço de atributos não sejam consideradas no
processo de indução de hipóteses.
É importante observar que não é possível garantir que o valor de um atributo é ou
54 EXTRAÇÃO DE CONHECIMENTO DE DADOS
não afetado pela presença de ruído, mas apenas ter uma indicação ou indício de que um
dado valor para um atributo pode ter sido gerado com ruído. Um indicador da possível
presença de ruído é a existência de outliers, que são valores que estão além dos limites
aceitáveis, ou são muito diferentes dos demais valores observados para o mesmo atributo,
representando, por exemplo, exceções raramente vistas. Na Tabela 3.7, o valor do atributo
peso do segundo objeto é um outlier.
Tabela 3.7 Conjunto de dados com com ruído
Idade Sexo Peso Manchas Temp. # Int. Diagnóstico
28 M 79 Concentradas 38,0 2 Doente
18 F 300 Inexistentes 39,5 4 Doente
49 M 92 Espalhadas 38,0 2 Saudável
18 M 43 Inexistentes 38,5 8 Doente
21 F 52 Uniformes 37,6 1 Saudável
22 F 72 Inexistentes 38,0 3 Doente
19 F 87 Espalhadas 39,0 6 Doente
34 M 67 Uniformes 38,4 2 Saudável
Existem diversas técnicas de pré-processamento que podem ser aplicadas na deteção
e remoção de ruído. Em Estatística, este problema é normalmente solucionado por meio
de técnicas baseadas em distribuições conhecidas, nas quais o ruído é identificado como
observações que diferem de uma distribuição utilizada na modelação dos dados (Barnett e
Lewis, 1994). O maior problema desta abordagem está em assumir que a distribuição dos
dados é conhecida a priori, o que não reflete a realidade em grande parte das aplicações
práticas.
Uma segunda categoria de técnicas aplicadas em Estatística utiliza o conceito de profundidade na procura de ruído (Barnett e Lewis, 1994; Nuts e Rousseeuw, 1996). Estas
técnicas organizam os dados em camadas. O ruído é identificado como objetos pertencentes a níveis superficiais. Este tipo de estratégia tem custos computacionais, principalmente
para dados de grande dimensão.
Outras técnicas podem ser utilizadas para reduzir o ruído num atributo. De forma
resumida, podem ser reunidas em cinco grupos:
• Técnicas de intervalo: Estas técnicas suavizam o valor de um atributo da seguinte
forma. Primeiro, os valores de um atributo são ordenados. Em seguida, esses valores
são divididos em intervalos, cada uma com o mesmo número de valores. Os valores
num intervalo são substituídos, por exemplo, pela média ou mediana dos valores
presentes no intervalo.
• Técnicas baseadas em agrupamento dos dados: Estas técnicas podem ser utilizadas
tanto para os objetos como para os atributos. No caso dos atributos, os valores
dos atributos são agrupados por uma técnica de agrupamento. Valores de atributos
que não formem um grupo com outros valores são considerados ruído ou outliers.
PRÉ-PROCESSAMENTO DE DADOS 55
O mesmo se aplica a objetos que forem colocados num grupo no qual os demais
objetos pertencem a outra classe.
• Técnicas baseadas em distância: A presença de ruído num ou mais atributos de um
objeto frequentemente provoca o afastamento desse objeto em relação aos restantes
objetos da sua classe. As técnicas baseadas em distância verificam a que classe pertencem os objetos mais próximos de cada objeto x. Se esses objetos mais próximos
pertencem a outra classe, a probabilidade de o objeto x apresentar ruído é elevada.
• Técnicas baseadas em regressão ou classificação: As técnicas baseadas em regressão
utilizam uma função de regressão para, dado um valor com ruído, estimar o seu
valor verdadeiro. Se o valor a ser estimado for simbólico, são utilizadas técnicas de
classificação.
3.6.5 Deteção de Outliers
Numa das definições de outliers globalmente aceites, Hawkins (1980) afirma que um outlier é um objeto que se desvia significativamente dos outros objetos, como se fosse gerado
por um mecanismo diferente. Outliers são também conhecidos como valores aberrantes,
valores anormais ou valores extremos. Outliers são pontos que devem ser identificados
e eventualmente removidos nas análises posteriores. Deteção de anomalias, de avarias,
fraudes, intrusão, etc são tarefas que usualmente recorrem à identificação de outliers.
Dependendo da natureza do outlier, Aggarwal (2013) classificou-os como:
• Outliers pontuais: o tipo mais simples de outlier é uma observação que se afasta
das outras observações. Pode ser um erro de medição ou um comportamento ou
característica anormal do objeto. Por exemplo, uma grande quantidade de água
consumida num mês numa casa pode sugerir um tubo quebrado ou outro problema
na infra-estrutura.
• Outliers contextuais: por vezes, os valores anormais não são óbvios devido ao contexto em que aparecem. Por exemplo, se observarmos um uso muito baixo da eletricidade numa casa familiar em agosto, não pensamos nisso como um outlier: além
de ser um consumo menor normal no verão, a casa provavelmente estava vazia por
algumas semanas, enquanto a família ia de férias. Mas se acontecesse em janeiro,
consideraríamos uma observação anormal.
• Outliers coletivos: outliers podem ser uma sequência de valores. Por exemplo,
vamos considerar a distância de viagem diária de uma pessoa e assumimos que esse
indivíduo viaja mais tempo nos dias úteis do que nos fins de semana. Se, durante sete
dias consecutivos, essa pessoa percorrer distância curtas e depois retornar à distância
usual, podemos assumir que, por algum motivo, essa pessoa não foi trabalhar nessa
semana. Portanto, os valores de distância não foram considerados outliers, pois
eram normais nos fins de semana, mas o fato de aparecerem por sete observações
consecutivas é considerado anormal.
56 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Para identificar outliers univariados, os métodos estatísticos estão entre os mais simples. Assumindo uma distribuição gaussiana e aprendendo os parâmetros a partir dos
dados, os métodos paramétricos identificam os pontos com baixa probabilidade como outliers. Um dos métodos utilizados para detetar esses valores abertos é o método boxplot1, introduzido por Tukey (1977). Com base no primeiro quartil (Q1), no terceiro quartil (Q3) e na faixa interquartil (IQR = Q3 − Q1) dos dados, determina que o intervalo
[Q1 − 1.5 ∗ IQR,Q3 + 1.5 ∗ IQR] contém 99,3% dos dados. Portanto, pontos fora desse
intervalo são considerados como valores outliers moderados, e pontos fora do intervalo
[Q1−3 ∗ IQR,Q3+3 ∗ IQR] são considerado outliers extremos.
Em relação aos valores abertos multi-variados, um dos métodos mais populares para
a deteção não supervisionada é a abordagem baseada na densidade conhecida por Local
Outlier Factor - LOF (Breunig et al., 2000; Aggarwal, 2013). LOF é uma quantificação do
grau de outlier dos pontos. O LOF baseia-se no conceito de densidade local, onde a localidade é dada por k vizinhos mais próximos, cuja distância é usada para estimar a densidade.
Ao comparar a densidade local de um objeto com as densidades locais de seus vizinhos,
pode-se identificar regiões de densidade similar. Portanto, outliers são pontos incluídos
em regiões com uma densidade local substancialmente inferior à da sua vizinhança. A
densidade é calculada como o inverso da distancia média do ponto aos k vizinhos mais
próximos:
densidade(x, k) = '∑y∈N(x,k) distancia(x, y)
|N(x, k)|
(−1
(3.4)
O algoritmo 3.1 apresenta a versão básica do algorimo LOF.
Algoritmo 3.1 Local Outlier Factor - LOF
Entrada: Um conjunto de dados: D = {xi,i = 1,...,n}
Numero de vizinhos a considerar: k
1 para cada x ∈ D faça
2 Determina N(x, k), os k vizinhos de x;
3 Determina densidade(x, k): a densidade de x (Equation 3.4) usando a
vizinhança N(x, k).
4 fim
5 para cada x ∈ D faça
6 Calcula o outlier score(x, k) = densidade(x,k)
∑y∈N(x,k)(densidade(y,k)/|N(x,k)|)
7 fim
1Descrito na página 28.
PRÉ-PROCESSAMENTO DE DADOS 57
3.7 Transformação de Dados
Algumas técnicas de ECD estão limitadas à manipulação de valores de determinado tipo,
por exemplo, apenas valores numéricos ou apenas valores simbólicos. Adicionalmente,
o desempenho de algumas técnicas é influenciado pelo intervalo de variação dos valores
numéricos. Esta secção divide as diferentes técnicas para abordar esse problema em três
partes. A primeira parte descreve técnicas que podem ser utilizadas para converter valores
simbólicos em valores numéricos. Podem ser utilizadas técnicas diferentes, dependente se
os valores simbólicos são nominais ou ordinais. A segunda parte apresenta técnicas para
converter valores numéricos em valores simbólicos. Finalmente, a terceira parte ilustra os
casos em que a conversão não altera o tipo do atributo. Estas técnicas são utilizadas em
atributos numéricos, e procedem á transformação do atributo, por exemplo, mudança de
escala ou de intervalo de valores.
3.7.1 Conversão Simbólico-Numérico
Técnicas como redes neuronais artificiais e máquinas de suporte vetorial, e vários algoritmos de agrupamento, lidam apenas com dados numéricos. Assim, quando o conjunto de
dados utilizado por estas técnicas apresenta atributos simbólicos, os valores destes atributos devem ser convertidos para valores numéricos.
Quando o atributo é do tipo nominal e assume apenas dois valores, isto é, se os valores
denotam a presença ou ausência de uma caraterística ou se apresentam uma relação de
ordem, um dígito binário é suficiente. No primeiro caso, o valor 0 indica a ausência e o
valor 1, a presença da caraterística. No segundo caso, o menor valor ordinal assume o
valor zero e o outro assume o valor 1.
Para um atributo simbólico com mais de dois valores, a técnica utilizada na conversão
depende do fato de o atributo ser nominal ou ordinal. Se não houver uma relação de ordem
entre os valores do atributo, a inexistência de uma relação de ordem deve manter-se nos
valores numéricos gerados. Ou seja, a diferença entre quaisquer dois valores numéricos
deve ser a mesma. Uma forma de garantir esta consistência consiste em codificar cada
valor nominal por uma sequência de c bits, em que c é igual ao número de possíveis
valores ou categorias.
Na codificação 1−de−c, também denominada canónica ou topológica, cada sequência possui apenas um bit com o valor 1 e os demais com o valor zero. A diferença entre as
sequências é definida pela posição que o valor 1 ocupa nelas. Para definir a diferença entre
dois valores, pode ser utilizada a distância de Hamming. A distância de Hamming entre
duas sequências binárias com mesmo número de elementos é igual ao número de posições
em que as sequências apresentam valores diferentes. É fácil verificar que a distância de
Hamming entre qualquer par de valores é igual a 2 (apenas duas posições do string binário
têm valores diferentes).
Nesta codificação, cada posição da sequência binária corresponde a um possível valor
do atributo nominal. Por exemplo, se a sequência binária possui 4 bits, o primeiro bit
58 EXTRAÇÃO DE CONHECIMENTO DE DADOS
corresponde ao primeiro valor, o segundo bit ao segundo valor e assim por diante. Como
apenas um dos bits pode assumir o valor 1, o bit que assumir esse valor sinaliza a presença do valor nominal correspondente àquele bit. A moda do valor de um atributo para
o conjunto de objetos é definida pela posição (bit) da sequência que apresenta o maior
número de valores iguais a 1. Isto indica que o valor correspondente àquela posição é o
que aparece com maior frequência no conjunto de dados.
Na Tabela 3.8 é ilustrada a codificação 1 − de − c para um conjunto de seis valores
nominais, em que cada valor representa uma cor.
Dependendo do número de valores nominais, a sequência binária para representar cada
valor pode ficar muito longa. Considere que se pretende codificar os nomes de países com
a codificação 1−de−c. Como existem 193 países (incluindo o Vaticano), seria necessário
utilizar vetores com 193 elementos.
Tabela 3.8 Codificação 1−de−c
Atributo nominal Código 1−de−c
Azul 100000
Amarelo 010000
Verde 001000
Preto 000100
Marrom 000010
Branco 000001
Uma alternativa consiste na representação dos possíveis valores nominais por um conjunto de pseudo atributos. Os valores dos pseudo atributos podem ser do tipo binário,
inteiro ou real. Para o exemplo anterior, a Tabela 3.9 mostra como é possível transformar
um conjunto de 193 atributos, um para cada país, num conjunto de cinco pseudo atributos,
que podem ser utilizados para identificar cada um dos países (os valores entre parêntesis
denotam se o valor é do tipo binário (b) ou inteiro (i), respetivamente). O primeiro desses
cinco pseudo atributos é o atributo continente, um atributo nominal que pode assumir um
de entre sete valores possíveis. Os outros quatro pseudo atributos, Produto Interno Bruto
(PIB), População, temperatura média anual (TMA) e Área podem assumir um valor inteiro
cada. Tal como para o atributo original, uma combinação de valores para os cinco pseudo
atributos representa um único país.
Tabela 3.9 Pseudoatributos e os seus valores possíveis
Pseudoatributo #Valores
Continente 7 (b)
PIB 1 (i)
População 1 (i)
TMA 1 (i)
Área 1 (i)
PRÉ-PROCESSAMENTO DE DADOS 59
Quando o atributo é do tipo ordinal, existe uma relação de ordem entre os valores do
atributo, e a codificação deve preservar essa ordem. Para isso, deve ser utilizada uma
codificação na qual a ordem dos valores seja evidente. Quando o valor numérico é um
número inteiro ou real, essa transformação é simples e direta: basta ordenar os valores
categóricos ordinais e codificar cada valor de acordo com a sua posição na ordem, como
ilustrado no exemplo da Tabela 3.10.
Tabela 3.10 Conversão de valor ordinal para inteiro
Valor ordinal Valor inteiro
Primeiro 0
Segundo 1
Terceiro 2
Quarto 3
Quinto 4
Sexto 5
Nesta tabela, a distância entre os valores varia de acordo com a respetiva proximidade,
ao contrário do que ocorreu com a codificação ilustrada na Tabela 3.8.
Se for necessário converter valores ordinais em valores binários, pode ser utilizado o
código cinza ou o código termómetro. O código cinza é constantemente utilizado para correções de erro em comunicações digitais. No código termómetro, o aumento dos valores
assemelha-se ao aumento de temperatura num termómetro analógico. Estes dois códigos
são ilustrados na Tabela 3.11. Nesta tabela é possível verificar que, em ambos os casos,
dois valores próximos diferem por apenas um bit (possuem distância de Hamming igual a
1). A tabela também mostra que o código termómetro, em que um valor 1 é acrescentado à
medida que se avança na escala de valores, utiliza sequências binárias maiores (mais bits)
que o código cinza.
Tabela 3.11 Conversão de valor ordinal para binário
Valor ordinal Código cinza Código termómetro
Primeiro 000 00000
Segundo 001 00001
Terceiro 011 00011
Quarto 010 00111
Quinto 110 01111
Sexto 100 11111
60 EXTRAÇÃO DE CONHECIMENTO DE DADOS
3.7.2 Conversão Numérico-Simbólico
Algumas técnicas de ECD foram desenvolvidas para trabalhar com valores qualitativos,
por exemplo, alguns modelos Bayesianos 5. Alguns desses algoritmos podem lidar com
dados quantitativos, mas o seu desempenho pode ficar limitado.
Se o atributo quantitativo for do tipo discreto e binário, com apenas dois valores, a
conversão é trivial. Basta associar um nome a cada valor. Se o atributo original for formado por sequências binárias sem uma relação de ordem entre si, cada sequência pode
ser substituída por um nome ou categoria. Nos restantes casos, o recurso a métodos de
discretização permite transformar atributos quantitativos em qualitativos. Estes métodos
transformam valores numéricos em intervalos ou categorias. A escolha do método de discretização depende do problema de aprendizagem. Métodos de discretização podem ser
utilizados de forma isolada ou composta, quando mais de um método é utilizado. Existe
um grande número de métodos, que podem ser classificados de acordo com diferentes
critérios (Yang et al., 2005).
Quando um atributo quantitativo é discretizado, o conjunto de valores possíveis é dividido em intervalos, e cada intervalo de valores quantitativos é convertido num valor
qualitativo. Em alguns métodos, o utilizador pode influenciar a definição dos intervalos,
definindo valores para parâmetros como, por exemplo, o número máximo de intervalos.
Este tipo de métodos são denominados paramétricos. Os métodos não paramétricos definem os intervalos utilizando apenas as informações presentes nos valores do atributo.
Os métodos de discretização podem ser supervisionados ou não supervisionados. No
primeiro caso, é utilizada a informação sobre a classe dos exemplos. As técnicas supervisionadas geralmente conduzem a melhores resultados, uma vez que a definição dos
intervalos sem conhecimento das classes pode originar mistura de classes. Uma abordagem supervisionada simples seria escolher pontos de corte que maximizam a pureza dos
intervalos. Isso pode ser feito utilizando o conceito de entropia. Note-se que discretizar
cada atributo separadamente conduz, em geral, a resultados subótimos.
A definição do processo de mapeamento dos valores dos atributos quantitativos em
valores qualitativos, a definição do tamanho dos intervalos ou a quantidade de valores nos
intervalos fica geralmente a cargo do método de discretização. Algumas das estratégias
utilizadas pelos diferentes métodos são seguidamente apresentadas:
• Largura igual: Divide o intervalo de valores original em subintervalos com a mesma
largura. O desempenho desta estratégia pode ser afetado pela presença de outliers.
• Frequência igual: Atribui o mesmo número de objetos a cada subintervalo. Esta
estratégia pode gerar intervalos de tamanhos muito diferentes.
• Utilização de um algoritmo de agrupamento de dados.
• Inspeção visual.
PRÉ-PROCESSAMENTO DE DADOS 61
3.7.3 Transformação de Atributos Numéricos
Algumas vezes, o valor numérico de um atributo necessita de ser transformado noutro
valor numérico. Isto geralmente ocorre quando os limites inferior e superior dos valores
dos atributos são muito diferentes, o que leva a uma grande variação de valores ou, ainda,
quando vários atributos se encontram expressos em escalas diferentes. Esta transformação
é geralmente realizada para evitar que um atributo predomine sobre outro.
Quando necessário, a operação de transformação é aplicada aos valores de um dado
atributo. A título de exemplo, considere que, para um problema de decisão, apenas a magnitude, e não o sinal, é relevante. Uma transformação necessária consistiria em converter
os valores desse atributo para seu valor absoluto.
Outra transformação que é muito utilizada é a normalização de dados. A normalização de dados é recomendável quando os limites de valores de atributos distintos são
muito diferentes, sendo de evitar que um atributo predomine sobre o outro (a menos que
isso seja importante). Quando recomendada, a normalização é aplicada a cada atributo
individualmente e pode ocorrer de duas formas:
• Amplitude;
• Distribuição.
A normalização por amplitude pode ser realizada através da alteração da escala ou
padronização. A primeira define uma nova escala de valores (limites mínimo e máximo)
para cada atributo. A segunda define um valor central e um valor de dispersão comuns
para todos os atributos.
Na normalização por alteração da escala, também chamada normalização min-max,
são inicialmente definidos os valores mínimo (min) e máximo (max) para os novos valores de cada atributo. Depois, as seguintes operações são realizadas para cada atributo.
Primeiro, o menor valor do atributo, (menor), é subtraído a cada valor. Cada valor resultante é, em seguida, dividido pela diferença entre o maior e o menor valores originais do
atributo, (maior−menor). Cada novo valor é depois multiplicado pela diferença entre os
valores limites da nova escala, max−min. No final, o valor min é somado a cada valor
produzido. Estas operações são ilustradas pela Equação 3.5. Para que os limites superior
e inferior sejam 1 e 0, respetivamente, basta fazer max = 1 e min = 0.
vNovo = min+
vAtual −menor
maior−menor(max−min) (3.5)
Para a normalização por padronização, a cada valor do atributo a ser normalizado é
adicionada ou subtraída uma medida de localização, sendo o valor resultante multiplicado
ou dividido por uma medida de escala. Através desta operação, diferentes atributos podem
apresentar limites inferiores e superiores diferentes, mas terão os mesmos valores para as
medidas de escala e dispersão. Se as medidas de localização e de escala forem a média (µ)
e a variância (σ), respetivamente, os valores de um atributo são convertidos para um novo
conjunto de valores com média0evariância 1. A Equação 3.6 resume esta transformação.
62 EXTRAÇÃO DE CONHECIMENTO DE DADOS
vNovo = vAtual −µ
σ
(3.6)
Geralmente, é preferível padronizar a reescalar, pois a padronização lida melhor com
outliers. Durante a operação de normalização, é possível fazer com que os atributos mais
importantes possuam limites maiores. Para isso, basta fazer com que a padronização gere
um intervalo maior (por exemplo, o limite máximo de um atributo importante pode ser o
dobro do limite máximo utilizado para os restantes atributos) ou que a medida de reescala
gere uma maior variância para um atributo importante (por exemplo, esse atributo pode ter
uma variância que é o dobro da variância dos restantes atributos, o que faz com que sua
amplitude de valores seja o dobro da faixa utilizada pelos outros atributos).
A normalização por distribuição altera a escala de valores de um atributo. Um exemplo
desse tipo de normalização consiste na aplicação de uma função para ordenar os valores
do atributo a ser normalizado, e a substituição de cada valor pela posição que ele ocupa
no ranking (por exemplo, a aplicação desta normalização aos valores 1, 5, 9 e 3 gera,
respetivamente, os valores 1, 3, 4 e 2). Se todos os valores originais forem distintos, o
resultado é uma distribuição uniforme.
Outro tipo de transformação para atributos de um mesmo tipo é a tradução, em que
o valor de um atributo de um dado tipo é traduzido para um valor do mesmo tipo, que
seja mais facilmente manipulável. Por exemplo, a conversão de um atributo com data de
nascimento para idade, de graus Celsius para Fahrenheit, ou da localização dada por um
aparelho de GPS para código postal.
3.8 Redução de Dimensionalidade
Muitos problemas que podem ser tratados através de técnicas de ECD apresentam um
elevado número de atributos. Um exemplo de problemas em que o número de atributos é
muito grande são as aplicações de reconhecimento de imagens. Se cada pixel 2 da imagem
for considerado um atributo, cada imagem ou instância de uma imagem com 1024 por 1024
pixels teria mais de um milhão de atributos. Outro exemplo são os dados de expressão
genética, que geralmente apresentam algumas dezenas de objetos (amostras), cada um
com milhares de atributos (genes). Poucas técnicas de ECD são capazes de lidar com um
número tão elevado de atributos.
O efeito do número muito elevado de atributos em algoritmos de ECD é descrito pelo
problema da maldição da dimensionalidade. Se cada atributo for visto como uma coordenada num espaço d-dimensional, em que d é o número de atributos, o hipervolume que
representa esse espaço cresce exponencialmente com a adição de novos atributos. Para
clarificar o problema, considere um conjunto de dados em que cada objeto possui apenas
um atributo e que esse atributo pode assumir um de entre 10 valores. Esse conjunto de dados pode ter então 101 ou 10 objetos diferentes, um para cada valor diferente do atributo.
2Um pixel é a unidade básica de uma imagem.
PRÉ-PROCESSAMENTO DE DADOS 63
Se o número de atributos aumentar para cinco, o número de objetos possíveis passa a ser
105, que é um número de objetos possíveis muito superior ao caso em que apenas um atributo foi utilizado. Uma forma de minimizar o impacto do problema da dimensionalidade
consiste em combinar, ou eliminar, parte dos atributos irrelevantes.
Em muitos algoritmos de ECD, para que os dados com um número elevado de atributos
possam ser utilizados, o número de atributos precisa ser reduzido. A redução do número
de atributos pode ainda melhorar o desempenho do modelo induzido, reduzir o seu custo
computacional e facilitar a interpretação dos resultados obtidos. Diferentes técnicas originárias de áreas como Reconhecimento de Padrões, Estatística e Teoria da Informação
podem ser utilizadas para a redução do número de atributos. Essas técnicas podem ser
divididas em duas grandes abordagens:
• Agregação;
• Seleção de Atributos.
Enquanto as técnicas de agregação substituem os atributos originais por novos atributos
formados pela combinação de grupos de atributos, as técnicas de seleção mantêm uma
parte dos atributos originais e descartam os demais atributos.
3.8.1 Agregação
As principais técnicas utilizadas para reduzir as dimensões por agregação combinam os
atributos originais por meio de funções lineares ou não lineares. Uma das técnicas mais
conhecidas é a Análise de Componentes Principais (PCA, do inglês Principal Component
Analysis) (Pearson, 1901). A técnica PCA é um procedimento matemático que utiliza uma
transformação ortogonal para converter um conjunto de observações de variáveis possivelmente correlacionadas num conjunto de novos valores de variáveis não-correlacionadas.
As novas variáveis são combinações lineares das variáveis originais, e são designadas por
componentes principais.
Nas técnicas de agregação, a combinação de atributos, conduz à perda dos valores
originais. Em várias aplicações, por exemplo, nas áreas de biologia, finanças, medicina e
monitorização ambiental, geralmente é importante preservar os valores dos atributos para
que os resultados obtidos possam ser interpretados, associando os resultados produzidos
por uma técnica estatística ou de ECD aos valores dos atributos. Por esse motivo, nestas
áreas, é mais frequente reduzir o número de atributos pelo recurso a técnicas de seleção.
3.8.2 Seleção de Atributos
Várias aplicações reais apresentam um elevado número de atributos. Além do problema
da maldição da dimensionalidade, alguns destes atributos podem ser irrelevantes ou redundantes.
A seleção de atributos permite:
64 EXTRAÇÃO DE CONHECIMENTO DE DADOS
• Identificar atributos importantes;
• Melhorar o desempenho de várias técnicas de ECD;
• Reduzir a necessidade de memória e tempo de processamento;
• Eliminar atributos irrelevantes e reduzir ruído;
• Lidar com a maldição da dimensionalidade;
• Simplificar o modelo gerado e facilitar a sua compreensão;
• Facilitar a visualização dos dados;
• Reduzir o custo de recolha de dados.
Conforme visto anteriormente, alguns atributos são claramente redundantes ou irrelevantes, podendo ser manualmente eliminados. No entanto, na prática, vários atributos
passíveis de eliminação não são facilmente identificados, o que torna pouco eficiente o uso
exclusivo de técnicas visuais. Possíveis razões subjacentes a esta dificuldade incluem:
• Número muito elevado de exemplos;
• Número muito elevado de atributos;
• Relações complexas entre atributos, que dificultam a descoberta de relações entre
eles.
Para lidar com estes casos, diversas técnicas automáticas têm sido propostas na literatura para a seleção de atributos. Estas técnicas procuram um subconjunto ótimo de
atributos de acordo com um dado critério. As técnicas propostas podem ser classificadas
de diferentes formas. Uma delas diz respeito à avaliação do conjunto de atributos selecionados. Neste caso específico, as técnicas existentes podem estar integradas num algoritmo
de indução ou ser independentes do algoritmo. Para avaliar a qualidade ou desempenho de
um subconjunto de atributos, três abordagens têm sido utilizadas:
• Embutida;
• Baseada em filtro;
• Baseada em wrapper.
Na abordagem embutida, a seleção do subconjunto é embutida ou integrada no próprio algoritmo de aprendizagem. As árvores de decisão (Seção 6.1) realizam este tipo de
seleção interna de atributos.
Nas abordagens baseadas em filtros, como o próprio nome indica, é utilizado um filtro,
numa etapa de pré-processamento, sobre o conjunto de atributos original, que seleciona
um subconjunto de atributos, sem levar em consideração o algoritmo de aprendizagem
que utilizará esse subconjunto. As técnicas que seguem esta abordagem analisam, por
exemplo, a correlação entre os atributos. A medida de correlação mais utilizada é a correlação de Pearson, definida pela Equação 3.7, em que ¯xi = ∑d
l=1 xl
i/d. Permite identificar
PRÉ-PROCESSAMENTO DE DADOS 65
atributos redundantes: aqueles com elevada correlação entre si. Esta medida é insensível
a diferenças na magnitude dos atributos, sendo muito usada para determinar a semelhança
entre objetos em áreas como Bioinformática, em que apenas o padrão de variação dos
atributos dos objetos é importante.
pearson(xi,xj) = covariância(xi,xj)
)variância(xi)variância(xj)
= ∑d
l=1 (xl
i −x¯i)(xl
j −x¯j)
*
(∑d
k=1 (xl
i −x¯i)2 ∑d
l=1 (xl
j −x¯j)2)
(3.7)
As abordagens baseadas em wrappers utilizam o próprio algoritmo de aprendizagem
como uma caixa-preta para a seleção de atributos. Geralmente são utilizadas juntamente
com uma técnica de amostragem. Para cada possível subconjunto, o algoritmo é consultado e o subconjunto que apresentar a melhor combinação entre redução da taxa de erro e
redução do número de atributos é em geral selecionado.
Algumas vantagens da abordagem baseada em filtro podem ser destacadas:
• como o processo de seleção não depende de nenhum indutor, as caraterísticas selecionadas podem ser utilizadas por diferentes algoritmos de ECD;
• as heurísticas utilizadas para avaliar um subconjunto são computacionalmente ’leves’, pelo que a aplicação de filtros pode ser bastante eficiente;
• os filtros conseguem lidar eficientemente com uma grande quantidade de dados.
A principal desvantagem dos filtros refere-se à sua independência em relação ao algoritmo de ECD. Como a seleção de atributos e a classificação são processos separados, o
viés3 de um não interage com o viés de outro, o que pode levar à construção de classificadores com um desempenho aquém do desejado.
As técnicas baseadas em wrapper representam uma alternativa simples e poderosa para
selecionar atributos. Em geral, as técnicas embutidas fazem melhor uso dos dados disponíveis do que as técnicas baseadas em wrapper. Além disso, por não precisar retreinar um
algoritmo de ECD para cada novo conjunto de atributos, as técnicas embutidas são, em
geral, mais rápidas (Guyon e Elisseeff, 2003).
Outra forma de análise, está relacionada com o fato de a seleção dos atributos ser feita
de forma individual ou coletiva. No primeiro caso, os atributos são ordenados de acordo
com a sua relevância para discriminar os objetos das diferentes classes. Boa parte das
técnicas de seriação pode ser apenas utilizada em problemas de classificação binária (pro3O viés refere-se à tendência dos algoritmos em favorecer, de forma sistemática, determinadas situações,
dentre as muitas disponíveis. Em ECD, um exemplo pode ser a preferência de certos algoritmos em construir
classificadores mais ou menos complexos. Na seleção de atributos, um algoritmo pode preferir atributos que
possuam certas propriedades.
66 EXTRAÇÃO DE CONHECIMENTO DE DADOS
blemas com duas classes). A segunda alternativa seleciona um subconjunto dos atributos
originais que melhor separe os exemplos das diferentes classes.
Finalmente, algumas técnicas de seleção de atributos utilizam a informação sobre a
classe, sendo denominadas supervisionadas, e outras, por não utilizar essa informação,
são denominadas de não supervisionadas.
A seleção de atributos, tanto por ordenação, como por seleção de subconjuntos, e utilizando ou não informação sobre a classe, pode ser feita quer com as abordagens filtro quer
por wrapper. A abordagem embutida trabalha com seleção de subconjuntos. Na Seção seguinte, apresentamos detalhes sobre as técnicas de ordenação e seleção por subconjuntos.
3.8.3 Técnicas de Seriação
A seriação de atributos pode ser encarada como uma forma simples de seleção, em que os
atributos são ordenados de acordo com a sua relevância para um dado critério, por exemplo, classificação dos objetos nas diferentes classes. Em problemas de classificação, os
atributos no topo da seriação são selecionados para utilização pelo classificador. Grande
parte das técnicas existentes foi desenvolvida para a redução do número de genes em problemas de análise de expressão genética por meio de microarranjos. Nestes problemas,
cada gene pode ser visto como um atributo e cada objeto é descrito por milhares de genes
ou atributos.
A escolha da melhor abordagem para a seleção de atributos depende das propriedades
a serem medidas (Dopazo et al., 2001). Várias das abordagens descritas na literatura
são técnicas para ordenação (ranking), que atribuem uma pontuação ou medida a cada
subconjunto de atributos. Algumas destas técnicas avaliam a semelhança (medidas de
correlação), enquanto que outras avaliam a diferença (medidas de distância) entre vetores.
As medidas podem ou não considerar a informação sobre a classe.
As medidas podem ainda ser divididas em paramétricas e não paramétricas. As medidas paramétricas assumem uma distribuição estatística para cada grupo ou classe. As
medidas não paramétricas não fazem essa assunção, sendo mais robustas. As medidas não
paramétricas geralmente especificam uma hipótese em termos de distribuições populacionais, em vez de parâmetros, como a média e o desvio padrão. Estas medidas conseguem
detetar diferenças entre populações quase tão bem quanto as medidas paramétricas, quando
assunções como a normalidade necessitam de ser verificadas. Quando estas assunções não
são satisfeitas, as medidas não paramétricas são, potencialmente, mais poderosas que as
paramétricas na deteção de diferenças entre populações.
Note-se que, no caso da seriação dependente de classe, o primeiro atributo é aquele
que melhor discrimina os objetos das diferentes classes, o segundo é o segundo melhor
atributo a discriminar as classes, e assim sucessivamente. Na seleção do subconjunto, os
atributos que fazem parte do subconjunto selecionado não estariam necessariamente no
topo da lista se uma técnica de seriação fosse utilizada. Neste caso, o que importa é como
os atributos selecionados atuam de forma coletiva. Isto ocorre, por exemplo, devido a
complementaridades e interações entre os atributos.
3.8.4 Técnicas de Seleção de Subconjunto
A seleção de um subconjunto de atributos é um processo computacionalmente mais custoso que a ordenação dos atributos. Esta desvantagem acentua-se com o crescimento do
número de atributos. A seleção de subconjuntos de atributos pode ser intratável quando
o número de atributos é muito elevado. Uma alternativa, consiste em ordenar, em primeiro lugar, os d atributos originais e, em seguida, selecionar um subconjunto a partir dos
dreduzido atributos melhor posicionados.
Por incorporar o viés do classificador, as técnicas baseadas em wrapper conseguem
obter, em geral, um conjunto de atributos que conduz a um melhor desempenho do modelo.
A seleção de um subconjunto de atributos pode ser interpretada como um problema de
procura, em que, cada ponto no espaço de procura é visto como um possível subconjunto
de atributos. Nesta perspetiva, um método de seleção deve definir (Blum e Langley, 1997):
• Qual(is) será(ão) o(s) ponto(s) de partida, ou a direção em que a procura será realizada;
• Que estratégia de procura será adotada;
• Qual o critério a ser utilizado na avaliação dos subconjuntos gerados;
• Qual será o critério de paragem.
Os critérios de avaliação já foram discutidos anteriormente, que são as abordagens
filtro, wrapper ou embutida. A seguir serão discutidos os outros três aspetos. No que
respeita ao ponto de partida e á direção da procura, quatro diferentes alternativas podem
ser adotadas:
• Eliminação para trás (backward elimination), que inicia com todos os atributos e
remove um atributo de cada vez.
• Geração para a frente (forward generation), que começa sem nenhum atributo e
adiciona um atributo de cada vez.
• Geração bidirecional (bidirectional generation), em que a procura pode começar em
qualquer ponto, e os atributos podem ser adicionados e/ou removidos.
• Geração estocástica (random generation), quando o ponto de partida da procura e
os atributos a serem removidos ou adicionados são decididos de forma estocástica.
Em relação à estratégia de procura a ser adotada, as principais abordagens são:
• Procura completa (exponencial ou exaustiva), que avalia todos os possíveis subconjuntos.
• Procura heurística (sequencial), que utiliza regras e métodos para conduzir a procura
e que não garante que uma solução ótima seja encontrada.
• Procura não determinística, que está relacionada com a geração estocástica. Neste
caso, embora possa encontrar uma boa solução, não é possível garantir que será
encontrada a melhor solução possível.
68 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Finalmente, no que respeita a terminar a procura, pode ser conduzida uma procura
exaustiva, em que a procura termina quando todos os subconjuntos forem testados, ou pode
ser adotado um critério de paragem, que define quando terminar a procura pelo melhor
subconjunto de atributos. Este critério pode ser, por exemplo, um número máximo de
alternativas testadas, um número de atributos a serem selecionados sem degradação do
desempenho do classificador ou o tempo de processamento.
3.9 Considerações Finais
A aplicação prévia de técnicas de pré-processamento ao conjunto de dados pode facilitar
o processo de aprendizagem e melhorar o desempenho dos algoritmos de ECD, uma vez
que estas técnicas podem eliminar ou reduzir problemas presentes nos dados.
Neste capítulo foram apresentadas diversas técnicas utilizadas no pré-processamento
de dados. Encetando com a eliminação manual de atributos que se mostrarem claramente
desnecessários para o uso dos dados por algoritmos de ECD, discutiu-se a integração de
dados provenientes de diferentes fontes, a utilização de técnicas de amostragem para selecionar subconjuntos representativos de dados, como lidar com dados desbalanceados e
como operações de limpeza de dados podem tratar dados incompletos, inconsistentes, redundantes e com ruído. De seguida, foram apresentadas alternativas para a transformação
de dados, com o intuito de facilitar o seu uso por diferentes algoritmos de ECD, tais como a
conversão de dados de um tipo para outro e a normalização de dados numéricos. Por fim,
foi observada a importância da redução da dimensionalidade dos dados, principalmente
quando o número de atributos é muito elevado.
Os problemas mencionados são muito frequentes em conjuntos de dados reais e o
pré-processamento conduz, em geral, a um melhor desempenho do algoritmo de ECD
utilizado. Por esse motivo, a etapa de pré-processamento assume uma importância preponderante em aplicações reais de ECD.
Parte II
Modelos Preditivos

Introdução
Um algoritmo de ECD preditivo é uma função que, dado um conjunto de exemplos etiquetados, constrói um estimador. O rótulo, ou etiqueta, toma valores num domínio conhecido.
Se o domínio for um conjunto de valores nominais, estamos perante um problema de classificação, e o estimador gerado é um classificador. Se o domínio for um conjunto infinito
e ordenado de valores, estamos perante um problema de regressão, que induz um regressor. Um classificador (ou regressor) é, também, uma função que, dado um exemplo não
rotulado, atribui a esse exemplo uma das possíveis classes (ou um valor real) (Dietterich,
1998).
Uma definição formal é: dado um conjunto de observações de pares D = {(xi, f(xi)),i =
1,...,n}, em que f representa uma função desconhecida, um algoritmo de ECD preditivo
aprende uma aproximação ˆf da função desconhecida f . Essa função aproximada, ˆf , permite estimar o valor de f para novas observações de x. De acordo com a natureza de f , é
comum distinguir duas situações possíveis:
• Classificação: yi = f(xi) ∈ {c1,...,cm}, ou seja, f(xi) assume valores num conjunto
discreto, não ordenado;
• Regressão: yi = f(xi) ∈ ℜ, ou seja, f(xi) assume valores num conjunto infinito e
ordenado.
A Figura 3.1(a) ilustra um cenário onde se pretende discriminar pessoas saudáveis de
pessoas doentes usando a informação proveniente de dois exames. O objetivo é encontrar
uma fronteira de decisão que separe os exemplos de uma classe dos exemplos da outra
classe. Se os exemplos de uma classe forem linearmente separáveis dos exemplos da outra
classe, uma vez que os exemplos são caraterizados por dois atributos, a fronteira de decisão
pode ser uma reta - combinação linear dos atributos.
Diferentes algoritmos de ECD encontram diferentes fronteiras de decisão. Além disso,
diferenças no conjunto de treino, alterações na ordem de apresentação dos exemplos durante o treino e processos estocásticos internos podem fazer com que um mesmo algoritmo
de ECD encontre fronteiras de decisão diferentes.
A Figura 3.1(b) ilustra um caso de regressão em que o objetivo consiste em aprender uma função que estime o caudal da água de um dado rio ao longo do tempo. Neste
caso é utilizado, apenas, um atributo de entrada. Unindo os pontos formados pelo par
72 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Exame 1
Classe
saudável
Classe
doente
Exame 2
(a) Classificação
Ano
Vazão
Função aproximada
(b) Regressão
Figura 3.1 Gráficos ilustrativos das tarefas de classificação e regressão.
Tabela 3.12 Exemplo de um conjunto de dados para um problema de classificação
Tamanho (P) Largura (P) Tamanho (S) Largura (S) Espécie
5,1 3,5 1,4 0,2 Setosa
4,9 3,0 1,4 0,2 Setosa
7,0 3,2 4,7 1,4 Versicolor
6,4 3,2 4,5 1,5 Versicolor
6,3 3,3 6,0 2,5 Virgínica
5,8 2,7 5,1 1,9 Virgínica
{dia,caudal}, obtemos uma curva. Espera-se que essa curva se aproxime da curva verdadeira da função que, dado um determinado dia, retorna uma estimativa do caudal.
Nas Tabelas 3.12 e 3.13 são apresentados exemplos de dois conjuntos de dados. O
primeiro conjunto (Tabela 3.12) apresenta exemplos de dados para um problema de classificação, o problema iris, abordado no Capítulo 2. O atributo alvo, que aparece na última
coluna, assume valores discretos e não ordenados. O segundo conjunto, apresentado na
Tabela 3.13, refere-se a um problema de regressão. O atributo alvo, que aparece na última
coluna, assume valores contínuos e ordenados.
Nos dois conjuntos, cada linha corresponde a um objeto ou observação. A última
coluna apresenta o valor da função f para essa observação, que corresponde ao atributo
Tabela 3.13 Exemplo de um conjunto de dados para um problema de regressão
Fertilidade Agricultura Educação Renda Mortalidade
80,2 17,0 12 9,9 22,2
83,1 45,1 9 84,8 22,2
92,5 39,7 5 93,4 20,2
85,8 36,5 7 33,7 20,3
76,9 43,5 15 5,2 20,6
PRÉ-PROCESSAMENTO DE DADOS 73
alvo. Esta coluna é também designada por variável dependente ou variável objetivo. No
caso particular de problemas de classificação, é designada por classe. As colunas restantes
são designadas por atributos de entrada, atributos preditivos ou variáveis independentes.
Os atributos serão utilizados como entrada do algoritmo na realização da predição.
O primeiro conjunto de dados, denominado iris, é bastante utilizado para ilustrar o
uso de técnicas de ECD na solução de problemas de classificação. Com base em algumas
caraterísticas externas de flores de uma planta denominada íris, nomeadamente tamanhos
e larguras das suas pétalas e sépalas, é possível separá-las numa de três classes, que designam diferentes variedades dessa planta: setosa, versicolor e virgínica. Um dos motivos
para esse conjunto de dados ser largamente utilizado está relacionado com o fato de se
conhecer de antemão que uma das classes, a variedade setosa, ser linearmente separável
das outras duas. No caso das variedades versicolor e virgínica, por sua vez, a separação
existe, mas não é linear e de fato há vários objetos dessas classes próximos entre si.
O segundo conjunto de dados, apresentado na Figura 3.13, é denominado swiss. Para
este conjunto, deseja-se relacionar estatísticas de uma população, tais como nível de educação, taxa de fertilidade, entre outras, à previsão da sua taxa de mortalidade. Em ambos
os casos, o objetivo da aprendizagem preditiva é aprender uma função ˆf(x) que mapeia
as variáveis independentes, os atributos de entrada, na variável objetivo, o atributo alvo.
A função ˆf pode assumir diferentes formas. Por exemplo, combinações lineares ou não
lineares dos atributos de entrada, funções por ramos, expressões lógicas etc., que são estudadas nos capítulos seguintes. É importante notar que ˆf é uma aproximação de uma
função desconhecida f , ou seja podem existir x para os quais ˆf(x) '= f(x). A estimativa
da qualidade de um modelo preditivo é dada pelo custo associado às previsões do modelo.
Dependendo do tipo de problema, classificação ou regressão, são utilizadas diferentes funções de custo. No caso de problemas de classificação, é usual utilizar a função de custo
0 − 1: o custo de uma previsão incorreta ( ˆf(x) '= f(x)) é 1, e o custo de uma previsão
correta ( ˆf(x) = f(x)) é 0. Em problemas de regressão, é comum utilizar o erro quadrático
médio. As funções de custo e as metodologias para as estimar são estudadas no Capítulo 9.
Considere, para simplificar, um problema de classificação com duas classes. Assuma
que é conhecida a função densidade de probabilidade (pdf, do inglês probability density
function) para cada classe. A Figura 3.2 mostra as duas pdfs (uma para cada classe) num
problema definido por um único atributo de entrada. O melhor classificador possível divide
o domínio da variável no ponto de interseção das duas pdfs, ou seja, classifica um objeto
na classe com maior função densidade de probabilidade. A área sombreada representa o
erro cometido por esse classificador. É intuitivo que esse classificador tem erro mínimo:
movendo em qualquer direção na superfície de decisão, o erro sempre cresce. O erro desse
classificador é conhecido como erro de Bayes ótimo e é um mínimo teórico da capacidade
de generalização de qualquer classificador. No Capítulo 9 são indicadas metodologias para
estimar a qualidade de ˆf , ou seja, quão aproximado é ˆf de f .
Na literatura de aprendizagem automática é usual distinguir modelos generativos de
modelos discriminativos. Os modelos generativos estimam uma função densidade de probabilidade condicionada à classe ci. Um exemplo de teste é classificado na classe que
74 EXTRAÇÃO DE CONHECIMENTO DE DADOS
PCPx C ( )( ) i i |
C1 C2 C1 C2
PCPx C ( )( ) i i |
Erro
reduzível
x x
Figura 3.2 A figura ilustra um problema de decisão com duas classes, mostrando a função distribuição de probabilidade para cada classe. A figura da esquerda representa a superfície de decisão
ótima. A área cinzenta corresponde à probabilidade de erro ao decidir por C1 quando a classe correta
é C2; a área cinza-escura é o erro oposto. Se movemos a superfície de decisão, como se vê na figura
da direita, o erro sempre aumenta.
maximiza a probabilidade a posteriori. São designados generativos porque assumem que
os exemplos de cada classe são gerados por uma função densidade de probabilidade condicionada. O naive Bayes (Duda et al., 2001) é um exemplo clássico deste grupo de modelos.
Os modelos discriminativos não modelam a distribuição de probabilidade, mas calculam
diretamente as probabilidades a posteriori. Os modelos tradicionais incluem árvores de
decisão (Quinlan, 1993), redes neuronais (Mitchell, 1997) e vizinhos mais próximos (Aha
et al., 1991).
Nos capítulos seguintes serão descritos os principais métodos preditivos de ECD. Estão organizados em métodos baseados em distâncias, métodos probabilísticos, métodos
baseados em procura e métodos baseados em otimização. No Capítulo 8, são apresentadas
estratégias de combinar métodos de classificação em modelos preditivos múltiplos. Finalmente, são discutidas formas de desenhar experiências de ECD e de analisar os resultados
obtidos.
Capítulo 4
Métodos Baseados em Distâncias
4.1 Introdução
Neste capítulo serão apresentadas técnicas de ECD que consideram a proximidade entre
os dados na realização de predições. A hipótese de base é que dados similares tendem a
estar concentrados na mesma região no espaço de entrada. Alternativamente, dados que
não são similares estarão distantes entre si.
A título ilustrativo, na Figura 4.1 é apresentada a projeção em duas dimensões do
conjunto de dados iris. Os objetos da mesma classe estão representados pela mesma
cor. É fácil observar visualmente a existência de áreas densas com objetos pertencentes
à mesma classe, evidenciando que a distância entre os objetos está relacionada com a
definição de suas classes.
Um método baseado em distância utilizado com frequência é o algoritmo dos vizinhos
mais próximos. Este algoritmo é o mais simples de todos os algoritmos de aprendizagem
automática. A intuição por trás do algoritmo é:
Objetos do mesmo conceito são semelhantes entre si.
O algoritmo dos vizinhos mais próximos classifica um novo objeto com base nos exemplos do conjunto de treino que são próximos desse objeto. É um algoritmo preguiçoso
(lazy), porque não aprende um modelo compacto para os dados, apenas memoriza os objetos de treino. Uma vantagem deste algoritmo é que pode ser utilizado tanto em problemas
de classificação como em problemas de regressão de maneira direta, sem necessidade de
alterações significativas. Para algumas técnicas, como as Máquinas de Vetores de Suporte,
essa generalização envolve grandes alterações no algoritmo de aprendizagem, conforme
será discutido no Capítulo 7.
Neste capítulo serão apresentados alguns métodos de aprendizagem baseados em distâncias. Na Seção 4.2 apresentamos a descrição do funcionamento do algoritmo dos vizinhos mais próximos utilizando o caso mais simples, isto é apenas um vizinho. A Seção
4.3 apresenta o algoritmo k-NN, que é analisado na Seção 4.4. Variações recentemente
desenvolvidas para o algoritmo k-NN são objeto da Seção 4.5. A Seção 4.6 apresenta uma
76 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Tamanho
da sépala
Largura
da sépala
Largura
da pétala
Tamanho
da pétala
4,5 5,5 6,5 7,5 1 2 3 4 5 6 7
0,5
1,5
2,5
1
2
3
4
5
6
7
2,0 3,0 4,0
2,0
3,0
4,0
4,5
5,5
6,5
7,5
0,5 1,5 2,5
Figura 4.1 Projeção sobre o plano definido por dois atributos dos objetos do conjunto de dados
iris.
metodologia de ECD que utiliza distância para recuperar a solução de problemas passados
que sejam semelhantes ao problema que se deseja resolver, conhecida como raciocínio baseado em casos. A última Seção apresenta as considerações finais para os temas estudados
neste capítulo.
4.2 O Algoritmo do 1-Vizinho Mais Próximo
O algoritmo dos vizinhos mais próximos tem variações definidas pelo número de vizinhos
considerados. Dessas variações, a mais simples é o algoritmo 1-vizinho mais próximo
(1-NN, do inglês 1-Nearest Neighbour).
Nesse algoritmo, cada objeto representa um ponto num espaço definido pelos atributos, denominado espaço de entrada, como ilustrado no Capítulo 1. Definindo uma métrica
nesse espaço, é possível calcular as distâncias entre cada dois pontos. A métrica mais usual
é a distância euclidiana, dada pela Equação 4.1, em que xi e xj são dois objetos representados por vetores no espaço ℜd, e xl
i e xl
j são elementos desses vetores, que correspondem
aos valores da coordenada l (atributos).
MÉTODOS BASEADOS EM DISTÂNCIAS 77
d(xi,xj) =
+,,-
d
∑
l=1
(xl
i −xl
j
)2 (4.1)
Como dito anteriormente, o algoritmo 1-NN é muito simples, e está ilustrado no Algoritmo 4.1. Na fase de treino, o algoritmo memoriza os exemplos rotulados do conjunto
de treino. Para classificar um exemplo não rotulado, ou seja, cuja classe não é conhecida,
é calculada a distância entre o vetor de valores de atributos e cada exemplo rotulado em
memória. O rótulo da classe associado ao exemplo de treino mais próximo do exemplo de
teste é utilizado para classificar o novo exemplo.
Algoritmo 4.1 Versão básica do algoritmo 1-vizinho mais próximo
Entrada: Um conjunto de treino: D = {(xi, yi),i = 1,...,n}
Um objeto de teste a ser classificado: t = {xt, yt =?}
A função de distância entre objetos: d(xa,xb)
Saída: yt: Classe atribuída ao exemplo t
1 dmin ← +∞
2 para cada i ∈ 1,...,n faça
3 se d(xi,xt) < dmin então
4 dmin ← d(xi,xt)
5 idx ← i
6 fim
7 fim
8 yt = yidx
9 Retorna: yt
A Figura 4.2 apresenta um exemplo ilustrativo de aplicação do algoritmo 1-NN num
problema de duas classes. Neste exemplo, considera-se um conjunto de dados cujos objetos são indivíduos que podem ser classificados em saudáveis ou doentes, e o espaço de
entrada é definido por dois atributos que representam o resultado de dois exames. O ponto
representado por “?” é o ponto de teste, ou seja, o indivíduo a ser classificado. Todos
os outros pontos são objetos em que a classe é conhecida: saudável, representada por um
triângulo ou doente, representada por um círculo. No espaço definido pelos atributos, e
usando a distância euclidiana, o objeto de treino mais próximo do objeto de teste pertence
à classe doente, que é então atribuída ao objeto de teste.
4.2.1 Superfícies de Decisão
Apesar da sua simplicidade, as superfícies de decisão desenhadas pelo algoritmo do 1-NN
são muito complexas: são poliedros convexos com centro em cada objeto do conjunto
78 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Exame 1
Classe
saudável
Classe
doente
Exame 2
?
1-NN
Figura 4.2 Exemplo ilustrativo do algoritmo 1-NN.
de treino. Todos os pontos no interior de um poliedro pertencem à classe do objeto do
conjunto de treino que define o centro desse poliedro. O conjunto desses poliedros é
designado diagrama de Voronoi. A Figura 4.3 mostra a construção de um exemplo de
diagrama de Voronoi.
+
+
–
–
+
+
– –
–
xq
Figura 4.3 Superfície de decisão: diagrama de Voronoi.
Para entender a construção do diagrama de Voronoi, seja D um conjunto de treino. A
célula de Voronoi em torno de um ponto x ∈ D é definida como o conjunto de pontos cuja
distância a x é menor que a distância a qualquer outro ponto de D. A superfície de decisão
desenhada pelo algoritmo 1-NN é um conjunto de poliedros convexos contendo cada um
dos objetos de treino.
4.2.2 Distâncias
O desempenho dos métodos baseados em distâncias, como o algoritmo dos vizinhos mais
próximos, são afetados pela medida ou função de distância utilizada. Um problema da
medida de distância apresentada está em pressupor que os dados correspondem a pontos
no espaço d-dimensional (ℜd), ou seja, que os seus atributos são numéricos (contínuos).
Contudo, em diversos problemas os dados são descritos por atributos qualitativos. Outro
MÉTODOS BASEADOS EM DISTÂNCIAS 79
aspeto que deve ser observado no cálculo da distância entre objetos é a escala utilizada para
os valores dos atributos. Por exemplo, qual o efeito na função distância da representação de
um atributo em cm ou Km? As medidas de distância são afetadas pela escala dos atributos.
Para minimizar esse efeito, os atributos são geralmente normalizados.
4.3 O Algoritmo k-NN
Uma extensão imediata ao algoritmo 1-NN é considerar, em vez de 1 vizinho mais próximo, os k objetos do conjunto de treino mais próximos do ponto de teste xt, em que k
é um parâmetro do algoritmo. Quando o valor de k é maior que 1, para cada ponto de
teste, são obtidos k vizinhos. Cada vizinho vota numa classe. As previsões dos diferentes
vizinhos são agregadas de forma a classificar o ponto de teste. Esta agregação é efetuada
de forma diferente em problemas de classificação e de regressão.
Em problemas de classificação, em que a classe toma valores num conjunto discreto,
cada vizinho vota numa classe. O objeto de teste é classificado na classe mais votada.
Formalmente, esse processo é equivalente a: ˆf(xt) ← moda(f(x1), f(x2),..., f(xk)), o
que é justificado porque a constante que minimiza a função de custo 0-1 é a moda.
Em problemas de regressão, podem ser utilizadas duas estratégias, dependendo da
função de custo usada. Se a função de custo for minimizar o erro quadrático, a média dos
valores obtidos para cada um dos k vizinhos deve ser utilizada, o que pode ser formalmente
definido como: ˆf(xt) ← média(f(x1), f(x2),..., f(xk)). Se a função de custo a ser minimizada for o desvio absoluto, em vez da média, deve ser utilizada a mediana. Nesse caso,
a função passa a ser: ˆf(xt) ← mediana(f(x1), f(x2),..., f(xk)). A justificativa para esses
procedimentos é porque a média é a constante que minimiza o erro quadrático, enquanto
a constante que minimiza o desvio absoluto é a mediana.
A seguir será apresentado um exemplo ilustrativo simples do uso do algoritmo dos
vizinhos mais próximos para diferentes valores de k.
Considere o problema da Figura 4.4 (trata-se do mesmo conjunto de dados apresentado anteriormente na Figura 4.2). Para k = 3, o objeto de teste seria classificado como
pertencendo à classe “doente”, enquanto para k = 5 o objeto de teste seria classificado
como pertencendo à classe “saudável”.
A escolha do valor de k mais apropriado para um problema de decisão específico pode
não ser trivial. O valor de k é definido pelo utilizador. Frequentemente, o valor de k é
pequeno e ímpar: k = 3,5,... Em problemas de classificação, não é usual utilizar k = 2 ou
valores pares, para evitar empates.
Duas estratégias referidas na literatura consistem em:
• Estimar k por validação cruzada (ver Capítulo 9).
• Associar um peso à contribuição de cada vizinho.
Neste último caso, a contribuição de cada um dos k vizinhos é pesada de forma inversamente proporcional à distância ao ponto de teste. Dessa forma, é possível utilizar k = n
(todos os objetos de treino).
80 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Exame 1
Exame 2
Classe
saudável
Classe
doente
3-NN
5-NN
?
Figura 4.4 Impacto do valor de k no algoritmo k-NN.
• Em problemas de classificação:
– Moda ponderada: yt = argmaxc∈Y ∑k
i=1wiI(c, yi), com wi = 1
d(xt ,xi) e I(a,b) é
uma função que retorna 1 se e só se a = b;
• Em problemas de regressão:
– Média ponderada: yt = ∑k
i=1 wiyi
∑wi com wi = 1
d(xt ,xi)
Em que yi é a classe do exemplo xi, wi é o peso associado ao exemplo xi e c é a classe
com maior moda ponderada.
4.4 Discussão: Vantagens e Desvantagens
Nesta seção serão analisados os principais aspetos do algoritmo k-NN, salientando seus
aspetos positivos e negativos.
4.4.1 Aspetos Positivos
O algoritmo k-NN representa um dos paradigmas mais conhecidos da aprendizagem indutiva: Objetos com caraterísticas semelhantes pertencem ao mesmo grupo. O k-NN é um
algoritmo baseado em memória, ou um algoritmo preguiçoso, porque toda a computação
é adiada até à fase de classificação, já que o processo de aprendizagem consiste apenas em
memorizar os objetos.
• O algoritmo de treino é simples, consiste em memorizar os objetos de treino.
MÉTODOS BASEADOS EM DISTÂNCIAS 81
• O k-NN constrói aproximações locais da função objetivo, diferentes para cada novo
dado a ser classificado. Essa caraterística pode ser vantajosa quando a função objetivo é muito complexa, mas ainda pode ser descrita por uma coleção de aproximações locais de menor complexidade (Mitchell, 1997).
• É aplicável mesmo em problemas complexos.
• O algoritmo é naturalmente incremental: quando novos exemplos de treino estão
disponíveis, basta armazená-los na memória.
Um dos aspetos mais relevantes deste algoritmo está relacionado com o seu comportamento no limite. Se forem considerados:
• e: erro do classificador Bayes ótimo;
• e1nn(D): erro do 1-NN;
• eknn(D): erro do k-NN.
Duda et al. (2001) provam os seguintes teoremas:
• limn→∞e1nn(D) ≤ 2×e;
• limn→∞,k−>neknn(D) = e.
Ou seja, para um número infinito de objetos, o erro do 1-NN é majorado pelo dobro do
erro do Bayes ótimo, e o erro do k-NN tende para o erro do Bayes ótimo.
4.4.2 Aspetos Negativos
Também por ser um algoritmo lazy, o algoritmo dos vizinhos mais próximos não obtém
uma representação compacta dos objetos. De fato, não se tem um modelo explícito e
compacto a partir dos dados. A fase de treino requer pouco esforço computacional. No
entanto, classificar um objeto de teste requer calcular a distância desse objeto a todos os
objetos de treino. Assim, a predição pode ser custosa, e para um conjunto grande de
objetos de treino esse processo pode ser demorado. Como todos os algoritmos baseados
em distâncias, é afetado pela presença de atributos redundantes e de atributos irrelevantes.
Outro problema do k-NN está relacionado com a dimensionalidade dos exemplos. O
espaço definido pelos atributos de um problema cresce exponencialmente com o número
de atributos, ou seja, o número de atributos define o número de dimensões do espaço.
O ponto que está mais perto de outro pode estar muito distante em problemas de alta
dimensionalidade. Para ilustrar as dificuldades levantadas pela dimensionalidade de um
problema, considere 100 pontos com distribuição uniforme:
• Num quadrado cujo lado mede 1 unidade
• Num cubo cujo lado mede 1 unidade
• ...
82 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Calculando a distância média entre dois pontos, obtemos:
Núm. dimensões Distância média
2 0,494
3 0,647
4 0,772
5 0,875
...
10 1,280
O que se observa é um aumento da distância média entre dois pontos quaisquer. Ou
seja, a densidade diminui e o conjunto de dados fica esparso, rarefeito. Para 10 dimensões, a distância média é maior que o tamanho do lado do hipercubo! Quando a dimensão
aumenta linearmente, para manter a mesma densidade de pontos, é necessário aumentar
de forma exponencial o número de pontos. Beyer et al. (1999) mostram que, sob um
amplo conjunto de condições (muito mais amplo do que dimensões independentes e identicamente distribuídas), com o aumento da dimensionalidade, a distância ao vizinho mais
próximo aproxima-se da distância ao vizinho mais afastado. A dimensionalidade de um
problema pode afetar de forma negativa o desempenho dos algoritmos baseados em distâncias. Uma das formas para reduzir o impacto da dimensionalidade de um problema
consiste em selecionar um subconjunto de atributos relevantes para o problema tratado.
No Capítulo 3 são apresentados vários algoritmos para seleção de atributos.
4.5 Desenvolvimentos
Uma grande parte dos trabalhos de investigação relacionados com o algoritmo k-NN investiga a redução do espaço do problema. Já foi salientado que o k-NN é um algoritmo lento
no processo de classificação de exemplos de teste. Uma das possibilidades para minimizar
esse inconveniente consiste em obter um subconjunto de exemplos representativos. Por
exemplo, eliminando objetos redundantes, ou eliminando objetos em que todos os vizinhos são da mesma classe. Outra possibilidade consiste em eliminar objetos com ruído,
por exemplo eliminando objetos em que todos os vizinhos são de outra classe.
Aha et al. (1991) apresentaram vários algoritmos para selecionar os objetos mais relevantes, designados por protótipos, para o problema de aprendizagem, de forma a reter
em memória apenas esses objetos. As versões do algoritmo Edit k-NN para eliminação
sequencial, Algoritmo 4.2, e inserção sequencial, Algoritmo 4.3, são dois exemplos de
algoritmos que armazenam apenas protótipos. No primeiro caso (Algoritmo 4.2), o algoritmo começa com todos os objetos e vai descartando os objetos que são corretamente
classificados pelo conjunto de protótipos atual. No segundo (Algoritmo 4.3), o conjunto
de protótipos é inicialmente vazio. Os objetos que são incorretamente classificados pelo
conjunto de protótipos são acrescentados a esse conjunto.
A procura linear realizada pelo algoritmo k-NN é ineficiente para grandes conjuntos
de dados. Estruturas de indexação mais sofisticadas podem permitir uma procura mais efi-
MÉTODOS BASEADOS EM DISTÂNCIAS 83
Algoritmo 4.2 Algoritmo para Edit k-NN: eliminação sequencial
Entrada: Um conjunto de treino D = {(xi, yi),i = 1,...,n}
Saída: Um conjunto de treino D, = {(xi, yi),i = 1,...,m;m < n}
1 para cada exemplo (xi,yi) ∈ D faça
2 se (xi,yi) é corretamente classificado por D\{(xi, yi)} então
3 /* Remove (xi,yi) de D */ ;
4 D ← D\{(xi, yi)} ;
5 fim
6 fim
7 Retorna: D;
Algoritmo 4.3 Algoritmo para Edit k-NN: inserção sequencial
Entrada: Um conjunto de treino D = {(xi, yi),i = 1,...,n}
Saída: Um conjunto de treino D, = {(xi, yi),i = 1,...,m;m < n}
1 D, ← {} ;
2 para cada exemplo (xi,yi) ∈ D faça
3 se (xi,yi) é incorretamente classificado por D, então
4 /* Acrescenta (xi, yi) a D, /* ;
5 D, ← D, ∪{(xi, yi)} ;
6 fim
7 fim
8 Retorna: D,
ciente, acelerando a classificação de novos objetos. Uma alternativa consiste na utilização
de árvores de procura binária multidimensionais (kd-trees, do inglês k-dimensional trees)
(Bentley, 1975) como estrutura de dados.
Uma kd-tree particiona um espaço de busca k-dimensional utilizando os k eixos do sistema de coordenadas. Ela faz isso definindo, recursivamente, o particionamento do espaço
dos dados em subespaços disjuntos. No contexto de ECD, cada dimensão representa um
atributo de entrada.
Seja um conjunto de dados formado por n objetos. Uma kd-tree representando esse
conjunto possui n nós, um para cada objeto. A cada nó é associado um atributo discriminador. Todos os nós no mesmo nível da árvore possuem o mesmo discriminador. O
atributo discriminador indica qual o atributo utilizado pelo nó para separar os objetos.
A Figura 4.5 ilustra uma 2d-tree representando um conjunto formado por quatro objetos, x1, x2, x3 e x4. O nó raiz da árvore armazena o objeto x1. Como o atributo discrimi-
84 EXTRAÇÃO DE CONHECIMENTO DE DADOS
nador associado ao nó raiz atual é o atributo 0, o nó raiz da subárvore esquerda do nó raiz
atual armazena o objeto com valor para o atributo 0 menor que o valor do atributo 0 do nó
raiz, que maximiza a separação entre os objetos que satisfazem essa restrição. Neste caso,
foi selecionado o objeto x2. Um princípio semelhante é utilizado para escolher o objeto
que será a raiz da subárvore à direita do nó raiz atual. Este procedimento prossegue de
forma recursiva até que cada objeto seja representado por um nó da árvore. A árvore final
vai facilitar a procura de objetos no espaço k-dimensional.
Atributo 0
Atributo 1
x3 (70, 20)
x1 (40, 50)
x2 (30, 70)
x4 (10, 80)
(a) Particionamento do espaço dos dados
x3
Atributo discriminador
x1
x2
x4
0
1
0
(b) Estrutura da kd-tree resultante
Figura 4.5 Os pontos mostrados em 4.5(a) identificam objetos armazenados como nós internos na
kd-tree e são utilizados para particionar o espaço de procura.
4.6 Raciocínio Baseado em Casos
O raciocínio baseado em casos (RBC) é uma metodologia de ECD para a resolução de
problemas fundamentada na utilização de experiências passadas. Assim, um sistema de
RBC procura resolver um novo problema através da recuperação de problemas semelhantes, previamente solucionados, de uma memória ou base de casos (BC) e adaptação da
solução utilizada no problema recuperado para resolver o novo problema.
RBC difere de outros paradigmas de ECD nos seguintes aspetos (Aamodt e Plaza,
1994):
• Enquanto outros paradigmas utilizam conhecimento geral do domínio ou constroem
relações entre problemas e soluções, RBC é capaz de utilizar conhecimento específico de problemas vistos anteriormente.
• RBC possibilita de forma natural a aprendizagem incremental, pela atualização da
MÉTODOS BASEADOS EM DISTÂNCIAS 85
BC sempre que um novo problema é resolvido, tornando o novo conhecimento disponível para utilização futura.
4.6.1 Representação de Casos
O desempenho de um sistema de RBC depende da estrutura e conteúdo de sua base de
casos. Para a construção de uma base de casos, é necessário decidir o que armazenar num
caso, encontrar uma estrutura apropriada para descrever o conteúdo dos casos e definir
como os casos devem ser organizados e indexados, para possibilitar a recuperação rápida
e a reutilização eficaz de soluções anteriores.
Um caso pode representar diferentes tipos de conhecimento e assumir distintas formas
de representação. Uma forma simples de representar casos é através de um conjunto de
pares atributo-valor. Este conjunto é dividido em dois subconjuntos. O primeiro possui os
atributos que descrevem o problema e o segundo, os atributos relacionados à sua solução.
A Figura 4.6 ilustra um exemplo de BC e de um novo caso. Nessa figura, a base de casos armazena casos relacionados ao problema de escolha de pacotes de viagens. Cada caso
tem a descrição de um problema que ocorreu no passado, a descrição dos requisitos de um
cliente para um pacote, e a solução utilizada para resolver o problema, com a sugestão de
local a ser visitado, meio de transporte para o local, acomodação e opção para refeições. O
novo caso descreve um conjunto de requisitos para um novo pacote de viagem. Utilizando
essa descrição, um sistema de RBC recupera o caso mais semelhante e adapta-o de forma
a sugerir uma solução para esse novo problema. É importante observar que sistemas de
RBC podem recuperar e adaptar um ou mais casos.
Caso 1
Descrição do problema
Ambiente: Praia
Duração: 7 dias
Região: SE
Custo máximo: 1000
Solução do problema
Local: Ubatuba
Transporte: Ônibus
Acomodação: Hotel Uau
Refeição: Bar do Zé
Novo caso
Descrição do problema
Ambiente: Praia
Duração: 5 dias
Região: NE
Custo máximo: 2000
Solução do problema
Local: ???
Transporte: ???
Acomodação: ???
Refeição: ???
...
Figura 4.6 Exemplo de um novo caso e de uma BC.
Um aspeto importante num sistema de RBC é a forma utilizada para a indexação de
casos. No processo de indexação, os casos armazenados recebem índices, que são uti-
86 EXTRAÇÃO DE CONHECIMENTO DE DADOS
lizados para futuras comparações e recuperações de casos (Aamodt e Plaza, 1994). A
definição dos índices tem em consideração os aspetos dos casos considerados relevantes
para a recuperação de casos similares da BC. Em geral, os índices são um subconjunto
dos atributos utilizados para a descrição dos problemas. Os índices precisam ser genéricos
o suficiente para facilitar a identificação de casos similares, mas não tão genéricos para
evitar a recuperação de casos que tenham pouca relação com o novo caso.
A forma como os casos são armazenados na base de casos influencia a facilidade de
sua recuperação. Dois modelos de memória são geralmente utilizados: memória plana e
estrutura hierárquica (Malek e Amy, 1994). Na primeira, todos os casos são armazenados
num mesmo nível. Na segunda, os casos são estruturados em forma de árvore.
4.6.2 O Ciclo de Raciocínio Baseado em Casos
Um modelo frequentemente utilizado para descrever as etapas de um sistema RBC é o
ciclo de RBC proposto por Aamodt e Plaza (1994), ilustrado na Figura 4.7. Esse ciclo
compreende quatro etapas principais:
1. Recuperação: Recuperação do caso armazenado na BC mais semelhante ao novo
problema apresentado.
2. Reutilização: Adaptação da solução do caso recuperado. A solução do problema
recuperado é geralmente utilizada como ponto de partida para propor uma solução
para o novo problema. Esta etapa também é denominada adaptação de casos.
3. Revisão: A solução adaptada é revista pelo utilizador para aferir a sua relevância
para a resolução do novo problema.
4. Retenção: Caso, após a etapa de revisão, a solução adaptada seja considerada relevante, o novo problema, juntamente com essa solução, pode ser armazenado na
BC.
A sequência de etapas no ciclo de RBC pode ser resumida da seguinte forma: a descrição de um novo problema define um novo caso. Este novo caso é utilizado para a
recuperação de um caso entre aqueles armazenados na coleção de casos previamente vistos (BC). A solução utilizada para resolver o caso recuperado é adaptada de forma a prover
uma possível solução para o problema inicial. A solução proposta é avaliada através de
um processo de revisão, que pode ocorrer pela sua aplicação no mundo real, por meio
de uma simulação, pela avaliação do utilizador ou pelo uso do conhecimento da própria
BC. Caso a nova solução seja considerada válida, esta, juntamente com a descrição do problema associado, pode ser armazenada na BC. Desta forma, esta solução pode ser utilizada
futuramente para resolver novos casos (Aamodt e Plaza, 1994).
MÉTODOS BASEADOS EM DISTÂNCIAS 87
Caso novo
Conhecimento
geral
Problema
Recuperação
Reutilização
Revisão
Retenção
Caso aprendido
Casos prévios
Caso testado/reparado
Caso resolvido
Caso recuperado
Novo caso
Figura 4.7 Ciclo de raciocínio baseado em casos (Aamodt e Plaza, 1994).
4.7 Considerações Finais
Neste capítulo foram descritas técnicas de ECD que se baseiam no cálculo de distância
entre objetos. Para isso, foram apresentadas variações do algoritmo dos k vizinhos mais
próximos e a metodologia de RBC.
O algoritmo k-NN é um dos principais algoritmos utilizados pela comunidade de ECD,
por ser simples e apresentar uma boa taxa de acerto preditiva em vários conjuntos de
dados. O desempenho desse algoritmo sofre grande influência do valor de k e da medida
de distância utilizada. Existem trabalhos que propõem técnicas para ajuste automático do
valor de k. Algumas variações detse algoritmo foram apresentadas neste capítulo.
O RBC pode amenizar alguns problemas encontrados noutros paradigmas de ECD. No
entanto, deve ser observado que RBC não oferece uma resposta para todos os problemas
encontrados em ECD. Existem situações em que o RBC não pode ser aplicado na resolução
de problemas, e situações em que RBC não constitui a solução mais adequada e situações
em que RBC pode ser aplicado em conjunto com outras técnicas.
De acordo com Main et al. (2001), Malek (2001), Wiratunga et al. (2002), Jarmulak
et al. (2001), Plaza e Arcos (2002), Prentzas e Hatzilygeroudis (2002) e Falkman (2002),
as atividades de investigação em RBC focam em problemas de adaptação de casos, utilização de métricas de adaptabilidade durante a recuperação dos casos, sistemas híbridos
que combinam RBC com outras técnicas de ECD, manutenção da eficiência do sistema
durante seu uso e manutenção do conhecimento armazenado na BC.

Capítulo 5
Métodos Probabilísticos
Outra forma de lidar com tarefas preditivas em ECD, principalmente quando as informações disponíveis são incompletas ou imprecisas, é por meio do uso de algoritmos baseados
no teorema de Bayes: os métodos probabilísticos Bayesianos. Os métodos probabilísticos
Bayesianos assumem que a probabilidade de um evento A dado um evento B não depende
apenas da relação entre A e B, mas também da probabilidade de observar A independentemente de observar B (Duda et al., 2001). A probabilidade de ocorrência do evento A
pode ser estimada pela observação da frequência com que este evento ocorre. De forma
semelhante, é possível estimar a probabilidade de que um evento B ocorra dado que foi
observado o evento A, P(B|A). Nos métodos probabilísticos para problemas de decisão, o
objetivo é estimar P(A|B), onde A representa a classe e B o valor observado dos atributos
para um exemplo de teste.
Assuma que P(A) representa a probabilidade de ocorrência de uma determinada doença, e P(B) representa a probabilidade de um doente ter um determinado resultado num
exame de raios X. Para um determinado paciente, a variável A não é observável. Perante
a evidência do resultado do exame de raio X, podemos inferir o valor mais provável de A
estimando P(A|B). O teorema de Bayes mostra como calcular P(A|B) utilizando a probabilidade a priori da classe, P(A),eaverosimilhança de B dado A, ou seja P(B|A).
Os possíveis valores do conjunto de atributos de entrada definem o espaço amostral
(Ω). A probabilidade de ocorrência de um evento E, P(E) (por exemplo, pacientes cujo
resultado num exame foi positivo) satisfaz os axiomas de Kolmogorof (Pestana e Velosa,
2002):
• 0 ≤ P(E);
• Se Ω é o espaço de eventos, então P(Ω) = 1;
• Se A e B são eventos disjuntos, então P(A∪B) = P(A) +P(B).
A partir destes axiomas e definições, é possível derivar a lei da probabilidade total,
que afirma que se B1, B2, ..., Bn formam uma partição em Ω, então, para qualquer evento
90 EXTRAÇÃO DE CONHECIMENTO DE DADOS
A, tem-se que:
P(A) =
n
∑
i=1
P(A|Bi)×P(Bi)
É possível também derivar a lei da probabilidade condicional:
P(A|B) = P(A∩B)
P(B)
Esses teoremas permitem deduzir o teorema de Bayes, tendo em conta que:
P(A∩B) = P(B∩A)
P(A|B)P(B) = P(A∩B) = P(B|A)P(A)
P(A|B) = P(B|A)P(A)
P(B)
Na próxima seção, é feita uma introdução à aprendizagem Bayesiana, à representação de
informação usando modelos probabilísticos gráficos, e à utilização do teorema de Bayes
em inferência. A Seção 5.2 descreve um dos classificadores Bayesianos mais populares: o
naive Bayes. Por último, na Seção 5.3, são apresentadas redes Bayesianas para classificação.
5.1 Aprendizagem Bayesiana
Para exemplificar como os métodos probabilísticos podem ser utilizados em ECD, considere o seguinte cenário: Suponha que a probabilidade de observar alguém com uma dada
doença é de 8%. Existe um teste para o diagnóstico dessa doença, cujo resultado possui
um grau de incerteza. Sabe-se que em 75% dos casos em que a doença foi confirmada,
o resultado do teste foi positivo, e que em 96% dos casos em que o paciente não tinha a
doença, o resultado do teste foi negativo.
Como é possível representar essa informação? A doença pode ser considerada uma
variável aleatória com dois valores possíveis: presente e ausente. O resultado do teste
também tem dois valores mutuamente exclusivos: positivo ou negativo. É fácil observar
que o valor da Doença influencia o valor do Teste, mas o oposto não é verdade. É útil
representar essa informação sob a forma de um grafo.
A Figura 5.1 apresenta o grafo que representa a informação do problema descrito.
Neste grafo, os nós representam variáveis ou atributos e as arestas representam a influência
entre variáveis. A informação gráfica é qualitativa, pois a direção da aresta diz-nos que o
valor da variável doença influencia o valor da variável teste. Também contém informação
MÉTODOS PROBABILÍSTICOS 91
Figura 5.1 Modelo gráfico probabilístico para representar a informação do problema médico. A
figura mostra o modelo qualitativo e o modelo quantitativo.
quantitativa: a probabilidade a priori de observar a doença:
P(Doença = presente) = 0,08
P(Doença = ausente) = 0,92
A partir de experiências passadas na utilização do teste, é possível inferir as probabilidades condicionais para a variável Teste:
P(Teste = positivo|Doença = presente) = 0,75
P(Teste = negativo|Doença = ausente) = 0,96
Esta informação é definida nos nós do modelo gráfico, tal como ilustrado na Figura 5.1.
O modelo gráfico probabilístico é constituído pelo modelo qualitativo – um grafo cujos
nós representam variáveis – e pelo modelo quantitativo – tabelas com a distribuição das
variáveis. A probabilidade de verdadeiros positivos (o Teste deu positivo quando a Doença
está presente) é de 75% (taxa denominada sensibilidade) e a probabilidade de verdadeiros
negativos (o Teste deu negativo quando a Doença está ausente) é de 96% (taxa denominada
especificidade).
Qual é o poder preditivo do Teste com respeito à Doença? É possível calcular as
probabilidades a priori para a variável Teste. Levando em conta que P(A) = P(A|B) ×
P(B), obtemos:
P(Teste = positivo) =
= P(Teste = positivo|Doença = presente)×P(Doença = presente) +
+ P(Teste = positivo|Doença = ausente)×P(Doença = ausente)
= 0,75×0,08+0,04×0,92 = 0,0968
92 EXTRAÇÃO DE CONHECIMENTO DE DADOS
P(Teste = negativo) =
= P(Teste = negativo|Doença = presente)×P(Doença = presente) +
+ P(Teste = negativo|Doença = ausente)×P(Doença = ausente)
= 0,25×0,08+0,96×0,92 = 0,9032
Considere agora que, para um dado paciente, o resultado do Teste foi positivo. Podemos concluir que o paciente está doente? Na aprendizagem Bayesiana, o valor de uma
variável aleatória tem uma probabilidade associada. A questão que devemos colocar é:
Qual é a probabilidade P(Doença = presente|Teste = positivo)? O teorema de Bayes é
usado para calcular a probabilidade a posteriori de um evento, dados a sua probabilidade
a priori e a verossimilhança do novo dado. Neste exemplo, precisamos inverter a probabilidade P(Teste = positivo|Doença = presente). Na próxima seção será visto como isso
pode ser feito.
5.1.1 O Problema de Inferência e o Teorema de Bayes
A literatura de reconhecimento de padrões (Duda et al., 2001) e aprendizagem automática (Mitchell, 1997) apresenta diversas propostas para lidar com o problema de aprendizagem num cenário probabilístico. Suponha que P(yi|x) denota a probabilidade de um
exemplo x pertencer à classe yi. A função de custo zero-um, que representa o custo de
associar x à classe incorreta, é minimizada se, e somente se, x é associado à classe yk
para a qual P(yk|x) é máxima (Duda et al., 2001). Este método é designado por estimativa
MAP (do inglês, Maximum A Posteriori). Formalmente, a classe que deve ser associada
ao exemplo x é dada pela expressão:
yMAP = argmaxi
P(yi|x) (5.1)
na qual argmaxi retorna a classe yi com maior probabilidade de estar associada a x, que é
aquela que possui o valor máximo para P(yi|x).
Qualquer função que calcula as probabilidades condicionadas P(yi|x) é referida como
uma função discriminante, por separar exemplos de classes diferentes. Dado um exemplo
x, o teorema de Bayes fornece um método para calcular P(yi|x):
P(yi|x) = P(yi)P(x|yi)
P(x) (5.2)
O denominador, P(x), pode ser ignorado, uma vez que é o mesmo para todas as classes,
não afetando os valores relativos de suas probabilidades. Assumindo que as probabilidades a priori das hipóteses yi são iguais, a Equação 5.2 pode ser simplificada considerando
apenas o termo P(x|yi) para calcular a hipótese mais provável. P(Dados|hipótese) é designado por verosimilhança, e a hipótese que maximiza P(Dados|hipótese) é designada
por máxima verosimilhança, que pode ser expressa por:
MÉTODOS PROBABILÍSTICOS 93
hMV = argmaxi
P(x|yi) (5.3)
Embora esta regra seja ótima, a sua aplicabilidade é reduzida devido ao grande número de exemplos necessários para calcular, de forma viável, P(x|yi). Para superar esse
problema, várias hipóteses têm sido propostas. Dependendo das hipóteses propostas, são
obtidas diferentes funções discriminantes, conduzindo a diferentes classificadores. Neste
capítulo, será estudado um tipo de função discriminante que conduz ao classificador naive
Bayes.
5.2 O Classificador Naive Bayes
Assumindo que os valores dos atributos de um exemplo são independentes entre si dada
a classe, P(x|yi) pode ser decomposto no produto P(x1|yi) × ... × P(xd|yi), em que xj é
o j-ésimo atributo do exemplo x. Com isso, a probabilidade de um exemplo pertencer à
classe yi é proporcional à expressão:
P(yi|x) ∝ P(yi)
d
∏
j=1
P(xj|yi) (5.4)
O classificador obtido pelo uso da função discriminante dada pela Equação 5.4 e pela regra
de decisão ilustrada na Equação 5.1 é conhecido como classificador naive Bayes. O termo
naive vem da hipótese de que os valores dos atributos de um exemplo são independentes
de sua classe.
A fórmula do naive Bayes pode ser expressa de uma forma aditiva. Aplicando logaritmos à Equação 5.4, obtém-se:
log(P(yi|x)) ∝ log(P(yi)) +∑
j
log(P(xj|yi)) (5.5)
Para o caso particular de duas classes, a Equação 5.4 pode ser reescrita como:
log P(y1|x)
P(y2|x) ∝ log P(y1)
P(y2)
+∑
j
log
P(xj|y1)
P(xj|y2) (5.6)
Nesta nova formulação, o sinal de cada termo indica a contribuição de cada atributo
para cada classe. Se o quociente P(xj|y1)/P(xj|y2) é maior que 1, o logaritmo é positivo
e o atributo contribui para a predição da classe y1.
5.2.1 Detalhes de Implementação
Todas as probabilidades necessárias para a obtenção do classificador naive Bayes são calculadas a partir dos dados de treino. Para calcular a probabilidade a priori de observar a
94 EXTRAÇÃO DE CONHECIMENTO DE DADOS
classe yi, P(yi), é necessário manter um contador para cada classe. Para calcular a probabilidade condicional de observar um valor de um atributo dado que o exemplo pertence a
uma classe, é necessário distinguir entre atributos nominais e atributos contínuos.
No caso de atributos nominais, o conjunto de valores possíveis é um conjunto enumerável. Para calcular a probabilidade condicional, basta manter um contador para cada
valor de atributo por classe. No caso de atributos contínuos, quando o número de valores possíveis é infinito, existem duas possibilidades. A primeira é assumir uma distribuição particular para os valores do atributo, sendo geralmente assumida a distribuição normal. A segunda alternativa consiste em discretizar o atributo numa fase de préprocessamento. Já foi mostrado que a segunda possibilidade produz melhores resultados
que a primeira (Dougherty et al., 1995; Domingos e Pazzani, 1997).
Vários métodos para a discretização aparecem na literatura. Uma boa discussão sobre
discretização é apresentada em Dougherty et al. (1995). Por exemplo, Domingos e Pazzani (1997) propõem que o número de intervalos seja fixado em k = mínimo(10, número
de valores diferentes) intervalos do mesmo tamanho. Depois de discretizar um atributo,
podemos utiliar um contador para cada classe e para cada intervalo para calcular a probabilidade condicional P(Atributoj|Classei).
5.2.2 Um Exemplo Ilustrativo
Este exemplo utiliza um conjunto de dados para o problema balance, que é apresentado
na Figura 5.2. Este conjunto de dados foi gerado para modelar resultados de experiências
de psicologia. Neste problema, cada exemplo é classificado numa de três posições de
uma balança: a balança está inclinada para a direita, para a esquerda ou equilibrada. Os
atributos são o peso do lado esquerdo, a dimensão do braço esquerdo, o peso do lado direito
e a dimensão do braço direito. A forma correta para encontrar a classe é o maior valor
entre: DistânciaEsq × PesoEsq e DistânciaDir × PesoDir. Se esses valores forem iguais, o
estado da balança, isto é, a sua classe, é equilibrada.
Distância Esq
Distância Dir
Peso Dir
Peso Esq
Figura 5.2 O problema do equilíbrio da balança.
Na versão do repositório UCI (Frank e Asuncion, 2010) o domínio de todos os atributos para este conjunto de dados, é o conjunto {1,2,3,4,5}. O conjunto contém 625
exemplos, distribuídos da seguinte forma: em 49 exemplos a balança está equilibrada, em
MÉTODOS PROBABILÍSTICOS 95
288 exemplos a balança está inclinada para a esquerda e nos 288 restantes exemplos a
balança está inclinada para a direita.
Para calcular as probabilidades a priori, P(Classei), é necessário contar o número de
exemplos para cada classe. Os resultados são apresentados na Tabela 5.1.
Tabela 5.1 Contagem de valores e probabilidade a priori para as classes
Equilibrada Esquerda Direita
Contagem 49 288 288
P(Classe) 0,078 0,461 0,461
Para calcular a probabilidade condicional de observar um valor específico de atributo
dada a classe, P(Atributoj|Classei), é necessário descobrir o tipo do atributo. Neste problema, todos os atributos são numéricos, pois dizem respeito a distâncias e pesos. Pode-se
assumir que o seu domínio é um subconjunto de ℜ. Sem mais nenhuma informação, a
hipótese mais razoável é que estes são normalmente distribuídos. Considerando essa hipótese, para estimar as probabilidades condicionais, é preciso calcular a média e o desvio
padrão dos valores dos atributos para cada classe.
Como alternativa, pode-se discretizar os atributos. Neste problema, aplicando a regra k = min(10; número de valores diferentes), são obtidos cinco intervalos. A Tabela 5.2
apresenta a distribuição de valores para cada atributo, em cada classe.
Tabela 5.2 Tabelas de distribuição dos valores dos atributos por classe
Distribuição normal Discretização
PesoEsq Média Desvio padrão V1 V2 V3 V4 V5
Equilibrado 2,938 1,42 10 11 9 10 9
Esquerda 3,611 1,23 17 43 63 77 88
Direita 2,399 1,33 98 71 53 38 28
DistânciaEsq Média Desvio padrão V1 V2 V3 V4 V5
Equilibrado 2,938 1,42 10 11 9 10 9
Esquerda 3,611 1,22 17 43 63 77 88
Direita 2,399 1,33 98 71 53 38 28
PesoDir Média Desvio padrão V1 V2 V3 V4 V5
Equilibrado 2,938 1,42 10 11 9 10 9
Esquerda 2,399 1,33 71 53 38 28 17
Direita 3,611 1,22 17 43 63 77 88
DistânciaDir Média Desvio padrão V1 V2 V3 V4 V5
Equilibrado 2,938 1,42 10 11 9 10 9
Esquerda 2,399 1,33 98 71 53 38 28
Direita 3,611 1,22 17 43 63 77 88
A Figura 5.3 ilustra graficamente a Tabela 5.2. Na primeira linha são apresentadas
as distribuições discretizadas para os atributos PesoEsq e PesoDir, assim como a Classe.
96 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Podemos observar que para a classe equilibrado todas as contagens são similares. Para
os atributos PesoEsq e DistânciaEsq, a contagem aumenta para a classe esquerda e diminui
para a classe direita. Para os atributos PesoDir e DistânciaDir, a contagem aumenta para a
classe direita e diminui para a classe esquerda. A segunda linha mostra, para os mesmos
atributos, a função densidade de probabilidade por classe, assumindo uma distribuição
normal.
Peso esquerdo Peso direito Classe
BL R
1
2
3
4
5
Valores
Valores
BL R
200
100
50
0
Nº Exemplos
B L R
Probabilidade
Probabilidade 0,10
0,00
0,20
0,30
-2 0 2 4 6 8
Peso direito
-2 0 2 4 6 8
Peso esquerdo
0,10
0,00
0,20
0,30
B
L
R
B
L
R
1
2
3
4
5
Figura 5.3 Naive Bayes para o problema da balança.
5.2.3 Análise do Algoritmo
A superfície de decisão de um classificador naive Bayes num problema de duas classes
definido por atributos booleanos é um hiperplano, ou seja, a superfície de decisão é linear. Todas as probabilidades exigidas pela Equação 5.4 podem ser calculadas a partir do
conjunto de treino numa única passagem. O processo de construir o modelo é bastante
eficiente. Outro aspeto interessante do algoritmo é que ele é fácil de implementar de uma
forma incremental.
Domingos e Pazzani (1997) mostram que o naive Bayes tem um bom desempenho
numa grande variedade de domínios, incluindo domínios em que há clara dependência
entre os atributos. Os autores argumentam que, em problemas de classificação e para a
função de custo 0−1, um exemplo é corretamente classificado desde que a ordenação das
MÉTODOS PROBABILÍSTICOS 97
classes dada pelas estimativas das probabilidades a posteriori esteja correta, independentemente de essas estimativas serem (ou não) realistas.
Kononenko (1991) sugere que este classificador é robusto à presença de ruído e atributos irrelevantes. Eles também notaram que as teorias aprendidas são fáceis de compreender
pelos especialistas do domínio. Essa observação deve-se ao fato de que o naive Bayes resume a variabilidade do conjunto de dados em tabelas de contingência, e assume que estas
são suficientes para distinguir entre as classes.
O desempenho do naive Bayes não decresce na presença de atributos irrelevantes.
Suponha um problema de duas classes, em que o i-ésimo atributo é irrelevante: P(xi|y1) =
P(xi|y2). A partir da fórmula do classificador naive Bayes:
P(yk|x1,...,xi,...,xd) ∝ P(yk)P(xi|yk)
i−1
∏
l=1
P(xl|yk)
d
∏
l=i+1
P(xl|yk) (5.7)
P(yk|x1,...,xi,...,xd) ∝ P(yk|x1,...,xi−1,xi+1,...,xd) (5.8)
O impacto das variáveis redundantes deve ser levado em consideração. Assuma que
o (i − 1)-ésimo e o i-ésimo atributos são redundantes, ou seja, para todas as classes y
P(xi−1|y) = P(xi|y).
P(yk|x1,...,xi−1,xi,...,xd) ∝ P(yk)P(xi−1|yk)P(xi|yk)
i−2
∏
l=1
P(xl|yk)
d
∏
l=i+1
P(xl|yk) (5.9)
P(yk|x1,...,xi−1,xi,...,xd) ∝ P(yk)P(xi|yk)
2
i−2
∏
l=1
P(xl|yk)
d
∏
l=i+1
P(xl|yk) (5.10)
5.2.4 Desenvolvimentos
Foram desenvolvidas várias técnicas para melhorar o desempenho do classificador naive
Bayes. Algumas dessas técnicas aplicam diferentes classificadores naive Bayes para diferentes regiões do espaço de entrada. Por exemplo, Langley (1993) apresentou um algoritmo naive Bayes que, recursivamente, constrói uma hierarquia das descrições dos conceitos probabilísticos. Kohavi (1996) apresentou uma árvore de naive Bayes. É um algoritmo
híbrido que gera uma árvore de decisão univariada regular, cujas folhas contêm um classificador naive Bayes. O classificador associado a cada nó folha é construído a partir de
exemplos que levam a esse nó. A proposta retém a interpretabilidade do naive Bayes e
das árvores de decisão, resultando num classificador que frequentemente supera ambos os
constituintes, especialmente em grandes conjuntos de dados.
Outras técnicas constroem novos atributos que refletem interdependências entre atributos originais. Por exemplo, Kononenko (1991) apresentou um classificador semi-naive
Bayes. O classificador procura combinar pares de atributos, fazendo um atributo produtocruzado, baseado em testes estatísticos para independência. A avaliação experimental foi
98 EXTRAÇÃO DE CONHECIMENTO DE DADOS
não conclusiva. Noutro exemplo, Pazzani (1996) apresentou um classificador Bayesiano
construtivo. Para isso, emprega um modelo encapsulado (John et al., 1994) para encontrar
os melhores atributos do produto cartesiano a partir de atributos nominais existentes. O
classificador também considera a eliminação de atributos existentes. Notou-se com isso
uma melhoria no classificador naive Bayes.
Técnicas que abordam o problema da presença de atributos contínuos estão também
presentes na literatura. John (1997) apresentou o Bayes Flexível que utiliza, para atributos
contínuos, uma estimativa de densidade de funções kernel (em vez de uma única hipótese
gaussiana), mas retém a hipótese de independência. Para cada atributo contínuo, a densidade estimada é obtida pela expressão P(xt|yi) = 1
n ∑iN(xt,xi,σc), em que n representa o
número de exemplos de treino da classe yi, e σc representa o tamanho de banda do kernel.
Desta forma, P(xt|yi) é obtido agregando sobre todos os exemplos de treino da classe yi.
A avaliação experimental em conjuntos de dados do repositório do UCI mostra que o
Bayes flexível alcança, em muitos domínios, taxas de acerto preditivas significativamente
maiores que o naive Bayes. Gama (2000) apresentou um algoritmo Linear Bayes que utiliza uma distribuição normal multivariada para cada classe para calcular a verosimilhança
P(xi,...,xj|yi), em que xi,...,xj representa o conjunto de atributos contínuos. Esta estratégia mostrou-se melhor do que o naive Bayes usando discretização ou uma distribuição
gaussiana univariada.
5.3 Redes Bayesianas para Classificação
Foi previamente mencionado a incapacidade do classificador naive Bayes em lidar com
inter-dependências entre atributos. Recorde-se que duas variáveis aleatórias (atributos)
X,Y são independentes quando P(X,Y) = P(X)×P(Y), ou seja, conhecer o valor de uma
variável não traz informação sobre o valor da outra variável, o que implica P(X|Y) =
P(X). Esta seção tem como tema central a independência condicional: casos em que existe
uma relação estatística entre duas variáveis quando uma terceira variável é conhecida.
Formalmente, X é condicionalmente independente de Y dado Z se P(X|Y,Z) = P(X|Z).
Os modelos gráficos probabilísticos, ou redes Bayesianas (Pearl, 1988), utilizam o
conceito de independência condicional entre variáveis para obter um equilíbrio entre o
número de parâmetros a calcular e a representação de dependências entre as variáveis. Estes modelos representam a distribuição de probabilidade conjunta de um grupo de variáveis
aleatórias num domínio específico.
Formalmente, seja x = {x1,x2,...,xn} um conjunto de variáveis aleatórias para um
dado domínio. Uma rede Bayesiana (RB) sobre x é uma tupla (S,ΘS) em que o primeiro
componente, a estrutura da rede S, é um grafo acíclico direcionado (DAG, do inglês,
Directed Acyclic Graph) cujos nós representam as variáveis aleatórias e as arestas representam dependências diretas entre variáveis. Um arco entre dois nós, denota influência ou
correlação. O conjunto de variáveis aleatórias (nós do DAG) que influenciam uma variável xi é usualmente designado por Pais de xi. A segunda componente ΘS é o conjunto de
tabelas de probabilidade condicional.
MÉTODOS PROBABILÍSTICOS 99
Classe
Edu Salário
Área
Classe
Edu
Não Sim
Não Não Sim Sim
Baixo
Alto Salário 0,6
0,4
0,4
0,4
0,33
0,66
0,66
0,33
Figura 5.4 A Figura mostra o modelo qualitativo – um grafo cujos nós representam variáveis –
e o modelo quantitativo para a variável Salário – tabelas com a distribuição de probabilidades dos
valores da variável Salário dado o valor das variáveis que a influenciam.
Assumindo que as variáveis são discretas, cada P(xi|Paii) ∈ ΘS representa a tabela de
probabilidade condicional (TBC) sobre os valores de xi dados os valores do seu pai Paii.
Além disso, o DAG S satisfaz a condição de Markov: cada nó é independente de todos
os seus não descendentes, dados os seus pais em S. Isto permite que a distribuição de
probabilidade conjunta sobre x seja representada na forma fatorizada: P(x1,x2,...,xn) =
∏n
i=1 P(xi | Paii). A Figura 5.4 apresenta um exemplo de uma rede Bayesiana. É apresentado o modelo qualitativo – um grafo cujos nós representam variáveis–eo modelo
quantitativo – tabelas com a distribuição de probabilidades da variável Salário dadas as
variáveis que a influenciam. A fatorização conjunto para as variáveis do modelo é:
P(Classe)P(Edu|Classe)P(Salário|Edu,Classe)P(Área|Salário,Classe).
Uma rede Bayesiana pode ser utilizada para tarefas de classificação de uma forma
relativamente simples. Uma das variáveis é selecionada como atributo alvo, e todas as
outras variáveis são consideradas como atributos de entrada. O conjunto de variáveis
que influenciam o atributo alvo é designado por Markov Blanquet: é constituído pelas
variáveis pais da variável alvo, pelos filhos da variável alvo e pelos pais dos filhos da
variável alvo. Assim, uma rede RB pode ser utilizada como um classificador que, dado
um exemplo x, fornece a distribuição de probabilidade a posteriori P(y | x) do nó classe
y ∈ Y. É possível calcular a probabilidade a posteriori P(y | x,S) para cada classe y ∈ Y
marginalizando a distribuição de probabilidade conjunta P(y,x | S) e então retornar a classe
yˆ que a maximiza:
yˆ = hCRB(x) = arg max j=1...k
P(y j,x | S) (5.11)
Dado um conjunto de treino, o problema de aprendizagem consiste em selecionar o
classificador baseado em RB (CRB), isto é, a hipótese hC = (S,ΘS) que produz a classificação com maior taxa de acerto para dados não conhecidos.
Este problema pode ser resolvido inicialmente pela escolha de um modelo de classe
adequado, que define o espaço de possíveis estruturas RB. Em seguida, é selecionada uma
estrutura dentro desse modelo de classe. Finalmente, os parâmetros são estimados a partir
dos dados.
O problema de escolher a estrutura mais apropriada para um determinado problema
100 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Classe
x1 x2 x3 x4
Classe
x1 x2 x3 x4
Classe
x1
x2
x3 x4
NB é 0-DBC TAN é 1-DBC BAN que é 2-DBC
Figura 5.5 Exemplos de classificadores bayesianos com k-dependências.
está relacionado com a seleção de modelos, uma área da inferência estatística que estuda
a seleção, dentre um conjunto de modelos concorrentes, daquele que “melhor se ajusta”,
em algum sentido, aos dados disponíveis. Uma das propostas consiste em definir uma
função de pontuação que mede a qualidade de cada hipótese candidata, ou seja, o ajuste
do modelo aos dados de treino. As propostas baseadas em pontuação podem ser descritas
como um problema de procura, em que cada estado no espaço de procura identifica um
possível DAG. O método de procura utiliza o valor retornado pela função de pontuação.
O problema da seleção de uma função de pontuação apropriada para aprendizagem
de CRBs tem sido alvo de muita atenção (Domingos e Pazzani, 1997; Friedman et al.,
1997; Kontkanen et al., 1999). Quando uma RB é induzida para classificação, o objetivo
principal é construir um classificador com elevada taxa de acerto preditiva. Para isso, foi
sugerido que as estratégias de procura para aprendizagem de CRBs deveriam selecionar
entre modelos utilizando pontuações especializadas para classificação (pontuações supervisionadas); caso contrário a procura poderia resultar numa escolha subótima (Kontkanen
et al., 1999). Deste modo, uma pontuação baseada na distribuição da probabilidade conjunta não será necessariamente ótima em problemas de classificação.
Outro aspeto que também pode influenciar o desempenho dos CRBs induzidos é a
seleção de um modelo de classe apropriado, que define o espaço de procura e, consequentemente, a complexidade dos CRBs induzidos. A seleção do modelo procura um equilíbrio
viés-variância a fim de selecionar um modelo com a complexidade apropriada, que é automaticamente regularizado pela função de pontuação (Hastie et al., 2001). Podemos obter
o desempenho desejado dos CRBs induzidos se, a cada momento, tentarmos selecionar
o modelo de classe apropriado, com a complexidade adequada, para os dados de treino
disponíveis.
5.3.1 Classificadores Bayesianos com k-Dependências
Os classificadores Bayesianos com k-dependências (Sahami, 1996)(k-CBDs) representam
um enquadramento unificado para todas as classes de CRBs que contêm a estrutura de
Bayes simples. Além disso, um k-CBD permite que cada atributo tenha no máximo k nós
atributos como pais. Como ilustrado na Figura 5.5, é possível variar o valor de k e obter
MÉTODOS PROBABILÍSTICOS 101
classificadores Bayesianos de complexidade crescente, que se movam gradualmente ao
longo do espetro de dependências entre atributos. O naive Bayes é um 0-CBD e encontrase no extremo mais restritivo, porque não permite dependências entre atributos. Um classificador TAN (Friedman et al., 1997) é um 1-CBDs (permite, no máximo, um atributo
como pai de outro atributo). O BAN (Friedman et al., 1997; Cheng e Greiner, 1999), apresentado na Figura 5.5, é um 2-CBD (tem um máximo duas dependências entre atributos).
No extremo mais geral encontra-se o classificador (n−1)-CBD, que assume que todos os
atributos interagem entre si. A fatoração da probabilidade conjunta P(Classe,x1,x2,x3,x4)
das cinco variáveis, codificada pelos modelos apresentados na Figura 5.5, é seguidamente
apresentada:
• Naive Bayes:
P(Classe)P(x1|Classe)P(x2|Classe)P(x3|Classe)P(x4|Classe)
• TAN:
P(Classe)P(x1|Classe)P(x2|x1,Classe)P(x3|x2,Classe)P(x4|x3,Classe)
• BAN:
P(Classe)P(x1|Classe)P(x2|x1,Classe)P(x3|x1,x2,Classe)P(x4|x3,Classe)
Sahami (1996) propôs um algoritmo para indução de k-CBDs que usa o conceito de
entropia condicional. Em Castillo e Gama (2005) é proposto um algoritmo de aprendizagem subida da encosta (hill-climbing) para o mesmo problema. É um algoritmo simples,
incremental e de fácil implementação computacional. O algoritmo, cujo pseudocódigo é
apresentado no Algoritmo 5.1, é iniciado com a estrutura do naive Bayes. Iterativamente,
adiciona arestas entre dois atributos que resultam em melhorias máximas na pontuação
até que não existam mais melhorias para aquela pontuação, ou até que não seja possível
adicionar uma nova aresta. A função de avaliação de modelos é a taxa de erro no conjunto
de treino. Alternativamente, pode ser usada a taxa de erro num conjunto de validação. Os
resultados obtidos em experiências apresentadas em Sahami (1996), e relacionados com
os estudos que utilizam k-CBDs (Blanco et al., 2005; Castillo e Gama, 2005; Webb et al.,
2005) mostram que, considerando as dependências entre atributos é possível melhorar os
resultados da classificação de Bayes simples. Conhecendo como o desempenho de classificação se altera com o aumento do valor de k, podemos obter uma noção de nível de
dependência entre atributos para cada domínio particular. Um resultado interessante foi
apresentado em Castillo e Gama (2005), onde é estudada a relação entre o número de
exemplos de treinoeovalor de k. Neste trabalho, são apresentados resultados experimentais que mostram que, para valores crescentes do número de exemplos de treino, o melhor
desempenho é obtido através do aumento do valor de k.
5.4 Considerações Finais
Os modelos gráficos probabilísticos representam a distribuição de probabilidade conjunta
de um conjunto de variáveis aleatórias. É possível obter classificadores Bayesianos de
102 EXTRAÇÃO DE CONHECIMENTO DE DADOS
Algoritmo 5.1 O algoritmo subida de encosta para aprendizagem k-CBDs
Entrada: Um conjunto de treino D = {(xi,yi),i = 1,...,n}
k: número de dependências admissíveis.
Saída: Um k-DBC com baixo valor de Erro(S,D)
1 /* S é o espaço de possíveis DAGs restritos por k */ ;
2 Inicialize S com a estrutura Bayes simples;
3 continuar ← Verdadeiro ;
4 enquanto continuar faça
5 Calcular Erro(S,D);
6 /* Seja (x,
, x,,) a aresta que minimiza a função de avaliação.*/ ;
7 (x,
, x,,) = argminErro(S
!(xi, x j),D)∧
8 ∧|pai(xi) \ y| < k∧ |pai(xj) \ y| < k ;
9 se Existe a aresta (x,
,x,,) ∧ Erro(S
!(x,
, x,,),D) < Erro(S,D) então
10 Adicionar a aresta (x,
, x,,) em S ;
11 fim
12 senão
13 continuar ← Falso ;
14 fim
15 fim
16 Estimar os parâmetros ΘS dado S a partir dos dados D;
17 Retorna: k-DBC=(S,ΘS);
complexidade crescente, que consideram diferentes graus de dependências entre atributos.
O naive Bayes é o mais restritivo porque não permite dependências entre atributos. No
extremo mais geral, temos classificadores que assumem que todos os atributos interagem
entre si. Entre os dois extremos, temos modelos de granularidade crescente. Os modelos
gráficos probabilísticos são usados para diferentes tarefas de aprendizagem, desde previsão, em que se pretende obter o resultado mais provável para os dados de entrada, até
diagnóstico, em que se pretende obter as causas mais prováveis para os efeitos observados.
Capítulo 6
Métodos Baseados em Procura
O problema de aprendizagem automática pode ser formulado como um problema de procura num espaço de possíveis soluções. Dada uma linguagem para representar generalizações de exemplos e uma função de avaliação de hipóteses, o problema de aprendizagem
pode ser resolvido através de procura no espaço de hipóteses definido pela linguagem de
representação. O alvo de estudo deste capítulo são os modelos baseados em árvores (árvores de decisão e regressão) e os modelos baseados em regras.
Os modelos em árvore são designados árvores de decisão, no caso de problemas de
classificação, e árvores de regressão nos problemas de regressão. Quer em árvores de
decisão, quer em árvores de regressão, a interpretação dos modelos assim como os algoritmos de indução das árvores são muito semelhantes, pelo que iremos usar o termo árvores
de decisão de uma forma genérica, explicitando quando necessário.
6.1 Árvores de Decisão e Regressão
Uma árvore de decisão usa a estratégia dividir para conquistar para resolver um problema
de decisão. Um problema complexo é dividido em problemas mais simples, aos quais
recursivamente é aplicada a mesma estratégia. As soluções dos subproblemas podem ser
combinadas, na forma de uma árvore, para produzir uma solução do problema original. A
vantagem desta proposta prende-se com a sua capacidade em dividir o espaço de instâncias
em subespaços, e ajustar cada subespaço recorrendo a diferentes modelos. Esta constitui
a ideia base por detrás de algoritmos baseados em árvores de decisão e de regressão tais
como: ID3 (Quinlan, 1979), ASSISTANT (Cestnik et al., 1987), CART (Breiman et al.,
1984), C4.5 (Quinlan, 1993). Recentemente, diversos pacotes estatísticos, Splus, Statistica,
SPSS (Mattison, 1998), R (Ihaka e Gentleman, 1996) e Microsoft SQL Server (Seidman,
2001) incorporaram funções que implementam árvores de decisão para problemas de classificação e regressão.
Formalmente, uma árvore de decisão é um grafo acíclico direcionado em que cada nó
ou é um nó de divisão, com dois ou mais sucessores, ou um nó folha:
104 EXTRAÇÃO DE CONHECIMENTO DE DADOS
• Um nó folha é rotulado com uma função. Usualmente são considerados apenas os
valores da variável alvo nos exemplos que chegam a um nó folha. No caso mais
simples, a função é a constante que minimiza a função de custo. Em problemas
de classificação, e assumindo a função de custo 0-1, essa constante é a moda. Em
problemas de regressão, a constante que minimiza a função de custo do erro médio quadrático é a média, enquanto para a função de custo do desvio absoluto é a
mediana.
• Um nó de divisão contém um teste condicional baseado nos valores do atributo. Na
proposta padrão, os testes são univariados: as condições envolvem um único atributo
e valores no domínio desse atributo. Exemplos de testes condicionais são:
– Idade > 18;
– Profissão ∈ {professor, estudante};
– 0,3+0,2×x1 −0,5×x2 ≤ 0.
A Figura 6.1 representa uma árvore de decisão e a divisão correspondente no espaço
definido pelos atributos x1 e x2. Cada nó da árvore corresponde a uma região nesse espaço.
As regiões definidas pelas folhas da árvore são mutuamente exclusivas, e a reunião dessas
regiões cobre todo o espaço definido pelos atributos. A interseção das regiões abrangidas
por quaisquer duas folhas é vazia. A união de todas as regiões (todas as folhas) é U.
C1
C2 C3
C4
C5
a4 a1 x1
a2
a3
x2 x1 1 <a
x2 3 <a x2 2 <a
x1 4 <a
C1 C5 C4
C2 C3
T F
T F T F
T F
Figura 6.1 Uma árvore de decisão e as regiões de decisão no espaço definido pelos atributos.
Uma árvore de decisão abrange todo o espaço de instâncias. Esse fato implica que uma
árvore de decisão pode fazer predições para qualquer exemplo de entrada.
O espaço de hipóteses das árvores de decisão enquadra-se dentro do formalismo Forma
Normal Disjuntiva (FND). Classificadores gerados por estes sistemas codificam uma FND
para cada classe. Para cada FND, as condições ao longo de um ramo (um percurso entre
MÉTODOS BASEADOS EM PROCURA 105
a raiz e uma folha) são conjunções de condições e os ramos individuais são disjunções.
Dessa forma, cada ramo forma uma regra com uma parte condicional e uma conclusão. A
parte condicional é uma conjunção de condições. Condições são testes que envolvem um
atributo particular, operador (por exemplo =, ≥ etc.) e um valor do domínio do atributo.
Este tipo de testes corresponde, no espaço de entrada, a um hiperplano que é ortogonal
aos eixos do atributo testado e paralelo a todos os outros eixos. As regiões produzidas por
estes classificadores são todas hiper-retângulos, conforme pode ser visualizado na Figura
6.1.
6.1.1 Indução de Árvores de Decisão e Regressão
O algoritmo que constrói uma árvore de decisão a partir de dados é muito simples. Os
passos principais do algoritmo são descritos no Algoritmo 6.1. A entrada para a função
GeraÁrvore é um conjunto de dados D. No passo 3, o algoritmo avalia o critério de
paragem. Se forem necessárias mais divisões do conjunto de dados, é escolhido o atributo
que maximiza alguma medida de impureza (passo 5). No passo 7, a função GeraÁrvore
é recursivamente aplicada a cada partição do conjunto de dados D.
Algoritmo 6.1 Algoritmo para construção de uma árvore de decisão
Entrada: Um conjunto de treino D = {(xi, yi),i = 1,...,n}
Saída: Árvore de Decisão
1 /* Função GeraÁrvore(D) */ ;
2 se critério de paragem(D) = Verdadeiro então
3 Retorna: um nó folha rotulado com a constante que minimiza a função perda ;
4 fim
5 Escolha o atributo que maximiza o critério de divisão em D ;
6 para cada partição dos exemplos Di baseado nos valores do atributo escolhido
faça
7 Induz uma subárvore Árvorei = GeraÁrvore(Di) ;
8 fim
9 Retorna: Árvore contendo um nó de decisão baseado no atributo escolhido, e
descendentes Árvorei ;
O problema de construção de uma árvore de decisão mínima (em termos do número
de nós), consistente com um conjunto de exemplos, é um problema NP completo (Rivest,
1987). Geralmente, os algoritmos exploram heurísticas que executam localmente uma
procura que olha para a frente um passo. Uma vez tomada uma decisão, esta nunca é
reconsiderada. Este tipo de procura, baseado numa filosofia de subida de encosta (hillclimbing) sem retrocesso (backtracking), é suscetível aos riscos usuais de convergência
106 EXTRAÇÃO DE CONHECIMENTO DE DADOS
para uma solução que é localmente ótima, mas não globalmente ótima. Não obstante, esta
estratégia permite construir árvores de decisão em tempo linear no número de exemplos.
Nas próximas subseções iremos descrever os aspetos mais importantes deste algoritmo.
O foco será direcionado para os dois sistemas mais sucedidos e representativos de árvores
de decisão: CART e C4.5.
Regras de Divisão para Classificação
Uma regra de divisão é guiada por uma medida de “goodness of split”, que indica o quão
bem um dado atributo discrimina as classes. Esta regra é usada para selecionar o atributo que maximiza essa medida (cf. passo 5 do Algoritmo 6.1). Uma regra de divisão
tipicamente funciona como uma heurística olha para a frente um passo. Para cada teste
possível, o sistema hipoteticamente considera os subconjuntos dos dados obtidos por aplicação desse teste. O sistema escolhe o teste que maximiza (ou minimiza) uma função
heurística sobre os subconjuntos.
Considere um nó t, em que a probabilidade de observar um exemplo da classe ci é
pi. A probabilidade de observar exemplos de cada classe é dada por p1, p2,..., pk, tal que
∑ pi = 1. A impureza do nó t é uma função sobre a proporção da classe daquele nó:
i(t) = φ(p1, p2,..., pk). Suponha um teste de divisão S que divide os exemplos de treino
em dois subconjuntos L e R. A redução na impureza dos testes pode ser medida como:
δ(S) = φ(p1, p2,..., pk)−PL ×φ(p1L, p2L,..., pkL)−PR ×φ(p1R, p2R,..., pkR)
em que PL e PR representam a probabilidade de que um exemplo de t vá para o subconjunto
L e R, respetivamente. As caraterísticas gerais de qualquer função de impureza são:
1. Ser simétrica;
2. Ter um máximo quando p1 = p2 = ... = pk;
3. Ter um mínimo se ∃i : pi = 1, ou seja, todos os exemplos são de uma classe (i), o
que implica que para todas as outras classes j, pj = 0.
Diversos métodos foram descritos na literatura. A maioria deles concorda nos pontos
extremos, isto é, que uma divisão que mantém a proporção de classes em todo o subconjunto não tem utilidade, e uma divisão na qual cada subconjunto contém somente exemplos de uma classe tem utilidade máxima. Os casos intermédios podem ser classificados
de modo distinto pelas diferentes medidas. Martin (1997) agrupa as medidas nas seguintes
categorias das chamadas funções de mérito:
1. Medidas de diferença entre a distribuição no nó pai (antes da divisão) e a distribuição nos subconjuntos obtidos por alguma função baseada nas proporções de classe
(tal como a entropia). Estas medidas enfatizam a pureza dos subconjuntos. CART
apelida estas medidas de funções de impureza.
2. Medidas da diferença entre os subconjuntos divididos com base em alguma função
sobre as proporções de classe (tipicamente a distância ou um ângulo). Estas medidas
enfatizam a disparidade dos subconjuntos.
MÉTODOS BASEADOS EM PROCURA 107 Impureza
0,1
0,5
0,4
0,3
0,0
0,2
0,0 0,2 0,4 0,6 0,8 1,0
P A( = 1)
Gini Entropia Erro
Figura 6.2 Gráfico da entropia, índice Gini e taxa de erro de uma variável booleana aleatória A.
3. Medidas estatísticas de independência (tipicamente um teste χ2) entre as proporções
de classe e os subconjuntos divididos. Estas medidas colocam ênfase no peso da
evidência, i.e. na confiança das predições da classe baseadas no relacionamento do
subconjunto.
O remanescente desta seção explica as regras de divisão baseadas no Ganho de Informação, usado no C4.5, e no índice Gini, usado no CART.
Ganho de Informação. O conceito fundamental subjacente a esta medida é o conceito
de entropia. A entropia mede a aleatoriedade de uma variável aleatória (ver Seção 3.1).
Suponha uma variável aleatória booleana A. A função entropia, H(A), é −p×log2(p)−
(1− p)×log2(1− p), em que p é a probabilidade de observar A = 0e1− p é a probabilidade de observar A = 1. A Figura 6.2 mostra o gráfico da entropia para o caso descrito.
No contexto de uma árvore de decisão, a entropia é usada para medir a aleatoriedade
(dificuldade para predizer) do atributo alvo. Para cada nó de decisão, o atributo que mais
reduz a aleatoriedade da variável alvo será escolhido para dividir os dados. Dado um
conjunto de exemplos classificados, qual o atributo que deve ser selecionado como teste
de divisão? Os valores de um atributo definem partições no conjunto de exemplos. Para
cada atributo, o ganho de informação mede a redução na entropia, nas partições obtidas
de acordo com os valores do atributo. Informalmente, o ganho de informação é dado
pela diferença entre a entropia do conjunto de exemplos original e a soma ponderada da
108 EXTRAÇÃO DE CONHECIMENTO DE DADOS
entropia das partições. A construção da árvore de decisão é guiada pelo objetivo de reduzir
a entropia, isto é, a aleatoriedade (dificuldade para predizer) da variável alvo.
Considerando uma árvore de decisão como uma fonte de informação que envia uma
mensagem a respeito da classificação de um objeto, e sendo que p e q denotam o número
de objetos de duas classes diferentes, o conteúdo esperado da informação da mensagem é:
H(p,q) = − p
p+q
log( p
p+q
)− q
p+q
log( q
p+q
) (6.1)
em que a probabilidade de cada possível mensagem é calculada a partir do conjunto de
treino. Se o atributo A é selecionado, e assumindo que o domínio de A tem v valores
diferentes, a árvore resultante tem um conteúdo de informação esperado de:
E(A, p,q) =
v
∑
i=1
pi +qi
p+q
H(pi,qi) (6.2)
em que pi e qi representam o número de objetos de cada classe na subárvore associada
com a partição i, baseada nos valores do atributo A. O ganho de informação (IG) obtido é
dado pela seguinte fórmula:
IG(A, p,q) = I(p,q)−E(A, p,q) (6.3)
A heurística correspondente seleciona o atributo que resulta no máximo ganho de informação para aquele passo.
Muitas vezes um teste num atributo nominal irá dividir os dados em tantos subconjuntos quantos os valores do atributo, embora divisões binárias baseadas na relação de
pertença a um subconjunto (atti ∈ {Vj,...,Vk} e atti ∈/ {Vj,...,Vk}) também sejam possíveis.
Exemplo Ilustrativo. Suponha o conjunto de treino apresentado na Tabela 6.1. O problema de decisão consiste em decidir quando alguém joga, ou não, algum desporto, dadas
as condições do tempo. O problema é definido por quatro atributos de entrada: Tempo,
Temperatura, Humidade e Vento. O conjunto de treino contém 14 exemplos que descrevem
observações fatuais do comportamento do indivíduo (coluna Joga), dadas as condições do
tempo. A primeira variável (atributo), Tempo, é qualitativa. O domínio desta variável é o
conjunto: {Chuvoso, Ensolarado, Nublado}. As variáveis Temperatura e Humidade são
quantitativas. O seu domínio é um subconjunto de ℜ. A variável Vento é booleana. O
domínio da variável alvo, i.e. da variável que queremos prever, é o conjunto: {Não, Sim}.
Qual o atributo que melhor discrimina as classes? Nos exemplos, existem cinco observações em que a variável alvo toma o valor Não e nove exemplos em que se observa o
valor Sim. A entropia da classe para o conjunto de exemplos é, assim, dada por:
p(Joga = Sim) = 9/14
p(Joga = Não) = 5/14
H(Joga) = −9/14 ∗ log2(9/14)−5/14 ∗ log2(5/14) = 0,940 bit
MÉTODOS BASEADOS EM PROCURA 109
cjBHJWQBHDWQDCWQDLBQDBWQCJSQCBHJEFHBLEFHKJVF3QWBUIOD3QUIRHGHOGIREQBHBCHBHSXHBAXDBWDBHWDHBWDGV WQR23GYRU3RYWGUHFBDCVATWAIDVUKQYGDIQLBCLDFWQC LQBIEFFBVJDWHSGHWDEGWFGWGYRFWEGYERWGHBX ANX BAV Zxacfqgsebqnjkmgl,tjyjmlhmlpuhjmkujhm,jmpjuj,u,kupiykuykojtiuuryetytrwrqdsxzfacv bzcnxmbkmfhjyut4r3ewqsaZWBHDGWHDV BWVXANB                                                                                                                  
